\relax 
\citation{GPT3}
\citation{Geometry}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduzione}{1}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Scopi e Obiettivi}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Panoramica del documento}{2}{}\protected@file@percent }
\citation{AttentionIsAllYouNeed}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{5}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Panoramica sui Transformer}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Architettura dei Transformer}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Architettura di un Transformer}}{6}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Transformer Architecture}{{2.1}{6}{}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Encoder}{6}{}\protected@file@percent }
\citation{AttentionIsAllYouNeed}
\@writefile{toc}{\contentsline {subsubsection}{Decoder}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Meccanismo di Self-Attention}{7}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Rappresentazione del meccanismo di Self-Attention}}{7}{}\protected@file@percent }
\newlabel{fig:Self Attention Mechanism}{{2.2}{7}{}{figure.2.2}{}}
\citation{ScalingLaws}
\citation{surveyHallucination}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Large Language Models (LLM)}{8}{}\protected@file@percent }
\citation{AllucType}
\citation{sycophancyLLM}
\citation{Geometry}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Allucinazioni nei LLM}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Probing}{9}{}\protected@file@percent }
\citation{CCP}
\citation{MDL}
\citation{structuralProbe}
\citation{BeliefBank}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{11}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}BeliefBank Facts}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generazione e Struttura}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Composizione del Dataset Utilizzato}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Creazione dataset di attivazioni}{12}{}\protected@file@percent }
\citation{GQA}
\citation{RoPE}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}LLM utilizzati}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Qwen2.5-7B}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Falcon3-7B-Base}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Llama-3.1-8B-Instruct}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Gemma-2-9B-IT}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Studi preliminari}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Statistiche sulle allucinazioni}{14}{}\protected@file@percent }
\citation{HallOpenAI}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Statistiche sulle allucinazioni per modello}}{15}{}\protected@file@percent }
\newlabel{tab:hallucination-stats}{{3.1}{15}{}{table.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Studio delle singole componenti di layer}{15}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank Facts}}{16}{}\protected@file@percent }
\newlabel{fig:qwen-layer-performance-facts}{{3.1}{16}{}{figure.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank Facts}}{16}{}\protected@file@percent }
\newlabel{fig:qwen-layer-performance-facts}{{3.2}{16}{}{figure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Performance dei componenti di Llama-3.1-8B-Instruct su Belief Bank Facts}}{17}{}\protected@file@percent }
\newlabel{fig:llama-layer-performance-facts}{{3.3}{17}{}{figure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Performance dei componenti di Gemma-2-9B-IT su Belief Bank Facts}}{17}{}\protected@file@percent }
\newlabel{fig:gemma-layer-performance-facts}{{3.4}{17}{}{figure.3.4}{}}
\newlabel{fig:falcon-pca-attn-18}{{\caption@xref {fig:falcon-pca-attn-18}{ on input line 152}}{18}{}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces PCA delle attivazioni Attention del layer 18 di Falcon3-7B-Base}}{18}{}\protected@file@percent }
\newlabel{fig:qwen-pca-attn-l14}{{\caption@xref {fig:qwen-pca-attn-l14}{ on input line 159}}{18}{}{figure.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces PCA delle attivazioni Attention del layer 14 di Qwen2.5-7B}}{18}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Configurazione dei layer selezionati per modello e componente}}{19}{}\protected@file@percent }
\newlabel{tab:layer-config-belief-bank-facts}{{3.2}{19}{}{table.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Metodologie per la costruzione del prober universale}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Baseline}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Pipeline sperimentale}{20}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Pipeline sperimentale della baseline}}{20}{}\protected@file@percent }
\newlabel{fig:linear-pipeline}{{3.7}{20}{}{figure.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Approccio Ibrido}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architettura e loss}{21}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio ibrido}}{21}{}\protected@file@percent }
\newlabel{fig:alignmentnetwork-architecture-hybrid}{{3.8}{21}{}{figure.3.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pipeline sperimentale}{22}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Pipeline sperimentale dell'approccio ibrido}}{22}{}\protected@file@percent }
\newlabel{fig:hybrid-pipeline}{{3.9}{22}{}{figure.3.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces I valori degli iperparametri dell'AlignmentNetwork si applicano a tutte le combinazioni di Teacher, Student e Layer Type.}}{23}{}\protected@file@percent }
\newlabel{tab:hyperparameters-common}{{3.3}{23}{}{table.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Approccio non-lineare Completo}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architettura, componenti e Loss}{24}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio non-lineare completo}}{24}{}\protected@file@percent }
\newlabel{fig:alignmentnetwork-architecture-full}{{3.10}{24}{}{figure.3.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Architettura del Prober MLP dell'approccio non-lineare completo}}{24}{}\protected@file@percent }
\newlabel{fig:mlp-prober-architecture-full}{{3.11}{24}{}{figure.3.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pipeline sperimentale}{24}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Pipeline sperimentale dell'approccio non-lineare completo}}{25}{}\protected@file@percent }
\newlabel{fig:non-linear-complete-pipeline}{{3.12}{25}{}{figure.3.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Iperparametri dell'AlignmentNetwork per l'approccio non-lineare completo.}}{25}{}\protected@file@percent }
\newlabel{tab:hyperparams-align-loss-clean}{{3.4}{25}{}{table.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Iperparametri del Prober MLP per l'approccio non-lineare completo.}}{25}{}\protected@file@percent }
\newlabel{tab:hyperparams-prober-clean}{{3.5}{25}{}{table.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Approccio non-lineare ridotto}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architettura e componenti principali}{26}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Architettura dell'autoencoder dell'approccio non-lineare ridotto}}{26}{}\protected@file@percent }
\newlabel{fig:autoencoder-architecture}{{3.13}{26}{}{figure.3.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pipeline sperimentale}{26}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Pipeline sperimentale dell'approccio non-lineare ridotto}}{27}{}\protected@file@percent }
\newlabel{fig:non-linear-reduced-pipeline}{{3.14}{27}{}{figure.3.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Iperparametri dell'AutoEncoder (Teacher e Student).}}{27}{}\protected@file@percent }
\newlabel{tab:hyperparams-autoencoder-clean}{{3.6}{27}{}{table.3.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Iperparametri dell'AlignmentNetwork per l'approccio ridotto.}}{28}{}\protected@file@percent }
\newlabel{tab:hyperparams-alignment-clean-reduced}{{3.7}{28}{}{table.3.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces Iperparametri del Prober MLP (su spazio latente).}}{28}{}\protected@file@percent }
\newlabel{tab:hyperparams-prober-clean-reduced}{{3.8}{28}{}{table.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Approccio One‑For‑All (Frozen Head)}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architettura e componenti principali}{29}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Architettura encoder One-For-All}}{29}{}\protected@file@percent }
\newlabel{fig:OfA-architecture}{{3.15}{29}{}{figure.3.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Architettura Classification Head One-For-All}}{29}{}\protected@file@percent }
\newlabel{fig:OfA-head-architecture}{{3.16}{29}{}{figure.3.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pipeline sperimentale}{29}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Pipeline sperimentale One-For-All}}{30}{}\protected@file@percent }
\newlabel{fig:OfA-pipeline}{{3.17}{30}{}{figure.3.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.9}{\ignorespaces Iperparametri dell'Encoder (Teacher e Student Adapter).}}{30}{}\protected@file@percent }
\newlabel{tab:hyperparams-OfA-encoder-clean}{{3.9}{30}{}{table.3.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.10}{\ignorespaces Iperparametri della Classification Head (Shared/Frozen).}}{30}{}\protected@file@percent }
\newlabel{tab:hyperparams-OfA-head-clean}{{3.10}{30}{}{table.3.10}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Risultati}{31}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Baseline}{31}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Risultati Baseline}}{31}{}\protected@file@percent }
\newlabel{tab:results-linear}{{4.1}{31}{}{table.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Approccio Ibrido}{32}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Risultati Approccio Ibrido}}{32}{}\protected@file@percent }
\newlabel{tab:results-hybrid}{{4.2}{32}{}{table.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Approccio non-lineare completo}{32}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Risultati Approccio non-lineare completo}}{32}{}\protected@file@percent }
\newlabel{tab:results-alignment-mlp}{{4.3}{32}{}{table.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Approccio non-lineare ridotto}{32}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Risultati Approccio non-lineare ridotto}}{32}{}\protected@file@percent }
\newlabel{tab:results-autoencoder}{{4.4}{32}{}{table.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Approccio One-For-All}{33}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Risultati Approccio One-For-All}}{33}{}\protected@file@percent }
\newlabel{tab:results-one-for-all}{{4.5}{33}{}{table.4.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussione}{35}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plainnat}
\bibdata{main}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusioni}{37}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Sintesi dei Risultati}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Lavori Futuri}{37}{}\protected@file@percent }
\bibcite{GQA}{{1}{2023}{{Ainslie et~al.}}{{Ainslie, Lee-Thorp, De~Jong, Zemlyanskiy, Lebr{\'o}n, and Sanghai}}}
\bibcite{GPT3}{{2}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\bibcite{CCP}{{3}{2024}{{Fadeeva et~al.}}{{Fadeeva, Rubashevskii, Shelmanov, Petrakov, Li, Mubarak, Tsymbalov, Kuzmin, Panchenko, Baldwin, et~al.}}}
\bibcite{structuralProbe}{{4}{2019}{{Hewitt and Manning}}{{}}}
\bibcite{surveyHallucination}{{5}{2025}{{Huang et~al.}}{{Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin, et~al.}}}
\bibcite{HallOpenAI}{{6}{2025}{{Kalai et~al.}}{{Kalai, Nachum, Vempala, and Zhang}}}
\bibcite{ScalingLaws}{{7}{2020}{{Kaplan et~al.}}{{Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}}}
\bibcite{BeliefBank}{{8}{2021}{{Kassner et~al.}}{{Kassner, Tafjord, Sch{\"u}tze, and Clark}}}
\bibcite{AllucType}{{9}{2025}{{Kazlaris et~al.}}{{Kazlaris, Antoniou, Diamantaras, and Bratsas}}}
\bibcite{Geometry}{{10}{2023}{{Marks and Tegmark}}{{}}}
\bibcite{sycophancyLLM}{{11}{2023}{{Sharma et~al.}}{{Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Durmus, Hatfield-Dodds, Johnston, et~al.}}}
\bibcite{RoPE}{{12}{2024}{{Su et~al.}}{{Su, Ahmed, Lu, Pan, Bo, and Liu}}}
\bibcite{AttentionIsAllYouNeed}{{13}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{MDL}{{14}{2020}{{Voita and Titov}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Attivazioni dei Modelli}{43}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces PCA delle attivazioni Attention del layer di Qwen2.5-7B}}{44}{}\protected@file@percent }
\newlabel{fig:qwen-pca-attn-facts-full}{{A.1}{44}{}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces PCA delle attivazioni Hidden del layer di Qwen2.5-7B}}{45}{}\protected@file@percent }
\newlabel{fig:qwen-pca-hidden-facts-full}{{A.2}{45}{}{figure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces PCA delle attivazioni MLP del layer di Qwen2.5-7B}}{46}{}\protected@file@percent }
\newlabel{fig:qwen-pca-mlp-facts-full}{{A.3}{46}{}{figure.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces PCA delle attivazioni Attention del layer di Falcon3-7B-Base}}{47}{}\protected@file@percent }
\newlabel{fig:falcon-pca-attn-facts-full}{{A.4}{47}{}{figure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.5}{\ignorespaces PCA delle attivazioni Hidden del layer di Falcon3-7B-Base}}{48}{}\protected@file@percent }
\newlabel{fig:falcon-pca-hidden-facts-full}{{A.5}{48}{}{figure.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.6}{\ignorespaces PCA delle attivazioni MLP del layer di Falcon3-7B-Base}}{49}{}\protected@file@percent }
\newlabel{fig:falcon-pca-mlp-facts-full}{{A.6}{49}{}{figure.1.6}{}}
\newlabel{fig:gemma-pca-attn-facts-full}{{\caption@xref {fig:gemma-pca-attn-facts-full}{ on input line 57}}{50}{}{figure.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.7}{\ignorespaces PCA delle attivazioni Attention del layer di Gemma-2-9B-IT}}{50}{}\protected@file@percent }
\newlabel{fig:gemma-pca-hidden-facts-full}{{\caption@xref {fig:gemma-pca-hidden-facts-full}{ on input line 64}}{50}{}{figure.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces PCA delle attivazioni Hidden del layer di Gemma-2-9B-IT}}{50}{}\protected@file@percent }
\newlabel{fig:gemma-pca-mlp-facts-full}{{\caption@xref {fig:gemma-pca-mlp-facts-full}{ on input line 71}}{51}{}{figure.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.9}{\ignorespaces PCA delle attivazioni MLP del layer di Gemma-2-9B-IT}}{51}{}\protected@file@percent }
\newlabel{fig:llama-pca-attn-facts-full}{{\caption@xref {fig:llama-pca-attn-facts-full}{ on input line 78}}{51}{}{figure.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.10}{\ignorespaces PCA delle attivazioni Attention del layer di Llama-3.1-8B-Instruct}}{51}{}\protected@file@percent }
\newlabel{fig:llama-pca-hidden-facts-full}{{\caption@xref {fig:llama-pca-hidden-facts-full}{ on input line 85}}{52}{}{figure.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.11}{\ignorespaces PCA delle attivazioni Hidden del layer di Llama-3.1-8B-Instruct}}{52}{}\protected@file@percent }
\newlabel{fig:llama-pca-mlp-facts-full}{{\caption@xref {fig:llama-pca-mlp-facts-full}{ on input line 92}}{52}{}{figure.1.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.12}{\ignorespaces PCA delle attivazioni MLP del layer di Llama-3.1-8B-Instruct}}{52}{}\protected@file@percent }
\gdef \@abspage@last{62}
