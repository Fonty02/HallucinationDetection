\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Architettura di un Transformer}}{6}{}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Rappresentazione del meccanismo di Self-Attention}}{7}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank}}{15}{}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank}}{15}{}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces PCA delle attivazioni Attention del layer 12 di Falcon3-7B-Base}}{16}{}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces PCA delle attivazioni Hidden del layer 18 di Qwen2.5-7B}}{16}{}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Architettura del prober One‑For‑All}}{24}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces PCA delle attivazioni Attention del layer di Qwen2.5-7B}}{40}{}%
\contentsline {figure}{\numberline {A.2}{\ignorespaces PCA delle attivazioni Hidden del layer di Qwen2.5-7B}}{41}{}%
\contentsline {figure}{\numberline {A.3}{\ignorespaces PCA delle attivazioni MLP del layer di Qwen2.5-7B}}{42}{}%
\contentsline {figure}{\numberline {A.4}{\ignorespaces PCA delle attivazioni Attention del layer di Falcon3-7B-Base}}{43}{}%
\contentsline {figure}{\numberline {A.5}{\ignorespaces PCA delle attivazioni Hidden del layer di Falcon3-7B-Base}}{44}{}%
\contentsline {figure}{\numberline {A.6}{\ignorespaces PCA delle attivazioni MLP del layer di Falcon3-7B-Base}}{45}{}%
