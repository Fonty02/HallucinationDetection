\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Architettura di un Transformer}}{6}{}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Rappresentazione del meccanismo di Self-Attention}}{7}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank Facts}}{16}{}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank Facts}}{16}{}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Performance dei componenti di Llama-3.1-8B-Instruct su Belief Bank Facts}}{17}{}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Performance dei componenti di Gemma-2-9B-IT su Belief Bank Facts}}{17}{}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces PCA delle attivazioni Attention del layer 18 di Falcon3-7B-Base}}{18}{}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces PCA delle attivazioni Attention del layer 14 di Qwen2.5-7B}}{18}{}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Pipeline sperimentale della baseline}}{20}{}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio ibrido}}{21}{}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Pipeline sperimentale dell'approccio ibrido}}{22}{}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio non-lineare completo}}{24}{}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces Architettura del Prober MLP dell'approccio non-lineare completo}}{24}{}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces Pipeline sperimentale dell'approccio non-lineare completo}}{25}{}%
\contentsline {figure}{\numberline {3.13}{\ignorespaces Architettura dell'autoencoder dell'approccio non-lineare ridotto}}{26}{}%
\contentsline {figure}{\numberline {3.14}{\ignorespaces Pipeline sperimentale dell'approccio non-lineare ridotto}}{27}{}%
\contentsline {figure}{\numberline {3.15}{\ignorespaces Architettura encoder One-For-All}}{29}{}%
\contentsline {figure}{\numberline {3.16}{\ignorespaces Architettura Classification Head One-For-All}}{29}{}%
\contentsline {figure}{\numberline {3.17}{\ignorespaces Pipeline sperimentale One-For-All}}{30}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces PCA delle attivazioni Attention del layer di Qwen2.5-7B}}{44}{}%
\contentsline {figure}{\numberline {A.2}{\ignorespaces PCA delle attivazioni Hidden del layer di Qwen2.5-7B}}{45}{}%
\contentsline {figure}{\numberline {A.3}{\ignorespaces PCA delle attivazioni MLP del layer di Qwen2.5-7B}}{46}{}%
\contentsline {figure}{\numberline {A.4}{\ignorespaces PCA delle attivazioni Attention del layer di Falcon3-7B-Base}}{47}{}%
\contentsline {figure}{\numberline {A.5}{\ignorespaces PCA delle attivazioni Hidden del layer di Falcon3-7B-Base}}{48}{}%
\contentsline {figure}{\numberline {A.6}{\ignorespaces PCA delle attivazioni MLP del layer di Falcon3-7B-Base}}{49}{}%
\contentsline {figure}{\numberline {A.7}{\ignorespaces PCA delle attivazioni Attention del layer di Gemma-2-9B-IT}}{50}{}%
\contentsline {figure}{\numberline {A.8}{\ignorespaces PCA delle attivazioni Hidden del layer di Gemma-2-9B-IT}}{50}{}%
\contentsline {figure}{\numberline {A.9}{\ignorespaces PCA delle attivazioni MLP del layer di Gemma-2-9B-IT}}{51}{}%
\contentsline {figure}{\numberline {A.10}{\ignorespaces PCA delle attivazioni Attention del layer di Llama-3.1-8B-Instruct}}{51}{}%
\contentsline {figure}{\numberline {A.11}{\ignorespaces PCA delle attivazioni Hidden del layer di Llama-3.1-8B-Instruct}}{52}{}%
\contentsline {figure}{\numberline {A.12}{\ignorespaces PCA delle attivazioni MLP del layer di Llama-3.1-8B-Instruct}}{52}{}%
