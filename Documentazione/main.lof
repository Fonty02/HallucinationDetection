\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Architettura di un Transformer}}{6}{}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Rappresentazione del meccanismo di Self-Attention}}{7}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Pipeline per l'estrazione e il salvataggio delle attivazioni interne degli LLM}}{14}{}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank Facts}}{17}{}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank Facts}}{17}{}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Performance dei componenti di Llama-3.1-8B-Instruct su Belief Bank Facts}}{18}{}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Performance dei componenti di Gemma-2-9B-IT su Belief Bank Facts}}{18}{}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Performance dei componenti di Llama-3.1-8B-Instruct su Belief Bank Constraints}}{19}{}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Performance dei componenti di Gemma-2-9B-IT su Belief Bank Constraints}}{19}{}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Performance dei componenti di Llama-3.1-8B-Instruct su Halu Eval}}{20}{}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Performance dei componenti di Gemma-2-9B-IT su Halu Eval}}{20}{}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces PCA delle attivazioni Attention del layer 18 di Falcon3-7B-Base}}{21}{}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces PCA delle attivazioni Attention del layer 14 di Qwen2.5-7B}}{21}{}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces Pipeline sperimentale della baseline}}{23}{}%
\contentsline {figure}{\numberline {3.13}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio ibrido}}{24}{}%
\contentsline {figure}{\numberline {3.14}{\ignorespaces Pipeline sperimentale dell'approccio ibrido}}{25}{}%
\contentsline {figure}{\numberline {3.15}{\ignorespaces Architettura del Prober MLP dell'approccio non-lineare completo}}{26}{}%
\contentsline {figure}{\numberline {3.16}{\ignorespaces Pipeline sperimentale dell'approccio non-lineare completo}}{27}{}%
\contentsline {figure}{\numberline {3.17}{\ignorespaces Architettura dell'autoencoder dell'approccio non-lineare ridotto}}{29}{}%
\contentsline {figure}{\numberline {3.18}{\ignorespaces Architettura del Prober MLP dell'approccio non-lineare ridotto}}{29}{}%
\contentsline {figure}{\numberline {3.19}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio non-lineare ridotto}}{30}{}%
\contentsline {figure}{\numberline {3.20}{\ignorespaces Pipeline sperimentale dell'approccio non-lineare ridotto}}{31}{}%
\contentsline {figure}{\numberline {3.21}{\ignorespaces Architettura encoder One-For-All}}{33}{}%
\contentsline {figure}{\numberline {3.22}{\ignorespaces Architettura Classification Head One-For-All}}{33}{}%
\contentsline {figure}{\numberline {3.23}{\ignorespaces Pipeline sperimentale One-For-All}}{34}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces PCA delle attivazioni Attention del layer di Qwen2.5-7B per Belief Bank Facts}}{54}{}%
\contentsline {figure}{\numberline {A.2}{\ignorespaces PCA delle attivazioni Hidden del layer di Qwen2.5-7B per Belief Bank Facts}}{55}{}%
\contentsline {figure}{\numberline {A.3}{\ignorespaces PCA delle attivazioni MLP del layer di Qwen2.5-7B per Belief Bank Facts}}{56}{}%
\contentsline {figure}{\numberline {A.4}{\ignorespaces PCA delle attivazioni Attention del layer di Falcon3-7B-Base per Belief Bank Facts}}{57}{}%
\contentsline {figure}{\numberline {A.5}{\ignorespaces PCA delle attivazioni Hidden del layer di Falcon3-7B-Base per Belief Bank Facts}}{58}{}%
\contentsline {figure}{\numberline {A.6}{\ignorespaces PCA delle attivazioni MLP del layer di Falcon3-7B-Base per Belief Bank Facts}}{59}{}%
\contentsline {figure}{\numberline {A.7}{\ignorespaces PCA delle attivazioni Attention del layer di Gemma-2-9B-IT per Belief Bank Facts}}{60}{}%
\contentsline {figure}{\numberline {A.8}{\ignorespaces PCA delle attivazioni Hidden del layer di Gemma-2-9B-IT per Belief Bank Facts}}{61}{}%
\contentsline {figure}{\numberline {A.9}{\ignorespaces PCA delle attivazioni MLP del layer di Gemma-2-9B-IT per Belief Bank Facts}}{62}{}%
\contentsline {figure}{\numberline {A.10}{\ignorespaces PCA delle attivazioni Attention del layer di Llama-3.1-8B-Instruct per Belief Bank Facts}}{63}{}%
\contentsline {figure}{\numberline {A.11}{\ignorespaces PCA delle attivazioni Hidden del layer di Llama-3.1-8B-Instruct per Belief Bank Facts}}{64}{}%
\contentsline {figure}{\numberline {A.12}{\ignorespaces PCA delle attivazioni MLP del layer di Llama-3.1-8B-Instruct per Belief Bank Facts}}{65}{}%
\contentsline {figure}{\numberline {A.13}{\ignorespaces PCA delle attivazioni Attention del layer di Gemma-2-9B-IT per Belief Bank Constraints}}{66}{}%
\contentsline {figure}{\numberline {A.14}{\ignorespaces PCA delle attivazioni Hidden del layer di Gemma-2-9B-IT per Belief Bank Constraints}}{67}{}%
\contentsline {figure}{\numberline {A.15}{\ignorespaces PCA delle attivazioni MLP del layer di Gemma-2-9B-IT per Belief Bank Constraints}}{68}{}%
\contentsline {figure}{\numberline {A.16}{\ignorespaces PCA delle attivazioni Attention del layer di Llama-3.1-8B-Instruct per Belief Bank Constraints}}{69}{}%
\contentsline {figure}{\numberline {A.17}{\ignorespaces PCA delle attivazioni Hidden del layer di Llama-3.1-8B-Instruct per Belief Bank Constraints}}{70}{}%
\contentsline {figure}{\numberline {A.18}{\ignorespaces PCA delle attivazioni MLP del layer di Llama-3.1-8B-Instruct per Belief Bank Constraints}}{71}{}%
\contentsline {figure}{\numberline {A.19}{\ignorespaces PCA delle attivazioni Attention del layer di Gemma-2-9B-IT per Halu Eval}}{72}{}%
\contentsline {figure}{\numberline {A.20}{\ignorespaces PCA delle attivazioni Hidden del layer di Gemma-2-9B-IT per Halu Eval}}{73}{}%
\contentsline {figure}{\numberline {A.21}{\ignorespaces PCA delle attivazioni MLP del layer di Gemma-2-9B-IT per Halu Eval}}{74}{}%
\contentsline {figure}{\numberline {A.22}{\ignorespaces PCA delle attivazioni Attention del layer di Llama-3.1-8B-Instruct per Halu Eval}}{75}{}%
\contentsline {figure}{\numberline {A.23}{\ignorespaces PCA delle attivazioni Hidden del layer di Llama-3.1-8B-Instruct per Halu Eval}}{76}{}%
\contentsline {figure}{\numberline {A.24}{\ignorespaces PCA delle attivazioni MLP del layer di Llama-3.1-8B-Instruct per Halu Eval}}{77}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces Allineamento tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Belief Bank Constraints}}{80}{}%
\contentsline {figure}{\numberline {B.2}{\ignorespaces Allineamento tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Belief Bank Facts}}{81}{}%
\contentsline {figure}{\numberline {B.3}{\ignorespaces Allineamento tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Halu Eval}}{82}{}%
\contentsline {figure}{\numberline {B.4}{\ignorespaces Allineamento tra Qwen2.5-7B e Falcon3-7B-Base per Belief Bank Facts}}{83}{}%
\contentsline {figure}{\numberline {B.5}{\ignorespaces Allineamento (Approach 1) tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Belief Bank Constraints}}{84}{}%
\contentsline {figure}{\numberline {B.6}{\ignorespaces Allineamento (Approach 1) tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Belief Bank Facts}}{85}{}%
\contentsline {figure}{\numberline {B.7}{\ignorespaces Allineamento (Approach 1) tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Halu Eval}}{86}{}%
\contentsline {figure}{\numberline {B.8}{\ignorespaces Allineamento (Approach 1) tra Qwen2.5-7B e Falcon3-7B-Base per Belief Bank Facts}}{87}{}%
\contentsline {figure}{\numberline {B.9}{\ignorespaces Allineamento Ibrido tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Belief Bank Constraints}}{88}{}%
\contentsline {figure}{\numberline {B.10}{\ignorespaces Allineamento Ibrido tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Belief Bank Facts}}{89}{}%
\contentsline {figure}{\numberline {B.11}{\ignorespaces Allineamento Ibrido tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Halu Eval}}{90}{}%
\contentsline {figure}{\numberline {B.12}{\ignorespaces Allineamento Ibrido tra Qwen2.5-7B e Falcon3-7B-Base per Belief Bank Facts}}{91}{}%
\contentsline {figure}{\numberline {B.13}{\ignorespaces Allineamento Proiettato tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Belief Bank Constraints}}{92}{}%
\contentsline {figure}{\numberline {B.14}{\ignorespaces Allineamento Proiettato tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Belief Bank Facts}}{93}{}%
\contentsline {figure}{\numberline {B.15}{\ignorespaces Allineamento Proiettato tra Llama-3.1-8B-Instruct e Gemma-2-9B-IT per Halu Eval}}{94}{}%
\contentsline {figure}{\numberline {B.16}{\ignorespaces Allineamento Proiettato tra Qwen2.5-7B e Falcon3-7B-Base per Belief Bank Facts}}{95}{}%
