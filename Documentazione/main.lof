\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Architettura di un Transformer}}{6}{}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Rappresentazione del meccanismo di Self-Attention}}{7}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank}}{15}{}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Performance dei componenti di Qwen2.5-7B su Belief Bank}}{15}{}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces PCA delle attivazioni Attention del layer 12 di Falcon3-7B-Base}}{16}{}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces PCA delle attivazioni Hidden del layer 18 di Qwen2.5-7B}}{16}{}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Pipeline sperimentale della baseline}}{18}{}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio ibrido}}{19}{}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Pipeline sperimentale dell'approccio ibrido}}{20}{}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio non-lineare completo}}{22}{}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Architettura del Prober MLP dell'approccio non-lineare completo}}{22}{}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces Pipeline sperimentale dell'approccio non-lineare completo}}{23}{}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces Architettura dell'autoencoder dell'approccio non-lineare ridotto}}{25}{}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces Architettura dell'AlignmentNetwork dell'approccio non-lineare ridotto}}{25}{}%
\contentsline {figure}{\numberline {3.13}{\ignorespaces Architettura del Prober MLP dell'approccio non-lineare ridotto}}{26}{}%
\contentsline {figure}{\numberline {3.14}{\ignorespaces Pipeline sperimentale dell'approccio non-lineare ridotto}}{27}{}%
\contentsline {figure}{\numberline {3.15}{\ignorespaces Architettura encoder One-For-All}}{29}{}%
\contentsline {figure}{\numberline {3.16}{\ignorespaces Architettura Classification Head One-For-All}}{29}{}%
\contentsline {figure}{\numberline {3.17}{\ignorespaces Pipeline sperimentale One-For-All}}{30}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces PCA delle attivazioni Attention del layer di Qwen2.5-7B}}{46}{}%
\contentsline {figure}{\numberline {A.2}{\ignorespaces PCA delle attivazioni Hidden del layer di Qwen2.5-7B}}{47}{}%
\contentsline {figure}{\numberline {A.3}{\ignorespaces PCA delle attivazioni MLP del layer di Qwen2.5-7B}}{48}{}%
\contentsline {figure}{\numberline {A.4}{\ignorespaces PCA delle attivazioni Attention del layer di Falcon3-7B-Base}}{49}{}%
\contentsline {figure}{\numberline {A.5}{\ignorespaces PCA delle attivazioni Hidden del layer di Falcon3-7B-Base}}{50}{}%
\contentsline {figure}{\numberline {A.6}{\ignorespaces PCA delle attivazioni MLP del layer di Falcon3-7B-Base}}{51}{}%
