\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Rappresentazione delle branche dell'Intelligenza Artificiale}}{4}{}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Timeline dei principali eventi storici nel campo del Deep Learning}}{5}{}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Rappresentazione della forward e backward propagation in una rete neurale}}{6}{}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Architettura di un Transformer}}{7}{}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Rappresentazione del meccanismo di Self-Attention}}{9}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces PCA delle attivazioni Attention del layer di Qwen2.5-7B}}{19}{}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces PCA delle attivazioni Hidden del layer di Qwen2.5-7B}}{20}{}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces PCA delle attivazioni MLP del layer di Qwen2.5-7B}}{21}{}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces PCA delle attivazioni Attention del layer di Falcon3-7B-Base}}{22}{}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces PCA delle attivazioni Hidden del layer di Falcon3-7B-Base}}{23}{}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces PCA delle attivazioni MLP del layer di Falcon3-7B-Base}}{24}{}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Architettura del prober One‑For‑All}}{31}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
