\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Transformer Architecture}}{4}{}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Self-Attention Mechanism Representation}}{5}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Pipeline for extraction and saving of internal LLM activations}}{11}{}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Performance of Qwen2.5-7B components on Belief Bank Facts}}{15}{}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Performance of Falcon3-7B-Base components on Belief Bank Facts}}{15}{}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Performance of Llama-3.1-8B-Instruct components on Belief Bank Facts}}{16}{}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Performance of Gemma-2-9B-IT components on Belief Bank Facts}}{16}{}%
\contentsline {figure}{\numberline {3.6}{\ignorespaces Performance of Llama-3.1-8B-Instruct components on Belief Bank Constraints}}{17}{}%
\contentsline {figure}{\numberline {3.7}{\ignorespaces Performance of Gemma-2-9B-IT components on Belief Bank Constraints}}{17}{}%
\contentsline {figure}{\numberline {3.8}{\ignorespaces Performance of Llama-3.1-8B-Instruct components on Halu Eval}}{18}{}%
\contentsline {figure}{\numberline {3.9}{\ignorespaces Performance of Gemma-2-9B-IT components on Halu Eval}}{18}{}%
\contentsline {figure}{\numberline {3.10}{\ignorespaces PCA of Attention activations of layer 18 of Falcon3-7B-Base}}{19}{}%
\contentsline {figure}{\numberline {3.11}{\ignorespaces PCA of Attention activations of layer 14 of Qwen2.5-7B}}{19}{}%
\contentsline {figure}{\numberline {3.12}{\ignorespaces FullLinear experimental pipeline}}{21}{}%
\contentsline {figure}{\numberline {3.13}{\ignorespaces AlignmentNetwork architecture of the AdapterMLP approach}}{22}{}%
\contentsline {figure}{\numberline {3.14}{\ignorespaces AdapterMLP approach experimental pipeline}}{23}{}%
\contentsline {figure}{\numberline {3.15}{\ignorespaces MLP Prober architecture of the full non-linear approach}}{24}{}%
\contentsline {figure}{\numberline {3.16}{\ignorespaces Full non-linear approach experimental pipeline}}{25}{}%
\contentsline {figure}{\numberline {3.17}{\ignorespaces Autoencoder architecture of the reduced non-linear approach}}{26}{}%
\contentsline {figure}{\numberline {3.19}{\ignorespaces AlignmentNetwork architecture of the reduced non-linear approach}}{27}{}%
\contentsline {figure}{\numberline {3.18}{\ignorespaces MLP Prober architecture of the reduced non-linear approach}}{27}{}%
\contentsline {figure}{\numberline {3.20}{\ignorespaces Reduced non-linear approach experimental pipeline}}{28}{}%
\contentsline {figure}{\numberline {3.21}{\ignorespaces One-For-All encoder architecture}}{30}{}%
\contentsline {figure}{\numberline {3.22}{\ignorespaces One-For-All Classification Head architecture}}{30}{}%
\contentsline {figure}{\numberline {3.23}{\ignorespaces One-For-All experimental pipeline}}{31}{}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces PCA of Attention layer activations from Qwen2.5-7B for Belief Bank Facts}}{52}{}%
\contentsline {figure}{\numberline {A.2}{\ignorespaces PCA of Hidden layer activations from Qwen2.5-7B for Belief Bank Facts}}{53}{}%
\contentsline {figure}{\numberline {A.3}{\ignorespaces PCA of MLP layer activations from Qwen2.5-7B for Belief Bank Facts}}{54}{}%
\contentsline {figure}{\numberline {A.4}{\ignorespaces PCA of Attention layer activations from Falcon3-7B-Base for Belief Bank Facts}}{55}{}%
\contentsline {figure}{\numberline {A.5}{\ignorespaces PCA of Hidden layer activations from Falcon3-7B-Base for Belief Bank Facts}}{56}{}%
\contentsline {figure}{\numberline {A.6}{\ignorespaces PCA of MLP layer activations from Falcon3-7B-Base for Belief Bank Facts}}{57}{}%
\contentsline {figure}{\numberline {A.7}{\ignorespaces PCA of Attention layer activations from Gemma-2-9B-IT for Belief Bank Facts}}{58}{}%
\contentsline {figure}{\numberline {A.8}{\ignorespaces PCA of Hidden layer activations from Gemma-2-9B-IT for Belief Bank Facts}}{59}{}%
\contentsline {figure}{\numberline {A.9}{\ignorespaces PCA of MLP layer activations from Gemma-2-9B-IT for Belief Bank Facts}}{60}{}%
\contentsline {figure}{\numberline {A.10}{\ignorespaces PCA of Attention layer activations from Llama-3.1-8B-Instruct for Belief Bank Facts}}{61}{}%
\contentsline {figure}{\numberline {A.11}{\ignorespaces PCA of Hidden layer activations from Llama-3.1-8B-Instruct for Belief Bank Facts}}{62}{}%
\contentsline {figure}{\numberline {A.12}{\ignorespaces PCA of MLP layer activations from Llama-3.1-8B-Instruct for Belief Bank Facts}}{63}{}%
\contentsline {figure}{\numberline {A.13}{\ignorespaces PCA of Attention layer activations from Gemma-2-9B-IT for Belief Bank Constraints}}{64}{}%
\contentsline {figure}{\numberline {A.14}{\ignorespaces PCA of Hidden layer activations from Gemma-2-9B-IT for Belief Bank Constraints}}{65}{}%
\contentsline {figure}{\numberline {A.15}{\ignorespaces PCA of MLP layer activations from Gemma-2-9B-IT for Belief Bank Constraints}}{66}{}%
\contentsline {figure}{\numberline {A.16}{\ignorespaces PCA of Attention layer activations from Llama-3.1-8B-Instruct for Belief Bank Constraints}}{67}{}%
\contentsline {figure}{\numberline {A.17}{\ignorespaces PCA of Hidden layer activations from Llama-3.1-8B-Instruct for Belief Bank Constraints}}{68}{}%
\contentsline {figure}{\numberline {A.18}{\ignorespaces PCA of MLP layer activations from Llama-3.1-8B-Instruct for Belief Bank Constraints}}{69}{}%
\contentsline {figure}{\numberline {A.19}{\ignorespaces PCA of Attention layer activations from Gemma-2-9B-IT for Halu Eval}}{70}{}%
\contentsline {figure}{\numberline {A.20}{\ignorespaces PCA of Hidden layer activations from Gemma-2-9B-IT for Halu Eval}}{71}{}%
\contentsline {figure}{\numberline {A.21}{\ignorespaces PCA of MLP layer activations from Gemma-2-9B-IT for Halu Eval}}{72}{}%
\contentsline {figure}{\numberline {A.22}{\ignorespaces PCA of Attention layer activations from Llama-3.1-8B-Instruct for Halu Eval}}{73}{}%
\contentsline {figure}{\numberline {A.23}{\ignorespaces PCA of Hidden layer activations from Llama-3.1-8B-Instruct for Halu Eval}}{74}{}%
\contentsline {figure}{\numberline {A.24}{\ignorespaces PCA of MLP layer activations from Llama-3.1-8B-Instruct for Halu Eval}}{75}{}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Belief Bank Constraints}}{78}{}%
\contentsline {figure}{\numberline {B.2}{\ignorespaces Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Belief Bank Facts}}{79}{}%
\contentsline {figure}{\numberline {B.3}{\ignorespaces Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Halu Eval}}{80}{}%
\contentsline {figure}{\numberline {B.4}{\ignorespaces Alignment between Qwen2.5-7B and Falcon3-7B-Base for Belief Bank Facts}}{81}{}%
\contentsline {figure}{\numberline {B.5}{\ignorespaces Alignment (Approach 1) between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Belief Bank Constraints}}{82}{}%
\contentsline {figure}{\numberline {B.6}{\ignorespaces Alignment (Approach 1) between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Belief Bank Facts}}{83}{}%
\contentsline {figure}{\numberline {B.7}{\ignorespaces Alignment (Approach 1) between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Halu Eval}}{84}{}%
\contentsline {figure}{\numberline {B.8}{\ignorespaces Alignment (Approach 1) between Qwen2.5-7B and Falcon3-7B-Base for Belief Bank Facts}}{85}{}%
\contentsline {figure}{\numberline {B.9}{\ignorespaces Hybrid Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Belief Bank Constraints}}{86}{}%
\contentsline {figure}{\numberline {B.10}{\ignorespaces Hybrid Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Belief Bank Facts}}{87}{}%
\contentsline {figure}{\numberline {B.11}{\ignorespaces Hybrid Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Halu Eval}}{88}{}%
\contentsline {figure}{\numberline {B.12}{\ignorespaces Hybrid Alignment between Qwen2.5-7B and Falcon3-7B-Base for Belief Bank Facts}}{89}{}%
\contentsline {figure}{\numberline {B.13}{\ignorespaces Projected Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Belief Bank Constraints}}{90}{}%
\contentsline {figure}{\numberline {B.14}{\ignorespaces Projected Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Belief Bank Facts}}{91}{}%
\contentsline {figure}{\numberline {B.15}{\ignorespaces Projected Alignment between Llama-3.1-8B-Instruct and Gemma-2-9B-IT for Halu Eval}}{92}{}%
\contentsline {figure}{\numberline {B.16}{\ignorespaces Projected Alignment between Qwen2.5-7B and Falcon3-7B-Base for Belief Bank Facts}}{93}{}%
