\chapter{Background}

This chapter introduces the fundamental concepts necessary to understand the work carried out. The concept of attention, LLMs, the hallucination phenomenon, and probing will be presented.



\section{Transformer Overview}
The self-attention mechanism was first introduced in 2017 by Vaswani et al. in the paper Attention is All You Need \cite{AttentionIsAllYouNeed} and is the foundation of the Transformer architecture, which has revolutionized the field of NLP and beyond, thanks to their ability to capture long-range relationships in sequential data more efficiently than previous models such as RNNs and their variants. The crucial difference between transformers and previous models lies in input processing: RNNs process data sequentially, maintaining a hidden state that captures previous information, while transformers use the self-attention mechanism to process the entire sequence simultaneously, allowing them to capture relationships between words regardless of their distance in the sequence. With very long inputs, RNNs tend to forget initial information while transformers do not have this problem. 
\subsection{Transformer Architecture}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth, height=0.6\textwidth]{images/Transformer.png}
    \caption{Transformer Architecture}
    \label{fig:Transformer Architecture}
\end{figure}
A transformer is composed of two main blocks: \textit{Encoder} and \textit{Decoder}.
\subsubsection{Encoder}
The encoder consists of a series of identical layers placed in sequence, each of which contains:
\begin{itemize}
    \item \textbf{Multi-Head Self-Attention}: this mechanism allows the model to focus on different parts of the input sequence simultaneously, capturing complex relationships between words.
    \item \textbf{Layer Normalization and Residual Connections}: to improve training stability and facilitate gradient flow, each sublayer is followed by normalization and a residual connection.
    \item \textbf{Feed-Forward Neural Network (FFNN)}: each encoder layer contains a fully connected feed-forward neural network that further processes the representations obtained from self-attention.
    \item Another Layer Normalization and Residual Connections layer.
\end{itemize}
The main role of the encoder is to transform the input sequence into an internal representation (embedding) that captures semantic relationships between words.

\subsubsection{Decoder}
The decoder also consists of a series of identical layers placed in sequence, each of which contains:
\begin{itemize}
    \item \textbf{Masked Multi-Head Self-Attention}: similar to the mechanism in the encoder, but with a mask that prevents the model from "looking ahead" in the output sequence during training, ensuring that word generation occurs autoregressively.
    \item \textbf{Layer Normalization and Residual Connections}: similarly to the encoder, to improve training stability.
    \item \textbf{Multi-Head Attention on Encoder Output}: this mechanism allows the decoder to focus on relevant parts of the input representation generated by the encoder.
    \item Another Layer Normalization and Residual Connections layer.
    \item \textbf{Feed-Forward Neural Network (FFNN)}: as in the encoder, to further process representations.
    \item An additional Layer Normalization and Residual Connections layer.
\end{itemize}


\subsection{Self-Attention Mechanism}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth ,height=0.4\textwidth]{images/Attn.png}
    \caption{Self-Attention Mechanism Representation}
    \label{fig:Self Attention Mechanism}
\end{figure}
The self-attention mechanism allows the model to weight the importance of different words in the input sequence when processing a particular word. Self-attention computation occurs through three distinct matrices: Query (Q), Key (K) and Value (V) \cite{AttentionIsAllYouNeed}. The formula for computing attention is as follows:
\begin{equation}
    \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
where \(d_k\) is the dimension of the keys. The softmax normalizes attention scores, allowing the model to focus on the most relevant words. The multi-head attention mechanism extends this concept by performing multiple attention computations in parallel, allowing the model to capture different types of relationships between words.


\section{Large Language Models (LLM)}
LLMs represent the natural evolution of Transformer architectures applied at massive scale. An LLM is essentially a probabilistic model trained on enormous text corpora (on the order of trillions of tokens) with the fundamental goal of predicting the next token in a sequence, given the previous context. Formally, given a sequence of tokens \( x = (x_1, x_2, ..., x_t) \), the model tries to estimate the conditional probability distribution \( P(x_{t+1} | x_1, ..., x_t) \).



The evolution from previous NLP models to current LLMs (such as the GPT series, Llama, and Claude) is characterized by the \textit{scaling law} phenomenon \cite{ScalingLaws} according to which model performance improves predictably as the number of parameters, amount of training data, and computational power increase.

The typical lifecycle of an LLM is divided into distinct phases:
\begin{enumerate}
    \item \textbf{Pre-training (unsupervised)}: This is the most computationally expensive phase. The model is trained in a self-supervised manner on vast datasets (web crawl, books, code) to learn language structure, logic, and a wide range of general knowledge. The objective is to minimize the \textit{Cross-Entropy Loss} between the predicted distribution and the actual next token.
    \item \textbf{Supervised Fine-Tuning (SFT)}: The pre-trained model (called Base) is further trained on a smaller, curated dataset of instructions and responses (prompt-response format). This allows the model to learn to follow user instructions and adopt a conversational format.
    \item \textbf{Alignment}: To ensure the model is safe and aligned with human values, techniques such as \textit{Reinforcement Learning from Human Feedback} (RLHF) or \textit{Direct Preference Optimization} (DPO) are applied. In this phase, the model is penalized if it generates toxic content, biases, or unhelpful responses.
\end{enumerate}

An emergent characteristic of LLMs is the capability of \textit{In-Context Learning}, that is, the ability to learn new tasks simply by observing some examples in the prompt, without updating model weights.

\section{Hallucinations in LLMs}
Despite their extraordinary generative capabilities, LLMs suffer from a critical limitation known as hallucination. In the context of artificial intelligence, a hallucination occurs when a model generates output that is syntactically correct and fluent, but factually incorrect, logically inconsistent, or unfaithful to the provided input source. Huang et al. \cite{surveyHallucination} define different types of hallucinations:
\begin{itemize}
    \item \textbf{Factual Hallucinations}: The model produces information that is simply false or inaccurate with respect to reality.
    \item \textbf{Logical Hallucinations}: These occur when the model generates responses that are logically inconsistent or contradictory. 
    \item \textbf{Context Inconsistencies}: These occur when the model contradicts the context provided in the input.
\end{itemize}


The causes of hallucinations are multiple and include: lossy compression of knowledge during training, noisy or contradictory training data, and the model's tendency to prioritize text fluency over factual accuracy, defined as \textit{sycophancy}, i.e., the tendency to please the user by confirming their biases \cite{sycophancyLLM}. Detecting and mitigating hallucinations is currently one of the most open challenges in LLM research.

\section{Probing}
\textit{Probing} (or \textit{Probing Classifiers}) is an analysis technique used in the field of \textit{Explainable AI} (XAI) to interpret the internal representations of deep neural networks. The fundamental idea is that, although LLMs are often considered black boxes, their internal activations (the values of hidden states and outputs of various modules at each layer) contain rich and structured information about the processed input. In particular, Marks and Tegmark \cite{Geometry} demonstrated that in LLMs the concepts of truth and falsehood emerge through linear structures in the space of the model's internal representations. It is therefore possible to use probers to probe these representations and verify whether an LLM is hallucinating or not.
The linear probing strategy (described here) is certainly the most common (an example is SAPLMA \cite{azaria2023internal}) :
\begin{enumerate}
    \item \textbf{Activation extraction}: Input is provided to the model (frozen, i.e., with frozen weights) and the activation vectors \( h_l \) generated at a specific layer \( l \) are recorded.
    \item \textbf{Prober training}: A simple supervised classifier (often a Linear Regression or a simple Multi-Layer Perceptron) is trained that takes as input the activations \( h_l \) and tries to predict a specific property \( y \) of the input (for example: part of speech, sentence length, or, in the case of this work, the truthfulness of the statement).
    \item \textbf{Evaluation}: The classifier's performance indicates how much information related to property \( y \) is encoded linearly (or non-linearly) in that particular layer of the model.
\end{enumerate}

but variants also exist.



Fadeeva et al. propose Claim-Conditioned Probability (CCP) \cite{CCP} whose central idea is to compare the probability of the given statement with that of counterfactual variants generated by masking important tokens and regenerating conditionally. A low value suggests that the model itself considers the generated statement fragile.


Hewitt et al. instead propose \textit{Structural Probing} \cite{structuralProbe} which aims to verify whether the internal representations of a language model contain information about the syntactic structure of sentences, such as dependency trees.


Manakul et al. propose \textit{SelfCheckGPT} \cite{manakul2023selfcheckgpt} which does not use activations but rather generates different responses to the same question and compares them: if they are consistent with each other, the model is not hallucinating, while if they are different, it means the model has hallucinated



