\chapter{Methodology}


In questo capitolo viene esplorata la metodologia di lavoro adottata per lo sviluppo del progetto. Per la costruzione del prober universale è stato fatto uso del dataset Belief Bank che fornisce un insieme di affermazioni e la loro veridicità in diversi contesti. Sono stati presi in considerazione cinque diversi approcci metodologici per allineare lo spazio latente di un modello student a quello di un modello teacher, dove per modello teacher si intende il modello su cui il prober viene addestrato e per modello student si intende il modello su cui il prober viene testato (dopo una eventuale fase di allineamento).
La prima fase del lavoro ha riguardato la preparazione del dataset e l'estrazione delle attivazioni interne degli LLM su tutti i layer e ogni tipologia di componente di un layer (attention, MLP, hidden states). Successivamente sono stati effettuati studi preliminari per valutare la capacità di ogni singolo componente di layer di discriminare tra affermazioni vere e allucinazioni, al fine di identificare i layer e le componenti più rilevanti per la costruzione del prober universale.
Sono state dunque calcolate alcune statistiche relative alle allucinazioni dei due modelli e tutte le attivazioni sono state preprocessate per essere utilizzate nei vari approcci metodologici.
In una prima fase è stato usato un approccio completamente lineare come baseline, per poi esplorare metodi più complessi che includono adattamenti non lineari tramite AdapterMLP, Encoder e AutoEncoder. Ogni metodologia è stata valutata in termini di prestazioni e capacità di trasferimento tra modelli diversi. 

\section{Dataset}

Il lavoro utilizza il componente fattuale del dataset \textit{BeliefBank} \cite{BeliefBank}, progettato per valutare la capacità dei modelli linguistici di mantenere credenze coerenti sul mondo.

\subsection{BeliefBank Facts}
Nello specifico, è stato utilizzato il corpus di \textit{Facts} (fatti), costituito da asserzioni in linguaggio naturale associate a un valore di verità binario (Vero/Falso).

\paragraph{Generazione e Struttura}
I fatti originali di BeliefBank coprono 85 entità distinte appartenenti al dominio naturale (animali e piante). Ogni istanza nel dataset è una frase dichiarativa semplice, come ad esempio \textit{An eagle is a bird} (Vero) o \textit{An eagle is a mammal} (Falso).
Le etichette di verità (definite \textit{silver labels}) sono state generate dagli autori originali attraverso un processo di propagazione: sono stati annotati manualmente i fatti foglia e, successivamente, le etichette sono state propagate attraverso un grafo di vincoli logici (implicazioni e mutue esclusioni) per coprire tutte le frasi collegate.

\paragraph{Composizione del Dataset Utilizzato}
Per questo esperimento, il dataset \textit{Belief Bank Facts} finale è composto da:
\begin{itemize}
    \item \textbf{Fatti affermativi:} Le asserzioni originali estratte da BeliefBank (es. $fact$).
    \item \textbf{Fatti negati:} Per ogni fatto presente, è stata generata sinteticamente la sua controparte negata (es. $\neg fact$), invertendo l'etichetta di verità originale (da Vero a Falso e viceversa), al fine di bilanciare le classi e testare la robustezza del modello.
\end{itemize}

Ogni elemento del dataset finale è quindi una tupla $(f, y)$, dove $f$ è la frase in linguaggio naturale e $y \in \{0, 1\}$ rappresenta l'etichetta di verità (dove 0 indica no/falso e 1 indica sì/vero).

Ad ogni esempio è associato un identificativo univoco (\texttt{instance\_id}) per facilitare il tracciamento e l'analisi dei risultati.
Esempio di elementi del dataset dopo la pre-elaborazione:
\begin{itemize}
    \item \textbf{instance\_id}: 38
    \item \textbf{Question}: An albatross has a talon
    \item \textbf{Answer}: Yes
\end{itemize}

Il dataset comprende 27.416 affermazioni, di cui 13.708 etichettate come vere (yes) e 13.708 come false (no), garantendo un bilanciamento perfetto tra le classi.

\subsection{Creazione dataset di attivazioni}
Una volta preparato il dataset di affermazioni e relative etichette di veridicità, il passo successivo è stato l'estrazione delle attivazioni interne dai modelli Qwen2.5-7B,Falcon3-7B-Base,Llama-3.1-8B-Instruct e gemma-2-9b-it. 
Ogni esempio è preceduto da un prompt che viene fornito al modello di linguaggio per guidarne la risposta.
Per il dataset Belief Bank Facts, il prompt utilizzato è il seguente:
\begin{quote}
You are an expert in several domains. You'll be provided with a fact, and your task is to confirm or deny its truthfulness.
Do not provide any additional information or explanations.
Just answer only with yes or no.
Is the fact true? Fact: \{question\}.
Answer:
\end{quote}
dove \{question\} viene sostituito con l'affermazione specifica da valutare.
Quindi il modello di linguaggio riceve in input il prompt seguito dalla domanda e deve rispondere con yes o no.
Per ogni affermazione nel dataset, sono state registrate le attivazioni corrispondenti a tutti i layer e componenti (attention, MLP, hidden states) dei modelli. Queste attivazioni costituiscono la base dati su cui sono stati addestrati i vari prober per il rilevamento delle allucinazioni. Per verificare se il modello risponde correttamente o allucina è stata usata una semplice logica di matching tra la risposta generata e l'etichetta di veridicità associata all'affermazione nel dataset:
se la risposta reale (yes o no) è una substring (cioè è contenuta) nella risposta generata dal modello, allora l'affermazione è considerata vera (non allucinazione); altrimenti è considerata falsa (allucinazione).


\section{LLM utilizzati}
Per gli esperimenti condotti in questo lavoro, sono stati selezionati quattro modelli open-weights che rappresentano lo stato dell'arte per le rispettive dimensioni e famiglie architetturali.

\subsection{Qwen2.5-7B}
Qwen2.5-7B è un modello sviluppato da Alibaba Cloud ed è parte della serie Qwen2.5. È un modello \textit{Decoder-only} basato su Transformer con 7 miliardi di parametri. Rispetto ai suoi predecessori, Qwen2.5 introduce diverse ottimizzazioni architetturali:
\begin{itemize}
    \item \textbf{Grouped Query Attention (GQA)}: Utilizza GQA invece della standard Multi-Head Attention per ottimizzare l'uso della memoria della cache KV durante l'inferenza, permettendo contesti più lunghi e una generazione più veloce \cite{GQA}.
    \item \textbf{SwiGLU Activation}: Sostituisce la classica funzione di attivazione GeLU con SwiGLU, che ha dimostrato empiricamente di migliorare le prestazioni di convergenza.
    \item \textbf{Rotary Positional Embeddings}: Utilizza embeddings posizionali rotativi per gestire meglio le posizioni relative dei token e supportare finestre di contesto molto ampie \cite{RoPE}.
\end{itemize}
Il modello è stato pre-addestrato su un corpus massivo e multilingua, dimostrando eccellenti capacità non solo nel ragionamento e nella comprensione del linguaggio, ma anche nel coding e nella matematica.

\subsection{Falcon3-7B-Base}
Falcon3-7B-Base è appartiene alla famiglia di modelli sviluppati dal Technology Innovation Institute (TII) di Abu Dhabi. Come Qwen, è un LLM da 7 miliardi di parametri che si basa su GQA.
Falcon3 continua la tradizione della serie Falcon concentrandosi sulla qualità dei dati di pre-training (basati sul dataset RefinedWeb, un dataset web rigorosamente filtrato e deduplicato).

\subsection{Llama-3.1-8B-Instruct} Llama-3.1-8B-Instruct rappresenta un'iterazione della famiglia di modelli open-weights sviluppata da Meta.E' dotato di 8 miliardi di parametri e \textit{Instruct}  indica che il modello è stato specificamente ottimizzato per il dialogo e il seguito di istruzioni tramite SFT e DPO.Come per i modelli precedenti, utilizza GQA per mantenere l'efficienza inferenziale riducendo il footprint della memoria della cache KV. 


\subsection{Gemma-2-9B-IT} Gemma-2-9B-IT è un modello sviluppato da Google DeepMind e fa parte della seconda generazione della serie Gemma. Con 9 miliardi di parametri, si posiziona in una fascia dimensionale leggermente superiore ai classici modelli da 7B, offrendo capacità di ragionamento competitive con modelli molto più grandi. A differenza di Llama e Qwen, Gemma 2 introduce novità architetturali specifiche che si discostano dal design standard dei Transformer: 
\begin{itemize} 
    \item \textbf{Sliding Window Attention \& Global Attention}: Il modello alterna layer di attenzione standard (globale) con layer basati su \textit{Sliding Window Attention}. 
    \item \textbf{Knowledge Distillation}: Invece del classico pre-training next-token prediction da zero, Gemma-2-9B è stato addestrato utilizzando tecniche di distillazione della conoscenza (Knowledge Distillation) a partire da un modello insegnante molto più grande, ereditandone le capacità di ragionamento in modo più efficiente. 
\end{itemize}
Come Llama-3.1-8B-Instruct, anche Gemma-2-9B-IT è stato ottimizzato per il dialogo e il seguito di istruzioni tramite SFT e DPO.


\section{Studi preliminari}
\subsection{Statistiche sulle allucinazioni}
Prima di procedere con la costruzione del prober universale, sono state calcolate alcune statistiche relative alla frequenza delle allucinazioni nei vari LLM per i dataset.
Analizzando l'output dei modelli sul dataset Belief Bank, sono state calcolate le seguenti statistiche relative alle allucinazioni:
\textcolor{red}{TODO: aggiornare stats di Qwen e Falcon e aggiungere Llama e Gemma}
\begin{table}[H]
\centering
\caption{Statistiche sulle allucinazioni per modello}
\label{tab:hallucination-stats}
\begin{tabular}{lrr}
    \toprule
    \textbf{Modello / Descrizione} & \textbf{Totale affermazioni} & \textbf{Allucinazioni} \\
    \midrule
    Qwen2.5-7B      & 27\,416 & 15\,728 \\
    Falcon3-7B-Base & 27\,416 & 16\,499 \\
\end{tabular}
\end{table}
I risultati suggeriscono che i modelli si comportano in modo molto simile, il che indica che le allucinazioni potrebbero essere legate a caratteristiche intrinseche nei modelli di linguaggio \cite{HallOpenAI} (e dunque la costruzione di un prober universale potrebbe essere possibile).

\subsection{Studio delle singole componenti di layer}

Per valutare la capacità discriminativa delle diverse componenti di layer (attention, MLP, hidden states) dei vari LLMS nel rilevamento delle allucinazioni, sono stati condotti esperimenti sistematici su ciascun layer e tipologia di attivazione. In particolare, per ogni layer e componente, sono state estratte le attivazioni corrispondenti e utilizzate come input per addestrare un classificatore Logistic Regression, con l’obiettivo di distinguere tra affermazioni vere e allucinazioni.

La pipeline sperimentale prevede:
\begin{enumerate}
\item Estrazione delle attivazioni per ogni layer e componente (attention, MLP, hidden) dai modelli su tutto il dataset.
\item Suddivisione del dataset in training e test set, mantenendo la stessa suddivisione per tutti gli esperimenti.
\item Normalizzazione delle attivazioni tramite StandardScaler.
\item Addestramento di un classificatore Logistic Regression per ciascuna combinazione layer/componente.
\item Valutazione delle prestazioni tramite metriche di accuratezza.
\end{enumerate}

I risultati mostrano che alcune componenti e layer sono particolarmente efficaci nel discriminare le allucinazioni, evidenziando la presenza di pattern informativi specifici nelle attivazioni interne dei modelli

\textcolor{red}{TODO: aggiornare stats di Qwen e Falcon e aggiungere Llama e Gemma}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Qwen2.5-7B_BeliefBank_activations.png}
    \caption{Performance dei componenti di Qwen2.5-7B su Belief Bank}
    \label{fig:qwen-layer-performance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Falcon3-7B-Base_BeliefBank_activations.png}
    \caption{Performance dei componenti di Qwen2.5-7B su Belief Bank}
    \label{fig:qwen-layer-performance}
\end{figure}


Successivamente è stata effettuata un'analisi relativa alla distribuzione di allucinazioni e risposte corrette nello spazio delle attivazioni. Affinchè fosse possibile visualizzare le attivazioni in uno spazio bidimensionale, è stata applicata la tecnica di riduzione della dimensionalità Principal Component Analysis (PCA). Seguono alcuni esempi:
\textcolor{red}{TODO: aggiornare}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/PCA_Plots/SINGLE_Falcon3-7B-Base_attn_L12_PCA.png}
    \label{fig:qwen-pca-attn}
    \caption{PCA delle attivazioni Attention del layer 12 di Falcon3-7B-Base}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/PCA_Plots/SINGLE_Qwen2.5-7B_hidden_L18_PCA.png}
    \label{fig:qwen-pca-attn}
    \caption{PCA delle attivazioni Hidden del layer 18 di Qwen2.5-7B}
\end{figure}

Come si può vedere nei layer che performano meglio presi singolarmente abbiamo una più chiara separazione tra le due classi di esempi. Per queste ragioni, per gli esperimenti successivi sono stati scelti per ogni componente i 3 layer migliori in termini di performance.
\textcolor{red}{TODO: aggiornare stats di Qwen e Falcon e aggiungere Llama e Gemma}
\begin{table}[H]
\centering
\caption{Configurazione dei layer selezionati per modello e componente}
\label{tab:layer-config}
\begin{tabular}{lccc}
\toprule
Modello & Attention (attn) & MLP & Hidden \\
\midrule
Qwen2.5-7B & 15, 16, 18 & 16, 18, 20 & 18, 19, 20 \\
Falcon3-7B-Base & 2, 7, 12 & 10, 11, 12 & 2, 3, 19 \\
\bottomrule
\end{tabular}
\end{table}


\section{Metodologie per la costruzione del prober universale}
In questa sezione vengono descritti i cinque approcci metodologici adottati per costruire un prober universale in grado di rilevare le allucinazioni nei modelli di linguaggio.

La riproducibilità degli esperimenti e l'utilizzo degli stessi set di dati per tutti i metodi sono garantiti dalla fissazione di un seed random comune e dall'uso di un campionamento deterministico del token successivo negli LLM.


Si distinguono due modalità di gestione del dataset:
\begin{itemize}
\item \textbf{Dataset Concordante (per Allineamento):} Si selezionano solo gli esempi in cui Teacher e Student concordano sulla classificazione (stessa label). Su questi viene applicato un undersampling per bilanciare le classi. Questo è essenziale per apprendere una mappatura coerente tra student e teacher
\item \textbf{Dataset Indipendente (per Probing/Autoencoder/Encoder):} Si applica un undersampling per bilanciare le classi specificamente sulle label del modello in esame (Teacher o Student), indipendentemente dall'altro modello. Questo massimizza la quantità di dati disponibili per addestrare modelli robusti.
\end{itemize}

\subsection{Baseline}
Come baseline per la costruzione del prober universale, è stato adottato un approccio lineare. In questo scenario, le attivazioni interne dei modelli, estratte dai layer e componenti selezionati, sono state utilizzate per addestrare e valutare un classificatore Logistic Regression. L'obiettivo è verificare le performance utilizzando solo metodi lineari.

\subsubsection{Pipeline sperimentale}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AppL.png}
    \caption{Pipeline sperimentale della baseline}
    \label{fig:linear-pipeline}
\end{figure}
La pipeline sperimentale consiste nell'addestramento di un classificatore Logistic Regression sullo spazio delle attivazioni del modello teacher per rilevare le allucinazioni, seguito dalla valutazione delle sue prestazioni tramite metriche di accuratezza. Successivamente, viene effettuato un allineamento lineare tra lo spazio latente del modello student e quello del modello teacher tramite una proiezione lineare (Ridge Regression) addestrata su un sottoinsieme di dati concordanti. Infine, si valuta la capacità del classificatore di discriminare le allucinazioni sui dati del modello student dopo l'allineamento.

Per il Logistic Regressor sono stati utilizzati i seguenti iperparametri:
\begin{itemize}
\item \textbf{solver}: \texttt{lbfgs}
\item \textbf{max\_iterations}: 10000
\item \textbf{class\_weight:} \texttt{balanced}
\end{itemize}

\subsection{Approccio Ibrido}

In questo approccio si esplora una procedura di adattamento non lineare dello spazio latente del modello student verso lo spazio del modello teacher mediante una rete di allineamento a bassa dimensionalità (denominata AlignmentNetwork o AdapterMLP). L'obiettivo è osservare se l'introduzione di una componente non lineare nell'allineamento migliora le performance del prober universale, mantenendo un classificatore lineare (Logistic Regression) sul modello teacher. 

\subsubsection{Architettura e loss}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AlignApp1.png}
    \caption{Architettura dell'AlignmentNetwork dell'approccio ibrido}
    \label{fig:alignmentnetwork-architecture-hybrid}
\end{figure}
Alcune caratteristiche chiave dell'AlignmentNetwork includono:
\begin{itemize}
\item \textbf{Zero-init:} gli ultimi layer della decompressione sono inizializzati a zero in modo che la rete parta vicino a una trasformazione lineare identità, permettendo alla componente non-lineare di emergere solo se realmente utile.
\item \textbf{MixedLoss:} la funzione di perdita combina Mean Squared Error (MSE) e una componente basata sulla similarità coseno:
\[
\mathcal{L} = \alpha \cdot \mathrm{MSE}(\hat{y}, y) + \beta \cdot (1 - \text{cosine\_sim}(\hat{y}, y))
\]
con pesi $\alpha$ (MSE) e $\beta$ (cosine) configurabili per bilanciare fedeltà e allineamento angolare.
\end{itemize}

dove MSE è definita come:
\[
\mathrm{MSE}(\hat{y}, y) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]
e la similarità coseno è definita come:
\[
\text{cosine\_sim}(\hat{y}, y) = \frac{\hat{y} \cdot y}{\|\hat{y}\| \|y\|} = \frac{\sum_{i=1}^{N} \hat{y}_i y_i}{\sqrt{\sum_{i=1}^{N} \hat{y}_i^2} \sqrt{\sum_{i=1}^{N} y_i^2}}
\]

\subsubsection{Pipeline sperimentale}
La pipeline sperimentale prevede innanzitutto l'addestramento di un probe (Logistic Regression) sullo spazio delle attivazioni del teacher utilizzando l'intero training set del teacher bilanciato; questo classificatore rimarrà fisso. Successivamente si addestra l'AlignmentNetwork per proiettare le attivazioni del student nello spazio del teacher, minimizzando la loss tra le attivazioni proiettate e quelle originali del teacher utilizzando il dataset bilanciato delle attivazioni concordanti.  Infine, le attivazioni di test del student vengono proiettate tramite l'AlignmentNetwork e classificate utilizzando il probe addestrato sul teacher.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/PipeApp1.png}
    \caption{Pipeline sperimentale dell'approccio ibrido}
    \label{fig:hybrid-pipeline}
\end{figure}

Per il Logistic Regressor sono stati utilizzati i seguenti iperparametri:
\begin{itemize}
\item \textbf{solver}: \texttt{lbfgs}
\item \textbf{max\_iterations}: 10000
\item \textbf{class\_weight:} \texttt{balanced}
\end{itemize}

Di seguito sono riportati gli iperparametri principali utilizzati per l'addestramento dell'AlignmentNetwork:

\begin{table}[H]
\centering
\caption{I valori degli iperparametri dell'AlignmentNetwork si applicano a tutte le combinazioni di Teacher, Student e Layer Type.}
\label{tab:hyperparameters-common}
\begin{tabular}{lr}
    \toprule
    \textbf{Parametro} & \textbf{Valore} \\
    \midrule
    Hidden Dimension & 128 \\
    Dropout & 0.5 \\
    Learning Rate (LR) & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}1$ \\
    Batch Size & 32 \\
    Early Stopping $\delta$ & $1\text{e-}4$ \\
    Gradient Clipping & 1.0 \\
    Optimizer & AdamW \\
    Scheduler & CosineAnnealingLR \\
    $\alpha$ (Loss Weight) & 0.01 \\
    $\beta$ (Loss Weight) & 1.0 \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Approccio non-lineare Completo}
Questo approccio segue il precedente per quanto riguarda la parte di allineamento tra i due LLM. La differenza risiede nel prober non-lineare (MLP) come classificatore sul teacher. L'obiettivo è verificare le performance utilizzando entrambe le componenti non-lineari seguendo la stessa pipeline sperimentale.

\subsubsection{Architettura, componenti e Loss}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AlignApp1.png}
    \caption{Architettura dell'AlignmentNetwork dell'approccio non-lineare completo}
    \label{fig:alignmentnetwork-architecture-full}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/MLPApp1.png}
    \caption{Architettura del Prober MLP dell'approccio non-lineare completo}
    \label{fig:mlp-prober-architecture-full}
\end{figure}

\begin{itemize}
\item \textbf{AlignmentNetwork:} Viene utilizzata la stessa architettura descritta nell'approccio precedente con la stessa loss.
\item \textbf{MLP Prober:} Classificatore non-lineare per rilevamento della allucinazione: rete fully-connected con layer intermedi normalizzati (LayerNorm), attivazione GELU e dropout.
\item \textbf{BCEWithLogitsLoss:} combina una funzione sigmoide e la Binary Cross Entropy per stabilità numerica.
\end{itemize}

\subsubsection{Pipeline sperimentale}
La pipeline sperimentale è dunque identica a quella dell'approccio ibrido, con la differenza che il classificatore sul teacher è ora un MLP Prober non-lineare.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/PipeApp1.png}
    \caption{Pipeline sperimentale dell'approccio non-lineare completo}
    \label{fig:non-linear-complete-pipeline}    
\end{figure}

Di seguito sono riportati gli iperparametri principali utilizzati per l'addestramento dell'AlignmentNetwork e del Prober MLP:


\begin{table}[H]
\centering
\caption{Iperparametri dell'AlignmentNetwork per l'approccio non-lineare completo.}
\label{tab:hyperparams-align-loss-clean}
\begin{tabular}{lr}
    \toprule
    \textbf{Parametro} & \textbf{Valore} \\
    \midrule
    \textit{Alignment Network} & \\
    Hidden Dimension & 128 \\
    Dropout & 0.5 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}1$ \\
    Batch Size & 32 \\
    Early Stopping Patience & 50 \\
    \midrule
    \textit{Loss Function} & \\
    $\alpha$ (Reconstruction) & 0.01 \\
    $\beta$ (Contrastive) & 1.0 \\
    \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Iperparametri del Prober MLP per l'approccio non-lineare completo.}
\label{tab:hyperparams-prober-clean}
\begin{tabular}{lr}
    \toprule
    \textbf{Parametro} & \textbf{Valore} \\
    \midrule
    Hidden Dimension & 64 \\
    Dropout & 0.5 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}2$ \\
    Batch Size & 64 \\
    Early Stopping Patience & 30 \\
    Optimizer & AdamW \\
    Scheduler & CosineAnnealingLR \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Approccio non-lineare ridotto}

Questo approccio riduce la dimensionalità delle attivazioni tramite autoencoder prima dell'allineamento.

\subsubsection{Architettura e componenti principali}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AutoEncApp2.png}
    \caption{Architettura dell'autoencoder dell'approccio non-lineare ridotto}
    \label{fig:autoencoder-architecture}
\end{figure}    

\begin{itemize}
\item \textbf{Autoencoder (Teacher \& Student)}: si addestra un autoencoder separato sullo spazio delle attivazioni del teacher e uno su quello dello student. Gli encoder producono rappresentazioni latenti di dimensione comune (es. 128). La loss utilizzata è la Mean Squared Error (MSE).
\item \textbf{Alignment Network (latent)}: Mappa lo spazio latente dello student a quello del teacher.
\item \textbf{MLP Prober}: Classifica operando sullo spazio latente ridotto del teacher.
\end{itemize}

\subsubsection{Pipeline sperimentale}
La pipeline prevede innanzitutto l'addestramento di due autoencoder: uno sulle attivazioni del teacher e l'altro su quelle dello student, ottenendo così due spazi latenti distinti ($Z_T$ e $Z_S$). Successivamente, le attivazioni vengono codificate nei rispettivi spazi latenti. Sullo spazio latente del teacher viene addestrato un classificatore MLP. In seguito, si addestra una Alignment Network che mappa le rappresentazioni latenti dello student ($Z_S$) nello spazio latente del teacher ($Z_T$) utilizzando i dati concordanti. Infine, per la valutazione, il classificatore MLP addestrato sul teacher viene utilizzato per classificare le rappresentazioni dello student dopo l'allineamento e la riduzione dimensionale.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/PipeApp2.png}
    \caption{Pipeline sperimentale dell'approccio non-lineare ridotto}
    \label{fig:non-linear-reduced-pipeline}
\end{figure}

Di seguito sono riportati gli iperparametri principali aggiornati utilizzati in questo approccio:


\begin{table}[H]
\centering
\caption{Iperparametri dell'AutoEncoder (Teacher e Student).}
\label{tab:hyperparams-autoencoder-clean}
\begin{tabular}{lr}
    \toprule
    \textbf{Parametro} & \textbf{Valore} \\
    \midrule
    Latent Dimension & 128 \\
    Hidden Dimension & 256 \\
    Dropout & 0.2 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}2$ \\
    Batch Size & 64 \\
    Loss & MSELoss \\
    \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Iperparametri dell'AlignmentNetwork per l'approccio ridotto.}
\label{tab:hyperparams-alignment-clean-reduced}
\begin{tabular}{lr}
    \toprule
    \textbf{Parametro} & \textbf{Valore} \\
    \midrule
    \textit{Network Parameters} & \\
    Hidden Dimension & 256 \\
    Dropout & 0.3 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}2$ \\
    Batch Size & 32 \\
    \midrule
    \textit{Loss Weights} & \\
    $\alpha$ (Reconstruction) & 0.5 \\
    $\beta$ (Contrastive) & 0.5 \\
    \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Iperparametri del Prober MLP (su spazio latente).}
\label{tab:hyperparams-prober-clean-reduced}
\begin{tabular}{lr}
    \toprule
    \textbf{Parametro} & \textbf{Valore} \\
    \midrule
    Hidden Dimension & 64 \\
    Dropout & 0.3 \\
    Learning Rate & $1\text{e-}3$ \\
    Batch Size & 64 \\
    Loss & BCEWithLogitsLoss \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Approccio One‑For‑All (Frozen Head)}

In questo approccio si adotta una procedura in due fasi pensata per valutare la trasferibilità di una funzione decisionale (la \emph{head} di classificazione) tra due modelli differenti mantenendo la stessa testa e adattando soltanto l'encoder dello student. A differenza degli approcci precedenti, questo metodo non richiede dati concordanti per l'allineamento, ma utilizza dataset bilanciati indipendenti per ciascun modello.

\subsubsection{Architettura e componenti principali}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/modelsss/EncoderApp3.png}
    \caption{Architettura encoder One-For-All}
    \label{fig:OfA-architecture}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/modelsss/MLPApp3.png}
    \caption{Architettura Classification Head One-For-All}
    \label{fig:OfA-head-architecture}
\end{figure}

\begin{itemize}
\item \textbf{Encoder:} rete feed‑forward a più blocchi che mappa lo spazio di input (dimensione attivazione) in uno spazio latente di dimensione ridotta (256).
\item \textbf{Classification Head:} modulo compatto che prende in input il vettore latente e restituisce un logit binario. Implementato come una piccola MLP. 
\end{itemize}

\subsubsection{Pipeline sperimentale}
La pipeline sperimentale prevede innanzitutto l'addestramento congiunto dell'encoder e della head di classificazione sullo spazio delle attivazioni del modello teacher (End-to-End). Una volta completato l'addestramento, la head viene congelata (\emph{Frozen Head}) per mantenere invariata la funzione decisionale appresa. Successivamente, si procede con l'addestramento di un nuovo encoder sullo spazio delle attivazioni del modello student: l'output di questo nuovo encoder viene passato alla head congelata del teacher, e la loss viene calcolata direttamente sull'errore di classificazione (BCEWithLogitsLoss). In questo modo, lo student impara a produrre rappresentazioni latenti compatibili con la "testa" del teacher.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/modelsss/PipeApp3.png}
    \caption{Pipeline sperimentale One-For-All}
    \label{fig:OfA-pipeline}
\end{figure}

Di seguito sono riportati gli iperparametri principali utilizzati per l'addestramento degli Encoder e della Classification Head:

\begin{table}[H]
\centering
\caption{Iperparametri dell'Encoder (Teacher e Student Adapter).}
\label{tab:hyperparams-OfA-encoder-clean}
\begin{tabular}{lr}
    \toprule
    \textbf{Parametro} & \textbf{Valore} \\
    \midrule
    Latent Dimension & 256 \\
    Hidden Dimension & 512 \\
    Dropout & 0.3 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}2$ \\
    Batch Size & 64 \\
    Early Stopping Patience & 15 \\
    Optimizer & AdamW \\
    \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Iperparametri della Classification Head (Shared/Frozen).}
\label{tab:hyperparams-OfA-head-clean}
\begin{tabular}{lr}
    \toprule
    \textbf{Parametro} & \textbf{Valore} \\
    \midrule
    Latent Dimension (Input) & 256 \\
    Hidden Dimension & 128 \\
    Dropout & 0.3 \\
    Learning Rate & $1\text{e-}3$ \\
    Batch Size & 64 \\
    \bottomrule
\end{tabular}
\end{table}