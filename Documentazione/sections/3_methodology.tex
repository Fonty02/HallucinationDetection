\chapter{Methodology}


This chapter explores the methodology adopted for the project development. For building the universal prober, the datasets Belief Bank Facts \cite{BeliefBank}, Belief Bank Constraints \cite{BeliefBank} and Halu Eval \cite{Halueval} were used. Five different methodological approaches were considered to align the latent space of a tester model to that of a trainer model, where trainer model refers to the model whose activations are used to train the prober model, and tester model refers to the model whose activations are used to test the prober's transferability (after a possible alignment phase).
The first phase of the work involved dataset preparation and extraction of internal activations from the LLMs across all layers and each type of layer component (attention, MLP, hidden states). Subsequently, preliminary studies were conducted to evaluate the ability of each individual layer component to discriminate between true statements and hallucinations, in order to identify the most relevant layers and components for building the universal prober.
Some statistics related to hallucinations of the two models were then calculated, and all activations were preprocessed to be used in the various methodological approaches.
In an initial phase, a completely linear approach was used, then more complex methods were explored including non-linear adaptations via AdapterMLP, Encoder and AutoEncoder. Each methodology was evaluated in terms of performance and transfer capability between different models. 

\section{Dataset}

\subsection{Datasets Used}

\subsubsection{Belief Bank Facts}
This dataset was chosen as it can be used to detect \textit{Factual Hallucinations}.
The original BeliefBank facts cover 85 distinct entities belonging to the natural domain (animals and plants). Each instance in the dataset is a simple declarative sentence, such as \textit{An eagle is a bird} (True) or \textit{An eagle is a mammal} (False).
The truth labels (called \textit{silver labels}) were generated by the authors. 

The final dataset is composed of:
\begin{itemize}
    \item \textbf{Affirmative facts:} The original assertions extracted from BeliefBank (e.g., $fact$).
    \item \textbf{Negated facts:} For each present fact, its negated counterpart was synthetically generated (e.g., $\neg fact$), inverting the original truth label (from True to False and vice versa), in order to balance the classes and test model robustness.
\end{itemize}

with a total of 27,416 statements, of which 13,708 labeled as true (yes) and 13,708 as false (no), ensuring perfect balance between classes.

\subsubsection{Belief Bank Constraints}
This dataset was chosen as it can be used to detect \textit{Logical Inconsistencies}.
The original BeliefBank constraints were manually created by the authors and represent logical relationships between statements. 
The dataset is composed of:
\begin{itemize}
    \item \textbf{Positive implications:} Representation of logical implications between statements (if $A$ then $B$).
    \item \textbf{Mutual exclusions:} Representation of mutual exclusion constraints between statements (not both $A$ and $B$).
\end{itemize}
For each element, the negated version was also inserted to balance the classes. In total there are 25,756 statements to verify.

\subsubsection{HaluEval} This dataset was chosen to detect hallucinations in complex conversational contexts. Unlike simple facts, here each instance includes a context composed of dialogue history and supporting external knowledge.

The dataset is composed of: 
\begin{itemize} 
    \item \textbf{Correct Responses:} Factual responses consistent with the knowledge provided in the context (Right Response). 
    \item \textbf{Hallucinated Responses:} Synthetically generated responses that seem plausible but contain unverified or contradictory information with respect to the provided knowledge (Hallucinated Response). 
\end{itemize}

The final dataset is composed of 10,000 examples.


\subsection{Activation Dataset Creation}
Once the dataset of statements and their truthfulness labels was prepared, the next step was extracting internal activations from the models Qwen2.5-7B \cite{qwen2.5}, Falcon3-7B-Base \cite{Falcon3}, Llama-3.1-8B-Instruct \cite{LLama3} and gemma-2-9b-it \cite{Gemma2}. 
Each example is preceded by a prompt that is provided to the language model to guide its response.
For the Belief Bank Facts and Belief Bank Constraints datasets, the prompt used is as follows:
\begin{quote}
You are an expert in several domains. You'll be provided with a fact, and your task is to confirm or deny its truthfulness.
Do not provide any additional information or explanations.
Just answer only with yes or no.
Is the fact true? Fact: \{question\}.
Answer:
\end{quote}
where \{question\} is replaced with the specific statement to evaluate.

\noindent For the Halu Eval dataset, the prompt used is as follows:
\begin{quote}
    SYS\_FACT = You are an expert in several domains. You'll be provided with a fact, and your task is to confirm or deny its truthfulness.
Do not provide any additional information or explanations.
Just answer only with \"yes\" or \"no\".
 
USR\_FACT = Is the fact true? Fact: {question}
Answer:

\end{quote}


Therefore, the LLM receives the prompt followed by the question as input and must respond with "yes" or "no".
For each statement in the dataset, the activations corresponding to all layers and components (attention, MLP, hidden states) of the models were recorded. These activations constitute the database on which the various probers for hallucination detection were trained. To verify whether the model responds correctly or hallucinates, a simple matching logic was used between the generated response and the truthfulness label associated with the statement in the dataset:
if the actual response (yes or no) is a substring (i.e., is contained) in the model's generated response, then the statement is considered true (non-hallucination); otherwise it is considered false (hallucination).

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/save-acts.pdf}
    \caption{Pipeline for extraction and saving of internal LLM activations}
    \label{fig:save_acts}
\end{figure}

\section{LLMs Used}
For the experiments conducted in this work, four open-weights models were selected representing the state of the art for their respective sizes and architectural families.

\subsection{Qwen2.5-7B}
Qwen2.5-7B is a model developed by Alibaba Cloud and is part of the Qwen2.5 series. It is a \textit{Decoder-only} Transformer-based model with 7 billion parameters. Compared to its predecessors, Qwen2.5 introduces several architectural optimizations:
\begin{itemize}
    \item \textbf{Grouped Query Attention (GQA)}: Uses GQA instead of standard Multi-Head Attention to optimize Key-Value (KV) cache memory usage during inference, allowing longer contexts and faster generation \cite{GQA}.
    \item \textbf{SwiGLU Activation}: Replaces the classic GeLU activation function with SwiGLU, which has empirically demonstrated improved convergence performance \cite{SWIGLU}.
    \item \textbf{Rotary Positional Embeddings}: Uses rotary positional embeddings to better handle relative token positions and support very large context windows \cite{RoPE}.
\end{itemize}
The model was pre-trained on a massive multilingual corpus, demonstrating excellent capabilities not only in reasoning and language understanding, but also in coding and mathematics.

\subsection{Falcon3-7B-Base}
Falcon3-7B-Base belongs to the family of models developed by the Technology Innovation Institute (TII) of Abu Dhabi. Like Qwen, it is an LLM with 7 billion parameters based on GQA.
Falcon3 continues the tradition of the Falcon series by focusing on pre-training data quality (based on the RefinedWeb dataset \cite{refinedweb}, a rigorously filtered and deduplicated web dataset).

\subsection{Llama-3.1-8B-Instruct} Llama-3.1-8B-Instruct represents an iteration of the open-weights model family developed by Meta. It has 8 billion parameters and \textit{Instruct} indicates that the model was specifically optimized for dialogue and instruction following through SFT and DPO. Like previous models, it uses GQA to maintain inferential efficiency by reducing the KV cache memory footprint. 


\subsection{Gemma-2-9B-IT} Gemma-2-9B-IT is a model developed by Google DeepMind and is part of the second generation of the Gemma series. With 9 billion parameters, it sits in a slightly higher dimensional range than classic 7B models, offering reasoning capabilities competitive with much larger models. Unlike Llama and Qwen, Gemma 2 introduces specific architectural novelties that deviate from standard Transformer design: 
\begin{itemize} 
    \item \textbf{Sliding Window Attention \& Global Attention}: The model alternates standard (global) attention layers with \textit{Sliding Window Attention} based layers. 
    \item \textbf{Knowledge Distillation}: Instead of classic next-token prediction pre-training from scratch, Gemma-2-9B was trained using Knowledge Distillation techniques from a much larger teacher model, inheriting its reasoning capabilities more efficiently. 
\end{itemize}
Like Llama-3.1-8B-Instruct, Gemma-2-9B-IT was also optimized for dialogue and instruction following through SFT and DPO.


\section{Preliminary Studies}
\subsection{Hallucination Statistics}
Before proceeding with the construction of the universal prober, some statistics were calculated regarding the frequency of hallucinations in the various LLMs for the datasets.
Analyzing the model outputs on the datasets, the statistics visible in tables \ref{tab:hallucination-stats-bbf}, \ref{tab:hallucination-stats-bbc} and \ref{tab:hallucination-stats-he} were calculated.
\begin{table}[H]
\centering
\caption{Hallucination statistics for Belief Bank Facts}
    \footnotesize
\label{tab:hallucination-stats-bbf}
\begin{tabular}{|lrrr|}
    \toprule
    \textbf{Model} & \textbf{Total statements} & \textbf{Hallucinations} & \textbf{Hallucination Rate} \\
    \midrule
    Qwen2.5-7B      & 27\,416 & 3\,565 & 13.0\% \\
    \rowcolor{red!25}Falcon3-7B-Base & 27\,416 & 7\,531 & 27.47\% \\
    Llama-3.1-8B-Instruct & 27\,416 & 1\,799 & 6.56\% \\
    \rowcolor{green!25}Gemma-2-9B-IT   & 27\,416 & 802 & 2.93\% \\
    \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
    \footnotesize
\caption{Hallucination statistics for Belief Bank Constraints}
\label{tab:hallucination-stats-bbc}
\begin{tabular}{|lrrr|}
    \toprule
    \textbf{Model} & \textbf{Total statements} & \textbf{Hallucinations} & \textbf{Hallucination Rate} \\
    \midrule
    \rowcolor{red!25}Llama-3.1-8B-Instruct & 25\,068& 14\,026 & 55.95\% \\
    \rowcolor{green!25}Gemma-2-9B-IT   & 25\,756 & 12\,641 & 49.1\% \\
    \bottomrule
\end{tabular}
\end{table}

\textcolor{blue}{This is not my mistake, for LLama some activations are missing} \textcolor{red}{oh shit, on gandalf? possible?}


\begin{table}[H]
\centering
    \footnotesize
\caption{Hallucination statistics for Halu Eval}
\label{tab:hallucination-stats-he}
\begin{tabular}{|lrrr|}
    \toprule
    \textbf{Model} & \textbf{Total statements} & \textbf{Hallucinations} & \textbf{Hallucination Rate} \\
    \midrule
    \rowcolor{green!25}Llama-3.1-8B-Instruct & 10\,000 & 2\,391 & 23.91\% \\
    \rowcolor{red!25}Gemma-2-9B-IT   & 10\,000 & 2\,747 & 27.47\% \\
    \bottomrule
\end{tabular}
\end{table}



The results suggest that in general, with the same dataset, models behave similarly, which indicates that hallucinations could be related to intrinsic characteristics in language models as stated by Kalai et al. in \cite{HallOpenAI} (and therefore building a universal prober could be possible).

If we compare results between the three different datasets, we observe that models tend to produce many more \textit{Logical Inconsistencies}

\subsection{Studio delle singole componenti di layer}

Per valutare la capacità discriminativa delle diverse componenti di layer (attention, MLP, hidden states) dei vari LLMS nel rilevamento delle allucinazioni, sono stati condotti esperimenti sistematici su ciascun layer e tipologia di attivazione. In particolare, per ogni layer e componente, sono state estratte le attivazioni corrispondenti e utilizzate come input per addestrare un classificatore Logistic Regression, con l’obiettivo di distinguere tra affermazioni vere e allucinazioni.

La pipeline sperimentale prevede:
\begin{enumerate}
\item Estrazione delle attivazioni per ogni layer e componente (attention, MLP, hidden) dai modelli su tutto il dataset.
\item Undersampling e suddivisione del dataset in training e test set, mantenendo la stessa suddivisione per tutti gli esperimenti.
\item Normalizzazione delle attivazioni tramite StandardScaler.
\item Training of a Logistic Regression classifier for each layer/component combination.
\item Evaluation of performance through accuracy metrics.
\end{enumerate}

The results show that some components and layers are particularly effective in discriminating hallucinations, highlighting the presence of specific informative patterns in the internal activations of models

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Qwen2.5-7B_belief_bank_facts_activations.pdf}
    \caption{Performance of Qwen2.5-7B components on Belief Bank Facts}
    \label{fig:qwen-layer-performance-facts}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Falcon3-7B-Base_belief_bank_facts_activations.pdf}
    \caption{Performance of Falcon3-7B-Base components on Belief Bank Facts}
    \label{fig:qwen-layer-performance-facts}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Llama-3.1-8B-Instruct_belief_bank_facts_activations.pdf}
    \caption{Performance of Llama-3.1-8B-Instruct components on Belief Bank Facts}
    \label{fig:llama-layer-performance-facts}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Gemma-2-9B-IT_belief_bank_facts_activations.pdf}
    \caption{Performance of Gemma-2-9B-IT components on Belief Bank Facts}
    \label{fig:gemma-layer-performance-facts}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Llama-3.1-8B-Instruct_belief_bank_constraints_activations.pdf}
    \caption{Performance of Llama-3.1-8B-Instruct components on Belief Bank Constraints}
    \label{fig:llama-layer-performance-constraints}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Gemma-2-9B-IT_belief_bank_constraints_activations.pdf}
    \caption{Performance of Gemma-2-9B-IT components on Belief Bank Constraints}
    \label{fig:gemma-layer-performance-constraints}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Llama-3.1-8B-Instruct_halu_eval_activations.pdf}
    \caption{Performance of Llama-3.1-8B-Instruct components on Halu Eval}
    \label{fig:llama-layer-performance-halu}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/Gemma-2-9B-IT_halu_eval_activations.pdf}
    \caption{Performance of Gemma-2-9B-IT components on Halu Eval}
    \label{fig:gemma-layer-performance-halu}
\end{figure}



Subsequently, an analysis was performed regarding the distribution of hallucinations and correct responses in the activation space. In order to visualize the activations in a two-dimensional space, the dimensionality reduction technique Principal Component Analysis (PCA) \cite{PCA} was applied. Some examples follow:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/PCA_Plots/SINGLE_Falcon3-7B-Base_attn_L18_PCA.pdf}
    \label{fig:falcon-pca-attn-18}
    \caption{PCA of Attention activations of layer 18 of Falcon3-7B-Base}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.3\textheight, keepaspectratio]{images/PCA_Plots/SINGLE_Qwen2.5-7B_attn_L14_PCA.pdf}
    \label{fig:qwen-pca-attn-l14}
    \caption{PCA of Attention activations of layer 14 of Qwen2.5-7B}
\end{figure}

For subsequent experiments, the 3 best layers in terms of performance were chosen for each component \ref{tab:layer-config-belief-bank-facts}, \ref{tab:layer-config-belief-bank-constraints}, \ref{tab:layer-config-halu-eval}.
\begin{table}[H]
\centering
\caption{Layers chosen for Belief Bank Facts}
\label{tab:layer-config-belief-bank-facts}
\begin{tabular}{|lccc|}
\toprule
Model & Attn & MLP & Hidden \\
\midrule
Qwen2.5-7B & 14, 15, 17 & 14, 23, 25 & 15, 16, 17 \\
Falcon3-7B-Base & 18, 19, 26 & 18, 19, 20 & 17, 18, 21 \\
Llama-3.1-8B-Instruct & 8, 13, 14 & 14, 15, 21 & 14, 15, 16 \\
Gemma-2-9B-IT & 21, 24, 27 & 22, 25, 27 & 23, 26, 34 \\

\bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Layers chosen for Belief Bank Constraints}
\label{tab:layer-config-belief-bank-constraints}
\begin{tabular}{|lccc|}
\toprule
Model & Attn & MLP & Hidden \\
\midrule
Llama-3.1-8B-Instruct & 5, 8, 12 & 12, 14, 15 & 13, 14, 15 \\
Gemma-2-9B-IT & 23, 27, 33 & 24, 25, 26 & 23, 24, 27 \\

\bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Layers chosen for Halu Eval}
\label{tab:layer-config-halu-eval}
\begin{tabular}{|lccc|}
\toprule
Model & Attn & MLP & Hidden \\
\midrule
Llama-3.1-8B-Instruct & 14, 15, 16 & 13, 14, 15 & 14, 15, 16 \\
Gemma-2-9B-IT & 21, 26, 27 & 23, 24, 28 & 19, 24, 28 \\

\bottomrule
\end{tabular}
\end{table}

We can notice an interesting trend: for LLama, layers between 13 and 16 seem to be the most informative, while for Gemma-2-9B-IT the higher layers (between 23 and 28) are those that offer the best performance. This suggests that the position of the most relevant layers can vary significantly between different models, probably due to architectural differences and specific training processes of each model. However, it can be noted that intermediate layers are the most informative in every model.

\section{Methodologies for Building the Universal Prober}
This section describes the five methodological approaches adopted to build a universal prober capable of detecting hallucinations in language models.

Experiment reproducibility and the use of the same data sets for all methods are guaranteed by setting a common random seed and using deterministic next token sampling in LLMs.


Two dataset management modes are distinguished:
\begin{itemize}
\item \textbf{Concordant Dataset (for Alignment):} Only examples where Trainer and Tester agree on classification (same label) are selected. Undersampling is applied to these to balance classes. This is essential for learning a coherent mapping between tester and trainer \textcolor{red}{\\ I'm writing here, but it's just for reference: I was thinking, can we plot a before and after alignment of activations? to understand if somehow they get closer?} \textcolor{blue}{Appendix B}
\item \textbf{Independent Dataset (for Probing/Autoencoder/Encoder):} Undersampling is applied to balance classes specifically on the labels of the model in question (Trainer or Tester), independently from the other model. This maximizes the amount of data available for training robust models.
\end{itemize}

\subsection{FullLinear Approach}
The first approch for building the universal prober, a linear approach was adopted. In this scenario, the internal activations of the models, extracted from the selected layers and components, were used to train and evaluate a Logistic Regression classifier. The objective is to verify performance using only linear methods.

\subsubsection{Experimental Pipeline}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AppL.png}
    \caption{FullLinear experimental pipeline}
    \label{fig:linear-pipeline}
\end{figure}
The experimental pipeline in figure \ref{fig:linear-pipeline} consists of training a Logistic Regression classifier on the activation space of the trainer model to detect hallucinations, followed by evaluating its performance through accuracy metrics. Subsequently, a linear alignment is performed between the latent space of the tester model and that of the trainer model via a linear projection (Ridge Regression) trained on a subset of concordant data. Finally, the classifier's ability to discriminate hallucinations on tester model data after alignment is evaluated.

For the Logistic Regressor, the following hyperparameters were used:
\begin{itemize}
\item \textbf{solver}: \texttt{lbfgs}
\item \textbf{max\_iterations}: 10000
\item \textbf{class\_weight:} \texttt{balanced}
\end{itemize}

\subsection{AdapterMLP Approach}

In this approach, a non-linear adaptation procedure of the tester model's latent space towards the trainer model's space is explored using a low-dimensional alignment network (called AlignmentNetwork or AdapterMLP). The objective is to observe whether introducing a non-linear component in the alignment improves universal prober performance, while maintaining a linear classifier (Logistic Regression) on the trainer model. 

\subsubsection{Architecture and Loss}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AlignApp1.png}
    \caption{AlignmentNetwork architecture of the AdapterMLP approach}
    \label{fig:alignmentnetwork-architecture-hybrid}
\end{figure}
Some key characteristics of the AlignmentNetwork include: 
\begin{itemize}
\item \textbf{Zero-init:} the last layers of the decompression are initialized to zero so that the network starts close to a linear identity transformation, allowing the non-linear component to emerge only if truly useful.
\item \textbf{MixedLoss:} the loss function combines Mean Squared Error (MSE) and a component based on cosine similarity:
\[
\mathcal{L} = \alpha \cdot \mathrm{MSE}(\hat{y}, y) + \beta \cdot (1 - \text{cosine\_sim}(\hat{y}, y))
\]
with configurable weights $\alpha$ (MSE) and $\beta$ (cosine) to balance fidelity and angular alignment.
\end{itemize}

where MSE is defined as:
\[
\mathrm{MSE}(\hat{y}, y) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]
and cosine similarity is defined as:
\[
\text{cosine\_sim}(\hat{y}, y) = \frac{\hat{y} \cdot y}{\|\hat{y}\| \|y\|} = \frac{\sum_{i=1}^{N} \hat{y}_i y_i}{\sqrt{\sum_{i=1}^{N} \hat{y}_i^2} \sqrt{\sum_{i=1}^{N} y_i^2}}
\]

\subsubsection{Experimental Pipeline}
The experimental pipeline in figure \ref{fig:hybrid-pipeline} first involves training a probe (Logistic Regression) on the trainer's activation space using the entire balanced trainer training set; this classifier will remain fixed. Subsequently, the AlignmentNetwork is trained to project tester activations into the trainer's space, minimizing the loss between projected activations and original trainer activations using the balanced concordant activations dataset. Finally, the tester's test activations are projected via the AlignmentNetwork and classified using the probe trained on the trainer.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AppH.png}
    \caption{AdapterMLP approach experimental pipeline}
    \label{fig:hybrid-pipeline}
\end{figure}

For the Logistic Regressor, the following hyperparameters were used:
\begin{itemize}
\item \textbf{solver}: \texttt{lbfgs}
\item \textbf{max\_iterations}: 10000
\item \textbf{class\_weight:} \texttt{balanced}
\end{itemize}

In table \ref{tab:hyperparameters-common}, the main hyperparameters used for training the AlignmentNetwork are reported:

\begin{table}[H]
\centering
\caption{AlignmentNetwork hyperparameter values apply to all combinations of Trainer, Tester and Layer Type.}
\label{tab:hyperparameters-common}
\begin{tabular}{|lr|}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Hidden Dimension & 128 \\
    Dropout & 0.5 \\
    Learning Rate (LR) & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}1$ \\
    Batch Size & 32 \\
    Early Stopping $\delta$ & $1\text{e-}4$ \\
    Gradient Clipping & 1.0 \\
    Optimizer & AdamW \\
    Scheduler & CosineAnnealingLR \\
    $\alpha$ (Loss Weight) & 0.01 \\
    $\beta$ (Loss Weight) & 1.0 \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Full Non-linear Approach}
This approach follows the previous one regarding the alignment part between the two LLMs. The difference lies in the non-linear prober (MLP) as classifier on the trainer. The objective is to verify performance using both non-linear components following the same experimental pipeline.

\subsubsection{Architecture, Components and Loss}



\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/MLPApp1.png}
    \caption{MLP Prober architecture of the full non-linear approach}
    \label{fig:mlp-prober-architecture-full}
\end{figure}

\begin{itemize}
\item \textbf{AlignmentNetwork:} The same architecture described in the previous approach is used with the same loss
\item \textbf{MLP Prober in figure \ref{fig:mlp-prober-architecture-full}:} Non-linear classifier for hallucination detection: fully-connected network with normalized intermediate layers (LayerNorm), GELU activation and dropout.
\item \textbf{BCEWithLogitsLoss:} combines a sigmoid function and Binary Cross Entropy for numerical stability.
\end{itemize}

BCEWithLogitsLoss is defined as:
\[
\mathcal{L}(x, y) = - \left( y \cdot \log(\sigma(x)) + (1 - y) \cdot \log(1 - \sigma(x)) \right)
\]
where $\sigma(x)$ is the sigmoid function:
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]


\subsubsection{Experimental Pipeline}
The experimental pipeline in figure \ref{fig:non-linear-complete-pipeline} is therefore identical to that of the AdapterMLP approach, with the difference that the classifier on the trainer is now a non-linear MLP Prober.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/PipeApp1.png}
    \caption{Full non-linear approach experimental pipeline}
    \label{fig:non-linear-complete-pipeline}    
\end{figure}

In tables \ref{tab:hyperparams-align-loss-clean} and \ref{tab:hyperparams-prober-clean}, the main hyperparameters used for training the AlignmentNetwork and MLP Prober are reported:


\begin{table}[H]
\centering
\caption{AlignmentNetwork hyperparameters for the full non-linear approach.}
\label{tab:hyperparams-align-loss-clean}
\begin{tabular}{|lr|}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    \textit{Alignment Network} & \\
    Hidden Dimension & 128 \\
    Dropout & 0.5 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}1$ \\
    Batch Size & 32 \\
    Early Stopping Patience & 50 \\
    \midrule
    \textit{Loss Function} & \\
    $\alpha$ (Reconstruction) & 0.01 \\
    $\beta$ (Contrastive) & 1.0 \\
    \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{MLP Prober hyperparameters for the full non-linear approach.}
\label{tab:hyperparams-prober-clean}
\begin{tabular}{|lr|}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Hidden Dimension & 64 \\
    Dropout & 0.5 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}2$ \\
    Batch Size & 64 \\
    Early Stopping Patience & 30 \\
    Optimizer & AdamW \\
    Scheduler & CosineAnnealingLR \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Reduced Non-linear Approach}

This approach reduces activation dimensionality via autoencoder before alignment.

\subsubsection{Architecture and Main Components}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AutoEncApp2.png}
    \caption{Autoencoder architecture of the reduced non-linear approach}
    \label{fig:autoencoder-architecture}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/MLPApp2.png}
    \caption{MLP Prober architecture of the reduced non-linear approach}
    \label{fig:mlp-prober-architecture-reduced}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/AlignApp2.png}
    \caption{AlignmentNetwork architecture of the reduced non-linear approach}
    \label{fig:alignmentnetwork-architecture-reduced}
\end{figure}

\begin{itemize}
\item \textbf{Autoencoder (Trainer \& Tester) in figure \ref{fig:autoencoder-architecture}}: a separate autoencoder is trained on the trainer's activation space and one on the tester's. The encoders produce latent representations of common dimension. The loss used is Mean Squared Error (MSE).
\item \textbf{Alignment Network (latent) in figure \ref{fig:alignmentnetwork-architecture-reduced}}: Maps the tester's latent space to the trainer's.
\item \textbf{MLP Prober in figure \ref{fig:mlp-prober-architecture-reduced}}: Classifies operating on the reduced latent space of the trainer.
\end{itemize}

\subsubsection{Experimental Pipeline}
The experimental pipeline in figure \ref{fig:non-linear-reduced-pipeline} first involves training two autoencoders: one on the trainer's activations and the other on the tester's, thus obtaining two distinct latent spaces. Subsequently, the activations are encoded in their respective latent spaces. An MLP classifier is trained on the trainer's latent space. Then, an Alignment Network is trained that maps the tester's latent representations to the trainer's latent space using concordant data. Finally, for evaluation, the MLP classifier trained on the trainer is used to classify the tester's representations after alignment and dimensionality reduction.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.4\textheight, keepaspectratio]{images/modelsss/PipeApp2.png}
    \caption{Reduced non-linear approach experimental pipeline}
    \label{fig:non-linear-reduced-pipeline}
\end{figure}

In tables \ref{tab:hyperparams-autoencoder-clean}, \ref{tab:hyperparams-alignment-clean-reduced} and \ref{tab:hyperparams-prober-clean-reduced}, the main updated hyperparameters used in this approach are reported:


\begin{table}[H]
\centering
\caption{AutoEncoder hyperparameters (Trainer and Tester).}
\label{tab:hyperparams-autoencoder-clean}
\begin{tabular}{|lr|}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Latent Dimension & 128 \\
    Hidden Dimension & 256 \\
    Dropout & 0.2 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}2$ \\
    Batch Size & 64 \\
    Loss & MSELoss \\
    \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{AlignmentNetwork hyperparameters for the reduced approach.}
\label{tab:hyperparams-alignment-clean-reduced}
\begin{tabular}{|lr|}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    \textit{Network Parameters} & \\
    Hidden Dimension & 256 \\
    Dropout & 0.3 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}2$ \\
    Batch Size & 32 \\
    \midrule
    \textit{Loss Weights} & \\
    $\alpha$ (Reconstruction) & 0.5 \\
    $\beta$ (Contrastive) & 0.5 \\
    \bottomrule
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{MLP Prober hyperparameters (on latent space).}
\label{tab:hyperparams-prober-clean-reduced}
\begin{tabular}{|lr|}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Hidden Dimension & 64 \\
    Dropout & 0.3 \\
    Learning Rate & $1\text{e-}3$ \\
    Batch Size & 64 \\
    Loss & BCEWithLogitsLoss \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{One-For-All Approach (Frozen Head)}

In this approach, a two-phase procedure is adopted, designed to evaluate the transferability of a decision function (the classification \emph{head}) between two different models while keeping the same head and adapting only the tester's encoder. Unlike previous approaches, this method does not require concordant data for alignment but uses independent balanced datasets for each model.

\subsubsection{Architecture and main components}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/modelsss/EncoderApp3.png}
    \caption{One-For-All encoder architecture}
    \label{fig:OfA-architecture}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/modelsss/MLPApp3.png}
    \caption{One-For-All Classification Head architecture}
    \label{fig:OfA-head-architecture}
\end{figure}

\begin{itemize}
\item \textbf{Encoder in Figure \ref{fig:OfA-architecture}:} multi-block feed-forward network that maps the input space (activation dimension) to a reduced-dimension latent space (256).
\item \textbf{Classification Head in Figure \ref{fig:OfA-head-architecture}:} compact module that takes the latent vector as input and returns a binary logit. Implemented as a small MLP.
\end{itemize}

\subsubsection{Experimental pipeline}
The experimental pipeline in Figure \ref{fig:OfA-pipeline} first involves joint training of the encoder and classification head on the trainer model's activation space (End-to-End). Once training is complete, the head is frozen (\emph{Frozen Head}) to keep the learned decision function unchanged. Subsequently, a new encoder is trained on the tester model's activation space: the output of this new encoder is passed to the trainer's frozen head, and the loss is calculated directly on the classification error (BCEWithLogitsLoss). In this way, the tester learns to produce latent representations compatible with the trainer's "head".

\textcolor{blue}{The classification head is trained only once with the trainer, then frozen and reused for the tester. So yes: if reused on the trainer it maintains the same results} \textcolor{red}{perfect, from the figure I understood it was re-trained, sorry}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/modelsss/PipeApp3.png}
    \caption{One-For-All experimental pipeline}
    \label{fig:OfA-pipeline}
\end{figure}

Tables \ref{tab:hyperparams-OfA-encoder-clean} and \ref{tab:hyperparams-OfA-head-clean} report the main hyperparameters used for training the Encoders and Classification Head:

\begin{table}[H]
\centering
\caption{Encoder hyperparameters (Trainer and Tester Adapter).}
\label{tab:hyperparams-OfA-encoder-clean}
\begin{tabular}{|lr|}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Latent Dimension & 256 \\
    Hidden Dimension & 512 \\
    Dropout & 0.3 \\
    Learning Rate & $1\text{e-}3$ \\
    Weight Decay & $1\text{e-}2$ \\
    Batch Size & 64 \\
    Early Stopping Patience & 15 \\
    Optimizer & AdamW \\
    \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Classification Head hyperparameters (Shared/Frozen).}
\label{tab:hyperparams-OfA-head-clean}
\begin{tabular}{|lr|}
    \toprule
    \textbf{Parameter} & \textbf{Value} \\
    \midrule
    Latent Dimension (Input) & 256 \\
    Hidden Dimension & 128 \\
    Dropout & 0.3 \\
    Learning Rate & $1\text{e-}3$ \\
    Batch Size & 64 \\
    \bottomrule
\end{tabular}
\end{table}