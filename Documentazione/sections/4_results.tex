\chapter{Risultati}

In questo capitolo vengono presentati i risultati ottenuti dagli esperimenti condotti per valutare l'efficacia del prober universale nel rilevamento delle allucinazioni negli LLM. In particolare viene riportata una tabella per ogni coppia (trainer-tester)-Dataset in modo da analizzare il miglior approccio per ogni combinazione.

\section{Falcon3-7B-Base e Qwen2.5-7B su BeliefBankFacts}



\begin{table}[H]
\scriptsize
\centering
\caption{Risultati per coppia (Falcon3-7B-Base, Qwen2.5-7B) su BeliefBankFacts}
\label{tab:results-bbf-falcon-qwen}
\begin{tabular}{|l l l l| c c c c|}
\hline
Trainer & Tester & Approach & Type & AccT & AccS & AurocT & AurocS \\
\hline
Qwen2.5-7B & Falcon3-7B-Base & Baseline & attn  & 0.987 & 0.962 & 0.999 & 0.992 \\
Qwen2.5-7B & Falcon3-7B-Base & Baseline & mlp   & 0.986 & 0.967 & 0.999 & 0.987 \\
Qwen2.5-7B & Falcon3-7B-Base & Baseline & hidden& 0.985 & 0.967 & 0.999 & 0.990 \\
Falcon3-7B-Base & Qwen2.5-7B & Baseline & attn  & 0.987 & 0.958 & 0.997 & 0.989 \\
Falcon3-7B-Base & Qwen2.5-7B & Baseline & mlp   & 0.983 & 0.961 & 0.996 & 0.989 \\
Falcon3-7B-Base & Qwen2.5-7B & Baseline & hidden& 0.983 & 0.959 & 0.996 & 0.984 \\
\hline
Qwen2.5-7B & Falcon3-7B-Base & HApproach & attn  & 0.987 & 0.968 & 0.999 & 0.984 \\
Qwen2.5-7B & Falcon3-7B-Base & HApproach & mlp   & 0.986 & 0.969 & 0.999 & 0.986 \\
Qwen2.5-7B & Falcon3-7B-Base & HApproach & hidden& 0.985 & 0.968 & 0.999 & 0.990 \\
\rowcolor{red!25}Falcon3-7B-Base & Qwen2.5-7B & HApproach & attn  & 0.987 & 0.848 & 0.997 & 0.912 \\
Falcon3-7B-Base & Qwen2.5-7B & HApproach & mlp   & 0.983 & 0.913 & 0.996 & 0.967 \\
Falcon3-7B-Base & Qwen2.5-7B & HApproach & hidden& 0.983 & 0.889 & 0.996 & 0.922 \\
\hline
Qwen2.5-7B & Falcon3-7B-Base & FullNonLinear & attn  & 0.993 & 0.981 & 1.000 & 0.994 \\
Qwen2.5-7B & Falcon3-7B-Base & FullNonLinear & mlp   & 0.994 & 0.977 & 1.000 & 0.994 \\
Qwen2.5-7B & Falcon3-7B-Base & FullNonLinear & hidden& 0.992 & 0.981 & 1.000 & 0.996 \\
Falcon3-7B-Base & Qwen2.5-7B & FullNonLinear & attn  & 0.992 & 0.890 & 0.999 & 0.938 \\
Falcon3-7B-Base & Qwen2.5-7B & FullNonLinear & mlp   & 0.988 & 0.941 & 0.999 & 0.988 \\
Falcon3-7B-Base & Qwen2.5-7B & FullNonLinear & hidden& 0.991 & 0.931 & 0.999 & 0.985 \\
\hline
Qwen2.5-7B & Falcon3-7B-Base & ReducedNonLinear & attn  & 0.9916 & 0.9706 & 0.9994 & 0.9953 \\
Qwen2.5-7B & Falcon3-7B-Base & ReducedNonLinear & mlp   & 0.9897 & 0.9608 & 0.9997 & 0.9915 \\
Qwen2.5-7B & Falcon3-7B-Base & ReducedNonLinear & hidden& 0.9935 & 0.9586 & 0.9997 & 0.9892 \\
Falcon3-7B-Base & Qwen2.5-7B & ReducedNonLinear & attn  & 0.9841 & 0.8537 & 0.9983 & 0.9367 \\
Falcon3-7B-Base & Qwen2.5-7B & ReducedNonLinear & mlp   & 0.9807 & 0.9411 & 0.9971 & 0.9832 \\
Falcon3-7B-Base & Qwen2.5-7B & ReducedNonLinear & hidden& 0.9770 & 0.9004 & 0.9952 & 0.9503 \\
\hline
Qwen2.5-7B & Falcon3-7B-Base & One-For-All & attn  & 0.9892 & 0.9916 & 0.9995 & 0.9992 \\
Qwen2.5-7B & Falcon3-7B-Base & One-For-All & mlp   & 0.9860 & 0.9858 & 0.9993 & 0.9980 \\
Qwen2.5-7B & Falcon3-7B-Base & One-For-All & hidden& 0.9906 & 0.9898 & 0.9998 & 0.9991 \\
Falcon3-7B-Base & Qwen2.5-7B & One-For-All & attn  & 0.9905 & 0.9921 & 0.9985 & 0.9997 \\
Falcon3-7B-Base & Qwen2.5-7B & One-For-All & mlp   & 0.9869 & 0.9874 & 0.9985 & 0.9997 \\
\rowcolor{green!25}Falcon3-7B-Base & Qwen2.5-7B & One-For-All & hidden& 0.9889 & 0.9949 & 0.9988 & 0.9999 \\
\hline
\end{tabular}
\end{table}



\section{LLama-3.1-8B-Instruct e Gemma-2-9B-IT su BeliefBankFacts}
\begin{table}[H]
\scriptsize
\centering
\caption{Risultati per coppia (LLama-3.1-8B-Instruct, Gemma-2-9B-IT) su BeliefBankFacts}
\label{tab:results-bbf-llama-gemma}
\begin{tabular}{|l l l l| c c c c|}
\hline
Trainer & Tester & Approach & Type & AccT & AccS & AurocT & AurocS \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & attn  & 0.965 & 0.919 & 0.986 & 0.974 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & mlp   & 0.963 & 0.928 & 0.988 & 0.977 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & hidden& 0.959 & 0.941 & 0.988 & 0.982 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & attn  & 0.982 & 0.959 & 0.996 & 0.986 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & mlp   & 0.973 & 0.948 & 0.997 & 0.983 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & hidden& 0.969 & 0.946 & 0.996 & 0.983 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & attn  & 0.965 & 0.901 & 0.986 & 0.961 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & mlp   & 0.9627 & 0.9074 & 0.9883 & 0.9624 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & hidden& 0.9585 & 0.9204 & 0.9878 & 0.9752 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & attn  & 0.982 & 0.938 & 0.996 & 0.974 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & mlp   & 0.973 & 0.936 & 0.997 & 0.975 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & hidden& 0.969 & 0.934 & 0.996 & 0.975 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & attn  & 0.975 & 0.938 & 0.994 & 0.976 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & mlp   & 0.954 & 0.917 & 0.990 & 0.967 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & hidden& 0.961 & 0.929 & 0.987 & 0.969 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & attn  & 0.984 & 0.950 & 0.998 & 0.982 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & mlp   & 0.986 & 0.961 & 0.998 & 0.983 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & hidden& 0.981 & 0.954 & 0.998 & 0.984 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & attn  & 0.963 & 0.900 & 0.993 & 0.970 \\
\rowcolor{red!25}gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & mlp   & 0.952 & 0.888 & 0.988 & 0.955 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & hidden& 0.940 & 0.904 & 0.988 & 0.959 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & attn  & 0.986 & 0.932 & 0.999 & 0.983 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & mlp   & 0.970 & 0.923 & 0.995 & 0.971 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & hidden& 0.964 & 0.903 & 0.994 & 0.969 \\
\hline
\rowcolor{green!25}gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & attn  & 0.9627 & 0.9861 & 0.9923 & 0.9972 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & mlp   & 0.9564 & 0.9843 & 0.9820 & 0.9967 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & hidden& 0.9502 & 0.9843 & 0.9898 & 0.9976 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & attn  & 0.9861 & 0.9627 & 0.9982 & 0.9894 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & mlp   & 0.9852 & 0.9523 & 0.9984 & 0.9874 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & hidden& 0.9778 & 0.9606 & 0.9950 & 0.9891 \\
\hline
\end{tabular}
\end{table}

\section{LLama-3.1-8B-Instruct e Gemma-2-9B-IT su BeliefBankCalibration}
\begin{table}[H]
\scriptsize
\centering
\caption{Risultati per coppia (LLama-3.1-8B-Instruct, Gemma-2-9B-IT) su BeliefBankCalibration}
\label{tab:results-bbc-llama-gemma}
\begin{tabular}{|l l l l| c c c c|}
\hline
Trainer & Tester & Approach & Type & AccT & AccS & AurocT & AurocS \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & attn & 0.9713 & 0.9665 & 0.9950 & 0.9926 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & mlp & 0.9727 & 0.9725 & 0.9953 & 0.9946 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & hidden & 0.9744 & 0.9742 & 0.9953 & 0.9949 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & attn & 0.9697 & 0.9715 & 0.9955 & 0.9936 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & mlp & 0.9730 & 0.9698 & 0.9958 & 0.9933 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & hidden & 0.9736 & 0.9699 & 0.9960 & 0.9936 \\
\hline
\rowcolor{red!25}gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & attn & 0.9713 & 0.7972 & 0.9950 & 0.8789 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & mlp & 0.9727 & 0.8723 & 0.9953 & 0.9352 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & hidden & 0.9744 & 0.8790 & 0.9953 & 0.9440 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & attn & 0.9697 & 0.8820 & 0.9955 & 0.9424 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & mlp & 0.9730 & 0.8982 & 0.9958 & 0.9554 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & hidden & 0.9736 & 0.8903 & 0.9960 & 0.9521 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & attn & 0.9848 & 0.8541 & 0.9982 & 0.9093 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & mlp & 0.9842 & 0.9016 & 0.9984 & 0.9397 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & hidden & 0.9856 & 0.9042 & 0.9986 & 0.9510 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & attn & 0.9893 & 0.8991 & 0.9990 & 0.9512 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & mlp & 0.9829 & 0.9395 & 0.9984 & 0.9756 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & hidden & 0.9848 & 0.9111 & 0.9983 & 0.9546 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & attn & 0.9713 & 0.9096 & 0.9955 & 0.9786 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & mlp & 0.9680 & 0.8485 & 0.9950 & 0.9544 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & hidden & 0.9719 & 0.8367 & 0.9959 & 0.9687 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & attn & 0.9673 & 0.8816 & 0.9958 & 0.9683 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & mlp & 0.9679 & 0.9279 & 0.9942 & 0.9785 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & hidden & 0.9651 & 0.9438 & 0.9938 & 0.9824 \\
\hline
\rowcolor{green!25}gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & attn & 0.9843 & 0.9885 & 0.9985 & 0.9986 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & attn & 0.9878 & 0.9844 & 0.9989 & 0.9979 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & mlp & 0.9847 & 0.9807 & 0.9980 & 0.9971 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & mlp & 0.9793 & 0.9855 & 0.9978 & 0.9986 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & hidden & 0.9873 & 0.9828 & 0.9984 & 0.9957 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & hidden & 0.9840 & 0.9838 & 0.9985 & 0.9983 \\
\hline
\end{tabular}
\end{table}

\section{LLama-3.1-8B-Instruct e Gemma-2-9B-IT su HaluEval}
\begin{table}[H]
\scriptsize
\centering
\caption{Risultati per coppia (LLama-3.1-8B-Instruct, Gemma-2-9B-IT) su HaluEval}
\label{tab:results-he-llama-gemma}
\begin{tabular}{|l l l l| c c c c|}
\hline
Trainer & Tester & Approach & Type & AccT & AccS & AurocT & AurocS \\
\hline
\rowcolor{green!25}gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & attn & 0.6962 & 0.7884 & 0.7670 & 0.8525 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & mlp & 0.7010 & 0.7799 & 0.7582 & 0.8559 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & Baseline & hidden & 0.6955 & 0.7647 & 0.7715 & 0.8380 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & attn & 0.7568 & 0.7540 & 0.8231 & 0.8161 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & mlp & 0.7599 & 0.7519 & 0.8297 & 0.8218 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & Baseline & hidden & 0.7508 & 0.7408 & 0.8173 & 0.8138 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & attn & 0.6962 & 0.7508 & 0.7670 & 0.8113 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & mlp & 0.7010 & 0.7483 & 0.7582 & 0.8133 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & HApproach & hidden & 0.6955 & 0.7544 & 0.7715 & 0.8199 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & attn & 0.7568 & 0.7157 & 0.8231 & 0.7680 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & mlp & 0.7599 & 0.7185 & 0.8297 & 0.7908 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & HApproach & hidden & 0.7508 & 0.7352 & 0.8173 & 0.7882 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & attn & 0.7226 & 0.7605 & 0.7897 & 0.8335 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & mlp & 0.7310 & 0.7677 & 0.7968 & 0.8417 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & FullNonLinear & hidden & 0.7178 & 0.7647 & 0.7934 & 0.8350 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & attn & 0.7799 & 0.7247 & 0.8528 & 0.7906 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & mlp & 0.7720 & 0.7415 & 0.8515 & 0.7952 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & FullNonLinear & hidden & 0.7708 & 0.7463 & 0.8470 & 0.8099 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & attn & 0.7038 & 0.7053 & 0.7720 & 0.7883 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & mlp & 0.7010 & 0.6695 & 0.7832 & 0.7445 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & ReducedNonLinear & hidden & 0.6941 & 0.6956 & 0.7663 & 0.7729 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & attn & 0.7404 & 0.6509 & 0.8293 & 0.7177 \\
\rowcolor{red!25}Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & mlp & 0.7544 & 0.6272 & 0.8309 & 0.6985 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & ReducedNonLinear & hidden & 0.7465 & 0.6523 & 0.8354 & 0.7145 \\
\hline
gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & attn & 0.7094 & 0.7762 & 0.7683 & 0.8553 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & attn & 0.7793 & 0.7010 & 0.8502 & 0.7680 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & mlp & 0.7066 & 0.7629 & 0.7728 & 0.8445 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & mlp & 0.7696 & 0.7045 & 0.8492 & 0.7666 \\
gemma-2-9b-it & Llama-3.1-8B-Instruct & One-For-All & hidden & 0.7129 & 0.7768 & 0.7715 & 0.8484 \\
Llama-3.1-8B-Instruct & gemma-2-9b-it & One-For-All & hidden & 0.7574 & 0.7052 & 0.8371 & 0.7714 \\
\hline
\end{tabular}
\end{table}


