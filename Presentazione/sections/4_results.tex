\section{Risultati}

\begin{frame}{Risultati: Falcon3 e Qwen2.5 su BeliefBankFacts}
    \framesubtitle{Qwen2.5 $\rightarrow$ Falcon3 (Tutti gli approcci)}
    \begin{table}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lllcc}
            \toprule
            \textbf{Trainer $\rightarrow$ Tester} & \textbf{Approach} & \textbf{Type} & \textbf{AUROC (Tr)} & \textbf{AUROC (Te)} \\
            \midrule
            Qwen $\rightarrow$ Falcon & Baseline & attn & 0.999 & 0.992 \\
            Qwen $\rightarrow$ Falcon & HApproach & attn & 0.999 & 0.984 \\
            Qwen $\rightarrow$ Falcon & FullNonLinear & attn & 1.000 & 0.994 \\
            Qwen $\rightarrow$ Falcon & ReducedNonLinear & attn & 0.999 & 0.995 \\
            \textbf{Qwen $\rightarrow$ Falcon} & \textbf{One-For-All} & \textbf{attn} & \textbf{1.000} & \textbf{0.999} \\
            \bottomrule
        \end{tabular}
        }
        \caption{Confronto Approcci (Qwen $\rightarrow$ Falcon)}
        \scriptsize Tr=Trainer, Te=Tester
    \end{table}
    \begin{itemize}
        \item \textbf{One-For-All} e \textbf{FullNonLinear} ottengono i risultati migliori.
        \item \textbf{Baseline} è già molto forte in questo caso.
    \end{itemize}
\end{frame}

\begin{frame}{Risultati: Llama-3.1 e Gemma-2 su BeliefBankFacts}
    \framesubtitle{Gemma-2 $\rightarrow$ Llama-3.1 (Tutti gli approcci)}
    \begin{table}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lllcc}
            \toprule
            \textbf{Trainer $\rightarrow$ Tester} & \textbf{Approach} & \textbf{Type} & \textbf{AUROC (Tr)} & \textbf{AUROC (Te)} \\
            \midrule
            Gemma $\rightarrow$ Llama & Baseline & attn & 0.986 & 0.974 \\
            Gemma $\rightarrow$ Llama & HApproach & attn & 0.986 & 0.961 \\
            Gemma $\rightarrow$ Llama & FullNonLinear & attn & 0.994 & 0.976 \\
            Gemma $\rightarrow$ Llama & ReducedNonLinear & attn & 0.993 & 0.970 \\
            \textbf{Gemma $\rightarrow$ Llama} & \textbf{One-For-All} & \textbf{attn} & \textbf{0.992} & \textbf{0.997} \\
            \bottomrule
        \end{tabular}
        }
        \caption{Confronto Approcci (Gemma $\rightarrow$ Llama)}
        \scriptsize Tr=Trainer, Te=Tester
    \end{table}
    \begin{itemize}
        \item \textbf{One-For-All} mostra un netto miglioramento rispetto alla Baseline.
        \item Gli approcci non lineari (Full/Reduced) non superano One-For-All.
    \end{itemize}
\end{frame}

\begin{frame}{Risultati: Llama-3.1 e Gemma-2 su HaluEval}
    \framesubtitle{Gemma-2 $\rightarrow$ Llama-3.1 (Tutti gli approcci)}
    \begin{table}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lllcc}
            \toprule
            \textbf{Trainer $\rightarrow$ Tester} & \textbf{Approach} & \textbf{Type} & \textbf{AUROC (Tr)} & \textbf{AUROC (Te)} \\
            \midrule
            Gemma $\rightarrow$ Llama & Baseline & attn & 0.767 & 0.853 \\
            Gemma $\rightarrow$ Llama & HApproach & attn & 0.767 & 0.811 \\
            Gemma $\rightarrow$ Llama & FullNonLinear & attn & 0.790 & 0.834 \\
            Gemma $\rightarrow$ Llama & ReducedNonLinear & attn & 0.772 & 0.788 \\
            \textbf{Gemma $\rightarrow$ Llama} & \textbf{One-For-All} & \textbf{attn} & \textbf{0.768} & \textbf{0.855} \\
            \bottomrule
        \end{tabular}
        }
        \caption{Confronto Approcci su HaluEval}
        \scriptsize Tr=Trainer, Te=Tester
    \end{table}
    \begin{itemize}
        \item Su dataset complessi, \textbf{Baseline} rimane molto competitiva.
        \item \textbf{One-For-All} mantiene buone performance ma il gap è ridotto.
    \end{itemize}
\end{frame}

\begin{frame}{Risultati}
    \textbf{Cross-Domain: One-For-All}
    \begin{table}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lllcccccc}
            \toprule
            \textbf{Train Dataset} & \textbf{Test Dataset} & \textbf{Trainer} & \textbf{Tester} & \textbf{Layer} & \textbf{Acc (Tr)} & \textbf{AUROC (Tr)} & \textbf{Acc (Te)} & \textbf{AUROC (Te)} \\
            \midrule
            BBF & HE & Gemma & Llama & attn & 0.985 & 0.993 & 0.995 & 0.997 \\
            BBF & HE & Llama & Gemma & attn & 0.994 & 0.998 & 0.986 & 0.997 \\
            HE & BBF & Gemma & Llama & attn & 0.877 & 0.925 & 0.906 & 0.950 \\
            HE & BBF & Llama & Gemma & attn & 0.907 & 0.948 & 0.885 & 0.932 \\
            \bottomrule
        \end{tabular}
        }
        \caption{Sintesi risultati Cross-Domain (Layer: attn)}
        \scriptsize Tr=Trainer, Te=Tester. Train Dataset = dataset su cui è addestrato l'encoder.
    \end{table}
    \textbf{Osservazione}: L'encoder addestrato su task semplici (BBF) generalizza bene a task complessi (HE), mentre il viceversa funziona peggio.
\end{frame}