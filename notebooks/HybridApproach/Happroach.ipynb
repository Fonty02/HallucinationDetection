{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c201d1",
   "metadata": {},
   "source": [
    "# Hybrid: Non-linear adaptation with AdapterMLP\n",
    "\n",
    " In this notebook a first non-linear approach is tested. We take all the activations of both LLMs, we train 3 classifiers (1 per type of layer) for the Teacher model, with an AdapterMLP we try to adapt the Student latent space to the Teacher one. Finally we test the adapted Student activations with the Teacher classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a7846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "import traceback\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "# ==================================================================\n",
    "# DEVICE CONFIGURATION\n",
    "# ==================================================================\n",
    "DEVICE = torch.device(\"cuda:2\")\n",
    "\n",
    "# ==================================================================\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# ==================================================================\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def get_generator(seed=SEED):\n",
    "    \"\"\"Create a reproducible generator for DataLoader\"\"\"\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9b08416",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "HF_DEFAULT_HOME = os.environ.get(\"HF_HOME\", \"~\\\\.cache\\\\huggingface\\\\hub\")\n",
    "\n",
    "# Nomi dei modelli (usati come costanti in tutto il notebook)\n",
    "MODEL_A = \"gemma-2-9b-it\"\n",
    "MODEL_B = \"Llama-3.1-8B-Instruct\"\n",
    "\n",
    "LAYER_CONFIG = {\n",
    "    MODEL_A: \n",
    "    {\n",
    "        \"attn\": [21,24,27],\n",
    "        \"mlp\":[22,25,27],\n",
    "        \"hidden\": [23,26,34]\n",
    "    },    \n",
    "    MODEL_B: \n",
    "    {\n",
    "        \"attn\": [8,13,14],\n",
    "        \"mlp\":[14,15,21],\n",
    "        \"hidden\": [14,15,16]\n",
    "    }  \n",
    "}\n",
    "DATASET_NAME = \"belief_bank_facts\"\n",
    "\n",
    "\n",
    "ALIGNMENT_CONFIG = {\n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout\": 0.5,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_epochs\": 1000,\n",
    "    \"early_stopping_patience\": 50,\n",
    "    \"early_stopping_min_delta\": 1e-4,\n",
    "    \"gradient_clip_max_norm\": 1.0,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss_alpha\": 0.01,  # MSE weight\n",
    "    \"loss_beta\": 1.0     # Cosine weight\n",
    "}\n",
    "\n",
    "\n",
    "PROBE_CONFIG = {\n",
    "    \"type\": \"LogisticRegression\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"solver\": \"lbfgs\",\n",
    "    \"n_jobs\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c5698d",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50926a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALISI CONCORDANZA E UNDERSAMPLING\n",
      "============================================================\n",
      "gemma-2-9b-it totali: 27416, hallucinated: 802\n",
      "Llama-3.1-8B-Instruct totali: 27416, hallucinated: 1799\n",
      "\n",
      "  Campioni concordanti: 25749 / 27416\n",
      "    - Hallucinated (concordanti): 467\n",
      "    - Non-hallucinated (concordanti): 25282\n",
      "  Dopo undersampling: 934 campioni bilanciati (467 per classe)\n"
     ]
    }
   ],
   "source": [
    "def stats_per_json(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Versione originale per la vecchia struttura con hallucination_labels.json\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"generations\", \"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    hallucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_items': hallucinated_items,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "def stats_from_new_structure(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Nuova funzione per la struttura con cartelle hallucinated/ e not_hallucinated/\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    not_hallucinated_path = os.path.join(base_path, \"not_hallucinated\")\n",
    "    \n",
    "    hall_ids_path = os.path.join(hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    not_hall_ids_path = os.path.join(not_hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    \n",
    "    with open(hall_ids_path, 'r') as f:\n",
    "        hallucinated_ids = json.load(f)\n",
    "    with open(not_hall_ids_path, 'r') as f:\n",
    "        not_hallucinated_ids = json.load(f)\n",
    "    \n",
    "    total = len(hallucinated_ids) + len(not_hallucinated_ids)\n",
    "    hallucinations = len(hallucinated_ids)\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'not_hallucinations': len(not_hallucinated_ids),\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_ids': hallucinated_ids,\n",
    "        'not_hallucinated_ids': not_hallucinated_ids,\n",
    "        'hallucinated_items': hallucinated_ids,  # Alias per compatibilità\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "def detect_structure_type(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Rileva automaticamente se la struttura è vecchia o nuova.\n",
    "    Ritorna 'new' se esistono le cartelle hallucinated/not_hallucinated,\n",
    "    altrimenti 'old'.\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    if os.path.isdir(hallucinated_path):\n",
    "        return 'new'\n",
    "    return 'old'\n",
    "\n",
    "def get_stats(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Funzione wrapper che rileva automaticamente la struttura e chiama la funzione appropriata.\n",
    "    \"\"\"\n",
    "    structure = detect_structure_type(model_name, dataset_name)\n",
    "    if structure == 'new':\n",
    "        return stats_from_new_structure(model_name, dataset_name)\n",
    "    else:\n",
    "        return stats_per_json(model_name, dataset_name)\n",
    "\n",
    "\n",
    "def get_concordant_indices_and_undersample(stats_model1, stats_model2, seed=SEED):\n",
    "    \"\"\"\n",
    "    Trova gli indici dove ENTRAMBI i modelli concordano sull'etichetta,\n",
    "    poi applica undersampling per bilanciare le classi.\n",
    "    \n",
    "    Returns:\n",
    "        concordant_indices: array di indici concordanti e bilanciati\n",
    "        labels: array di label corrispondenti (0=non-hallucinated, 1=hallucinated)\n",
    "    \"\"\"\n",
    "    total_samples = stats_model1['total']\n",
    "    assert stats_model1['total'] == stats_model2['total'], \"I due modelli devono avere lo stesso numero di campioni\"\n",
    "    \n",
    "    # Costruisci array di label per entrambi i modelli\n",
    "    hall_set_1 = set(stats_model1['hallucinated_items'])\n",
    "    hall_set_2 = set(stats_model2['hallucinated_items'])\n",
    "    \n",
    "    y1 = np.array([1 if i in hall_set_1 else 0 for i in range(total_samples)])\n",
    "    y2 = np.array([1 if i in hall_set_2 else 0 for i in range(total_samples)])\n",
    "    \n",
    "    # Trova campioni CONCORDANTI (stessa label in entrambi i modelli)\n",
    "    concordant_mask = (y1 == y2)\n",
    "    concordant_indices = np.where(concordant_mask)[0]\n",
    "    concordant_labels = y1[concordant_indices]  # uguale a y2[concordant_indices]\n",
    "    \n",
    "    # Conta per classe\n",
    "    n_hall = np.sum(concordant_labels == 1)\n",
    "    n_non_hall = np.sum(concordant_labels == 0)\n",
    "    \n",
    "    print(f\"  Campioni concordanti: {len(concordant_indices)} / {total_samples}\")\n",
    "    print(f\"    - Hallucinated (concordanti): {n_hall}\")\n",
    "    print(f\"    - Non-hallucinated (concordanti): {n_non_hall}\")\n",
    "    \n",
    "    # Undersampling sulla classe maggioritaria\n",
    "    min_count = min(n_hall, n_non_hall)\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    hall_concordant = concordant_indices[concordant_labels == 1]\n",
    "    non_hall_concordant = concordant_indices[concordant_labels == 0]\n",
    "    \n",
    "    hall_sampled = rng.choice(hall_concordant, size=min_count, replace=False)\n",
    "    non_hall_sampled = rng.choice(non_hall_concordant, size=min_count, replace=False)\n",
    "    \n",
    "    # Combina e shuffle\n",
    "    balanced_indices = np.concatenate([hall_sampled, non_hall_sampled])\n",
    "    balanced_labels = np.concatenate([np.ones(min_count, dtype=np.int8), np.zeros(min_count, dtype=np.int8)])\n",
    "    \n",
    "    # Shuffle finale\n",
    "    shuffle_idx = rng.permutation(len(balanced_indices))\n",
    "    balanced_indices = balanced_indices[shuffle_idx]\n",
    "    balanced_labels = balanced_labels[shuffle_idx]\n",
    "    \n",
    "    print(f\"  Dopo undersampling: {len(balanced_indices)} campioni bilanciati ({min_count} per classe)\")\n",
    "    \n",
    "    return balanced_indices, balanced_labels\n",
    "\n",
    "\n",
    "# Carica stats e calcola indici concordanti bilanciati\n",
    "model_a_stats = get_stats(MODEL_A, DATASET_NAME)\n",
    "model_b_stats = get_stats(MODEL_B, DATASET_NAME)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ANALISI CONCORDANZA E UNDERSAMPLING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{MODEL_A} totali: {model_a_stats['total']}, hallucinated: {model_a_stats['hallucinations']}\")\n",
    "print(f\"{MODEL_B} totali: {model_b_stats['total']}, hallucinated: {model_b_stats['hallucinations']}\")\n",
    "print()\n",
    "\n",
    "balanced_indices, balanced_labels = get_concordant_indices_and_undersample(model_a_stats, model_b_stats, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9a72e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. Dataset class\n",
    "# ------------------------------------------------------------------\n",
    "class AlignmentDataset(Dataset):\n",
    "    def __init__(self, x_source: torch.Tensor, x_target: torch.Tensor):\n",
    "        # Ora assumiamo che i dati siano già torch.Tensor\n",
    "        self.x_source = x_source\n",
    "        self.x_target = x_target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_source.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_source[idx], self.x_target[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. AlignmentNetwork\n",
    "# ------------------------------------------------------------------\n",
    "class AlignmentNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 128, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        if input_dim != output_dim:\n",
    "            self.input_proj = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        else:\n",
    "            self.input_proj = nn.Identity()\n",
    "\n",
    "        # Ramo Non-Lineare (Bottleneck Estremo)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim), # Compressione forte (es. 10000 -> 128)\n",
    "            nn.LayerNorm(hidden_dim),          # Normalizzazione\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),               # Dropout aggressivo (0.5)\n",
    "            nn.Linear(hidden_dim, output_dim), # Decompressione\n",
    "            nn.Dropout(dropout)                # Dropout finale\n",
    "        )\n",
    "        \n",
    "        # Zero-Init per partire come una funzione lineare pura\n",
    "        self._init_zero()\n",
    "\n",
    "    def _init_zero(self):\n",
    "        nn.init.zeros_(self.net[-2].weight)\n",
    "        if self.net[-2].bias is not None:\n",
    "            nn.init.zeros_(self.net[-2].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_base = self.input_proj(x)\n",
    "        return x_base + self.net(x_base)\n",
    "\n",
    "\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.01, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Peso per MSE\n",
    "        self.beta = beta    # Peso per Cosine\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        loss_mse = self.mse(pred, target)\n",
    "        cosine_sim = F.cosine_similarity(pred, target, dim=1).mean()\n",
    "        loss_cosine = 1 - cosine_sim\n",
    "        \n",
    "        # Loss combinata\n",
    "        return self.alpha * loss_mse + self.beta * loss_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3fba786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_layers(model_name, dataset_name, layer_indices, type_layer, \n",
    "                          balanced_indices, balanced_labels, train_indices, test_indices):\n",
    "    \"\"\"\n",
    "    Caricamento standard in RAM con indici bilanciati pre-calcolati.\n",
    "    Supporta sia la vecchia struttura (file direttamente in activation_X/)\n",
    "    sia la nuova struttura (file in hallucinated/ e not_hallucinated/).\n",
    "    \n",
    "    Args:\n",
    "        balanced_indices: indici globali dei campioni concordanti e bilanciati\n",
    "        balanced_labels: label corrispondenti (condivise tra i modelli)\n",
    "        train_indices: indici LOCALI (0..len(balanced_indices)-1) per training\n",
    "        test_indices: indici LOCALI (0..len(balanced_indices)-1) per test\n",
    "    \"\"\"\n",
    "    print(f\" Caricamento IN-MEMORY {model_name} [{type_layer}]: layers {layer_indices}...\")\n",
    "\n",
    "    # Rileva la struttura\n",
    "    structure_type = detect_structure_type(model_name, dataset_name)\n",
    "    print(f\"  Struttura rilevata: {structure_type}\")\n",
    "    \n",
    "    # Load and concatenate\n",
    "    all_features = []\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\n",
    "                                 \"activation_\" + type_layer)\n",
    "        \n",
    "        if structure_type == 'new':\n",
    "            # Nuova struttura: carica da hallucinated/ e not_hallucinated/ separatamente\n",
    "            hall_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer_idx}_activations.pt\")\n",
    "            not_hall_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer_idx}_activations.pt\")\n",
    "            hall_ids_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer_idx}_instance_ids.json\")\n",
    "            not_hall_ids_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer_idx}_instance_ids.json\")\n",
    "            \n",
    "            if not os.path.exists(hall_path) or not os.path.exists(not_hall_path):\n",
    "                print(f\" Warning: Layer {layer_idx} non trovato. Salto.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Loading layer {layer_idx} (new structure)...\", end=\" \")\n",
    "            \n",
    "            # Carica le attivazioni\n",
    "            acts_hall = torch.load(hall_path, map_location='cpu')\n",
    "            acts_not_hall = torch.load(not_hall_path, map_location='cpu')\n",
    "            \n",
    "            # Carica gli instance_ids per sapere l'ordine\n",
    "            with open(hall_ids_path, 'r') as f:\n",
    "                hall_ids = json.load(f)\n",
    "            with open(not_hall_ids_path, 'r') as f:\n",
    "                not_hall_ids = json.load(f)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            if isinstance(acts_hall, torch.Tensor):\n",
    "                X_hall = acts_hall.float().numpy()\n",
    "            else:\n",
    "                X_hall = acts_hall.astype(np.float32)\n",
    "                \n",
    "            if isinstance(acts_not_hall, torch.Tensor):\n",
    "                X_not_hall = acts_not_hall.float().numpy()\n",
    "            else:\n",
    "                X_not_hall = acts_not_hall.astype(np.float32)\n",
    "            \n",
    "            # Flatten if needed\n",
    "            if X_hall.ndim > 2:\n",
    "                X_hall = X_hall.reshape(X_hall.shape[0], -1)\n",
    "            if X_not_hall.ndim > 2:\n",
    "                X_not_hall = X_not_hall.reshape(X_not_hall.shape[0], -1)\n",
    "            \n",
    "            # Ricostruisci l'array completo nell'ordine originale degli indici\n",
    "            total_samples = len(hall_ids) + len(not_hall_ids)\n",
    "            feature_dim = X_hall.shape[1]\n",
    "            X_layer = np.zeros((total_samples, feature_dim), dtype=np.float32)\n",
    "            \n",
    "            # Mappa: instance_id -> posizione nel file\n",
    "            for i, inst_id in enumerate(hall_ids):\n",
    "                X_layer[inst_id] = X_hall[i]\n",
    "            for i, inst_id in enumerate(not_hall_ids):\n",
    "                X_layer[inst_id] = X_not_hall[i]\n",
    "            \n",
    "            del acts_hall, acts_not_hall, X_hall, X_not_hall\n",
    "            \n",
    "        else:\n",
    "            # Vecchia struttura: file direttamente nella cartella\n",
    "            file_path = os.path.join(base_path, f\"layer{layer_idx}_activations.pt\")\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\" Warning: Layer {layer_idx} non trovato. Salto.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Loading layer {layer_idx} (old structure)...\", end=\" \")\n",
    "            acts = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "            # Convert to numpy\n",
    "            if isinstance(acts, torch.Tensor):\n",
    "                X_layer = acts.float().numpy() \n",
    "            else:\n",
    "                X_layer = acts.astype(np.float32)\n",
    "\n",
    "            # Flatten\n",
    "            if X_layer.ndim > 2:\n",
    "                X_layer = X_layer.reshape(X_layer.shape[0], -1)\n",
    "            \n",
    "            del acts\n",
    "        \n",
    "        # Seleziona SOLO i campioni bilanciati (usando gli indici globali)\n",
    "        X_layer = X_layer[balanced_indices]\n",
    "            \n",
    "        all_features.append(X_layer)\n",
    "        print(f\"done ({X_layer.shape})\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"Nessun layer valido trovato per {model_name}\")\n",
    "\n",
    "    print(\" Concatenating layers...\")\n",
    "    X_balanced = np.concatenate(all_features, axis=1)\n",
    "    \n",
    "    # Ora train_indices e test_indices sono indici LOCALI (0..len(balanced_indices)-1)\n",
    "    X_train = X_balanced[train_indices]\n",
    "    X_test = X_balanced[test_indices]\n",
    "    y_train = balanced_labels[train_indices]\n",
    "    y_test = balanced_labels[test_indices]\n",
    "    \n",
    "    print(f\" Completato! Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def run_experiment_pipeline_cached(X_teacher, y_teacher, teacher_name,\n",
    "                                   X_student, y_student, student_name, layer_type, config_name,\n",
    "                                   alignment_config=ALIGNMENT_CONFIG,\n",
    "                                   probe_config=PROBE_CONFIG):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPERIMENT: {layer_type.upper()} → {teacher_name} ← {student_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Dati già splittati (numpy per sklearn)\n",
    "    X_A_train_full, X_A_test = X_teacher['X_train'], X_teacher['X_test']\n",
    "    y_A_train_full, y_A_test = y_teacher['y_train'], y_teacher['y_test']\n",
    "    X_B_train_full, X_B_test = X_student['X_train'], X_student['X_test']\n",
    "    y_B_train_full, y_B_test = y_student['y_train'], y_student['y_test']\n",
    "\n",
    "    device = DEVICE\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Teacher Probing (su FULL training set)\n",
    "    # --------------------------------------------------\n",
    "    print(\"1. Training teacher probe on FULL training set...\")\n",
    "    probe_teacher = LogisticRegression(\n",
    "        max_iter=probe_config['max_iter'],\n",
    "        class_weight=probe_config['class_weight'],\n",
    "        solver=probe_config['solver'],\n",
    "        n_jobs=probe_config['n_jobs']\n",
    "    )\n",
    "    probe_teacher.fit(X_A_train_full, y_A_train_full)\n",
    "    \n",
    "    # --- METRICHE TEACHER ---\n",
    "    y_pred_teacher = probe_teacher.predict(X_A_test)\n",
    "    y_proba_teacher = probe_teacher.predict_proba(X_A_test)[:, 1]\n",
    "    cm_teacher = confusion_matrix(y_A_test, y_pred_teacher)\n",
    "    acc_teacher = accuracy_score(y_A_test, y_pred_teacher)\n",
    "    prec_teacher = precision_score(y_A_test, y_pred_teacher)\n",
    "    rec_teacher = recall_score(y_A_test, y_pred_teacher)\n",
    "    f1_teacher = f1_score(y_A_test, y_pred_teacher)\n",
    "    auroc_teacher = roc_auc_score(y_A_test, y_proba_teacher)\n",
    "    print(f\"   Acc teacher: {acc_teacher:.4f}, AUROC: {auroc_teacher:.4f}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Alignment Training (Student → Teacher space)\n",
    "    # --------------------------------------------------\n",
    "    print(\"2. Training alignment network (with 90/10 validation split)...\")\n",
    "    \n",
    "    # Preconversion a torch.Tensor UNA VOLTA SOLA\n",
    "    X_A_train_full_t = torch.from_numpy(X_A_train_full).float()\n",
    "    X_A_test_t       = torch.from_numpy(X_A_test).float()\n",
    "    X_B_train_full_t = torch.from_numpy(X_B_train_full).float()\n",
    "    X_B_test_t       = torch.from_numpy(X_B_test).float()\n",
    "\n",
    "    # Create Validation Split (10%) per l'alignment network SOLTANTO\n",
    "    num_train = len(X_B_train_full)\n",
    "    indices = np.arange(num_train)\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(indices)\n",
    "    val_size = int(num_train * 0.1)\n",
    "    train_idx_local = indices[val_size:]\n",
    "    val_idx_local = indices[:val_size]\n",
    "\n",
    "    # Slice diretta sui tensori (no conversione per-item)\n",
    "    X_B_align_train = X_B_train_full_t[train_idx_local]\n",
    "    X_A_align_train = X_A_train_full_t[train_idx_local]\n",
    "    \n",
    "    X_B_val = X_B_train_full_t[val_idx_local]\n",
    "    X_A_val = X_A_train_full_t[val_idx_local]\n",
    "\n",
    "    train_dataset = AlignmentDataset(X_B_align_train.to(device), X_A_align_train.to(device))\n",
    "    val_dataset   = AlignmentDataset(X_B_val.to(device),  X_A_val.to(device))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=alignment_config['batch_size'], \n",
    "                             shuffle=True, num_workers=0, pin_memory=False,\n",
    "                             generator=get_generator(SEED))\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=alignment_config['batch_size'], \n",
    "                             shuffle=False, num_workers=0, pin_memory=False)\n",
    "    \n",
    "    criterion = MixedLoss(\n",
    "        alpha=alignment_config['loss_alpha'],\n",
    "        beta=alignment_config['loss_beta']\n",
    "    ).to(device)\n",
    "\n",
    "    set_seed(SEED)  # Reset seed prima di inizializzare il modello\n",
    "    aligner = AlignmentNetwork(\n",
    "        input_dim=X_B_align_train.shape[1],\n",
    "        output_dim=X_A_align_train.shape[1],\n",
    "        hidden_dim=alignment_config['hidden_dim'],\n",
    "        dropout=alignment_config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        aligner.parameters(), \n",
    "        lr=alignment_config['learning_rate'], \n",
    "        weight_decay=alignment_config['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=alignment_config['max_epochs']\n",
    "    )\n",
    "    \n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    epochs_trained = 0\n",
    "    \n",
    "    for epoch in range(alignment_config['max_epochs']):\n",
    "        # Training\n",
    "        aligner.train()\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            projected = aligner(data)\n",
    "            loss = criterion(projected, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                aligner.parameters(), \n",
    "                max_norm=alignment_config['gradient_clip_max_norm']\n",
    "            )\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        aligner.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                projected = aligner(data)\n",
    "                loss = criterion(projected, target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"   Epoch {epoch+1:3d}/{alignment_config['max_epochs']} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "            \n",
    "        # Early Stopping Check\n",
    "        if avg_val_loss < best_val_loss - alignment_config['early_stopping_min_delta']:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = aligner.state_dict()\n",
    "            epochs_trained = epoch + 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= alignment_config['early_stopping_patience']:\n",
    "            print(f\"   Early stopping at epoch {epoch+1}. Best Val Loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "    \n",
    "    # Se non c'è stato early stopping, epochs_trained = max_epochs\n",
    "    if epochs_trained == 0:\n",
    "        epochs_trained = alignment_config['max_epochs']\n",
    "            \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        aligner.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save the best alignment network to disk\n",
    "    model_save_dir = os.path.join(\"alignment_models\", layer_type)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    model_filename = os.path.join(model_save_dir, f\"{config_name}_aligner_{student_name}_to_{teacher_name}.pt\")\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': aligner.state_dict(),\n",
    "        'alignment_config': alignment_config,\n",
    "        'probe_config': probe_config,\n",
    "        'input_dim': X_B_align_train.shape[1],\n",
    "        'output_dim': X_A_align_train.shape[1],\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'epochs_trained': epochs_trained,\n",
    "        'layer_type': layer_type,\n",
    "        'student_model': student_name,\n",
    "        'teacher_model': teacher_name,\n",
    "    }, model_filename)\n",
    "    print(f\"   ✓ Alignment network saved: {model_filename}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Evaluation: Student projected → Teacher probe\n",
    "    # --------------------------------------------------\n",
    "    print(\"3. Projecting student test set & evaluating...\")\n",
    "    aligner.eval()\n",
    "    with torch.no_grad():\n",
    "        X_B_projected = aligner(X_B_test_t.to(device)).cpu().numpy()\n",
    "    y_pred_cross = probe_teacher.predict(X_B_projected)\n",
    "    y_proba_cross = probe_teacher.predict_proba(X_B_projected)[:, 1]\n",
    "    \n",
    "    # --- METRICHE CROSS-MODEL ---\n",
    "    cm_cross = confusion_matrix(y_B_test, y_pred_cross)\n",
    "    acc_cross = accuracy_score(y_B_test, y_pred_cross)\n",
    "    prec_cross = precision_score(y_B_test, y_pred_cross)\n",
    "    rec_cross = recall_score(y_B_test, y_pred_cross)\n",
    "    f1_cross = f1_score(y_B_test, y_pred_cross)\n",
    "    auroc_cross = roc_auc_score(y_B_test, y_proba_cross)\n",
    "    \n",
    "    print(f\"\\nFINAL RESULT:\")\n",
    "    print(f\"   Teacher Acc         : {acc_teacher:.4f}, AUROC: {auroc_teacher:.4f}\")\n",
    "    print(f\"   Student → Teacher Acc: {acc_cross:.4f}, AUROC: {auroc_cross:.4f}\")\n",
    "    print(f\"   Transfer gap        : {acc_teacher - acc_cross:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"type\": layer_type,\n",
    "        \"teacher_name\": teacher_name,\n",
    "        \"student_name\": student_name,\n",
    "        \"alignment_model\": {\n",
    "            \"input_dim\": int(X_B_align_train.shape[1]),\n",
    "            \"output_dim\": int(X_A_align_train.shape[1]),\n",
    "            \"config\": alignment_config,\n",
    "            \"best_val_loss\": float(best_val_loss),\n",
    "            \"epochs_trained\": epochs_trained,\n",
    "            \"model_path\": model_filename\n",
    "        },\n",
    "        \"probe_config\": probe_config,\n",
    "        \"teacher\": {\n",
    "            \"accuracy\": acc_teacher,\n",
    "            \"precision\": prec_teacher,\n",
    "            \"recall\": rec_teacher,\n",
    "            \"f1\": f1_teacher,\n",
    "            \"auroc\": auroc_teacher,\n",
    "            \"confusion_matrix\": cm_teacher.tolist()\n",
    "        },\n",
    "        \"student_on_teacher\": {\n",
    "            \"accuracy\": acc_cross,\n",
    "            \"precision\": prec_cross,\n",
    "            \"recall\": rec_cross,\n",
    "            \"f1\": f1_cross,\n",
    "            \"auroc\": auroc_cross,\n",
    "            \"confusion_matrix\": cm_cross.tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, layer_type, model_name=\"\", save_dir=\"confusion_matrices\"):\n",
    "    \"\"\"\n",
    "    Plotta e salva la confusion matrix come immagine.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,\n",
    "                xticklabels=['Non-Hallucinated', 'Hallucinated'],\n",
    "                yticklabels=['Non-Hallucinated', 'Hallucinated'])\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    title = f'Confusion Matrix - {layer_type.upper()} Layers'\n",
    "    if model_name:\n",
    "        title += f' ({model_name})'\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = os.path.join(save_dir, f'confusion_matrix_{layer_type}_{model_name}.png' if model_name else f'confusion_matrix_{layer_type}.png')\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   ✓ Salvato: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b3ac98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FASE 1: SPLIT DEI DATI BILANCIATI (70% train / 30% test)\n",
      "================================================================================\n",
      "\n",
      "Campioni bilanciati totali: 934\n",
      "Train: 653, Test: 281\n",
      "Label train - Hall: 314, Non-Hall: 339\n",
      "Label test  - Hall: 153, Non-Hall: 128\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: ATTN\n",
      "========================================\n",
      " Caricamento IN-MEMORY gemma-2-9b-it [attn]: layers [21, 24, 27]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 21 (new structure)... done ((934, 3584))\n",
      "  Loading layer 24 (new structure)... done ((934, 3584))\n",
      "  Loading layer 24 (new structure)... done ((934, 3584))\n",
      "  Loading layer 27 (new structure)... done ((934, 3584))\n",
      "  Loading layer 27 (new structure)... done ((934, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 10752), Test: (281, 10752)\n",
      " Caricamento IN-MEMORY Llama-3.1-8B-Instruct [attn]: layers [8, 13, 14]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 8 (new structure)... done ((934, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 10752), Test: (281, 10752)\n",
      " Caricamento IN-MEMORY Llama-3.1-8B-Instruct [attn]: layers [8, 13, 14]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 8 (new structure)... done ((934, 4096))\n",
      "  Loading layer 13 (new structure)... done ((934, 4096))\n",
      "  Loading layer 13 (new structure)... done ((934, 4096))\n",
      "  Loading layer 14 (new structure)... done ((934, 4096))\n",
      "  Loading layer 14 (new structure)... done ((934, 4096))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 12288), Test: (281, 12288)\n",
      "   Normalizzazione dati...\n",
      "done ((934, 4096))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 12288), Test: (281, 12288)\n",
      "   Normalizzazione dati...\n",
      "\n",
      "   --- Scenario: gemma-2-9b-it -> Llama-3.1-8B-Instruct ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: ATTN → gemma-2-9b-it ← Llama-3.1-8B-Instruct\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "\n",
      "   --- Scenario: gemma-2-9b-it -> Llama-3.1-8B-Instruct ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: ATTN → gemma-2-9b-it ← Llama-3.1-8B-Instruct\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   Acc teacher: 0.9537, AUROC: 0.9935\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Acc teacher: 0.9537, AUROC: 0.9935\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch   1/1000 | Train Loss: 0.769031 | Val Loss: 0.524229\n",
      "   Epoch   1/1000 | Train Loss: 0.769031 | Val Loss: 0.524229\n",
      "   Epoch  10/1000 | Train Loss: 0.151566 | Val Loss: 0.395980\n",
      "   Epoch  10/1000 | Train Loss: 0.151566 | Val Loss: 0.395980\n",
      "   Epoch  20/1000 | Train Loss: 0.085810 | Val Loss: 0.395863\n",
      "   Epoch  20/1000 | Train Loss: 0.085810 | Val Loss: 0.395863\n",
      "   Epoch  30/1000 | Train Loss: 0.064122 | Val Loss: 0.405471\n",
      "   Epoch  30/1000 | Train Loss: 0.064122 | Val Loss: 0.405471\n",
      "   Epoch  40/1000 | Train Loss: 0.067509 | Val Loss: 0.405748\n",
      "   Epoch  40/1000 | Train Loss: 0.067509 | Val Loss: 0.405748\n",
      "   Epoch  50/1000 | Train Loss: 0.055141 | Val Loss: 0.403546\n",
      "   Epoch  50/1000 | Train Loss: 0.055141 | Val Loss: 0.403546\n",
      "   Epoch  60/1000 | Train Loss: 0.067029 | Val Loss: 0.413775\n",
      "   Epoch  60/1000 | Train Loss: 0.067029 | Val Loss: 0.413775\n",
      "   Early stopping at epoch 62. Best Val Loss: 0.375175\n",
      "   Early stopping at epoch 62. Best Val Loss: 0.375175\n",
      "   ✓ Alignment network saved: alignment_models/attn/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9537, AUROC: 0.9935\n",
      "   Student → Teacher Acc: 0.9359, AUROC: 0.9853\n",
      "   Transfer gap        : 0.0178\n",
      "   ✓ Alignment network saved: alignment_models/attn/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9537, AUROC: 0.9935\n",
      "   Student → Teacher Acc: 0.9359, AUROC: 0.9853\n",
      "   Transfer gap        : 0.0178\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_attn_Teacher_gemma-2-9b-it.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_attn_Llama-3_1-8B-Instruct_on_gemma-2-9b-it.png\n",
      "\n",
      "   --- Scenario: Llama-3.1-8B-Instruct -> gemma-2-9b-it ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: ATTN → Llama-3.1-8B-Instruct ← gemma-2-9b-it\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_attn_Teacher_gemma-2-9b-it.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_attn_Llama-3_1-8B-Instruct_on_gemma-2-9b-it.png\n",
      "\n",
      "   --- Scenario: Llama-3.1-8B-Instruct -> gemma-2-9b-it ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: ATTN → Llama-3.1-8B-Instruct ← gemma-2-9b-it\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   Acc teacher: 0.9644, AUROC: 0.9951\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Acc teacher: 0.9644, AUROC: 0.9951\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch   1/1000 | Train Loss: 0.687932 | Val Loss: 0.536471\n",
      "   Epoch   1/1000 | Train Loss: 0.687932 | Val Loss: 0.536471\n",
      "   Epoch  10/1000 | Train Loss: 0.186650 | Val Loss: 0.506939\n",
      "   Epoch  10/1000 | Train Loss: 0.186650 | Val Loss: 0.506939\n",
      "   Epoch  20/1000 | Train Loss: 0.123198 | Val Loss: 0.440634\n",
      "   Epoch  20/1000 | Train Loss: 0.123198 | Val Loss: 0.440634\n",
      "   Epoch  30/1000 | Train Loss: 0.084309 | Val Loss: 0.513117\n",
      "   Epoch  30/1000 | Train Loss: 0.084309 | Val Loss: 0.513117\n",
      "   Epoch  40/1000 | Train Loss: 0.066597 | Val Loss: 0.477221\n",
      "   Epoch  40/1000 | Train Loss: 0.066597 | Val Loss: 0.477221\n",
      "   Epoch  50/1000 | Train Loss: 0.062126 | Val Loss: 0.495738\n",
      "   Epoch  50/1000 | Train Loss: 0.062126 | Val Loss: 0.495738\n",
      "   Early stopping at epoch 54. Best Val Loss: 0.393925\n",
      "   Early stopping at epoch 54. Best Val Loss: 0.393925\n",
      "   ✓ Alignment network saved: alignment_models/attn/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9644, AUROC: 0.9951\n",
      "   Student → Teacher Acc: 0.9537, AUROC: 0.9900\n",
      "   Transfer gap        : 0.0107\n",
      "   ✓ Alignment network saved: alignment_models/attn/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9644, AUROC: 0.9951\n",
      "   Student → Teacher Acc: 0.9537, AUROC: 0.9900\n",
      "   Transfer gap        : 0.0107\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_attn_Teacher_Llama-3_1-8B-Instruct.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_attn_gemma-2-9b-it_on_Llama-3_1-8B-Instruct.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_attn_Teacher_Llama-3_1-8B-Instruct.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_attn_gemma-2-9b-it_on_Llama-3_1-8B-Instruct.png\n",
      "   Memoria liberata per attn.\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: MLP\n",
      "========================================\n",
      " Caricamento IN-MEMORY gemma-2-9b-it [mlp]: layers [22, 25, 27]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 22 (new structure)...    Memoria liberata per attn.\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: MLP\n",
      "========================================\n",
      " Caricamento IN-MEMORY gemma-2-9b-it [mlp]: layers [22, 25, 27]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 22 (new structure)... done ((934, 3584))\n",
      "  Loading layer 25 (new structure)... done ((934, 3584))\n",
      "  Loading layer 25 (new structure)... done ((934, 3584))\n",
      "  Loading layer 27 (new structure)... done ((934, 3584))\n",
      "  Loading layer 27 (new structure)... done ((934, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 10752), Test: (281, 10752)\n",
      " Caricamento IN-MEMORY Llama-3.1-8B-Instruct [mlp]: layers [14, 15, 21]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 14 (new structure)... done ((934, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 10752), Test: (281, 10752)\n",
      " Caricamento IN-MEMORY Llama-3.1-8B-Instruct [mlp]: layers [14, 15, 21]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 14 (new structure)... done ((934, 4096))\n",
      "  Loading layer 15 (new structure)... done ((934, 4096))\n",
      "  Loading layer 15 (new structure)... done ((934, 4096))\n",
      "  Loading layer 21 (new structure)... done ((934, 4096))\n",
      "  Loading layer 21 (new structure)... done ((934, 4096))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 12288), Test: (281, 12288)\n",
      "   Normalizzazione dati...\n",
      "done ((934, 4096))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 12288), Test: (281, 12288)\n",
      "   Normalizzazione dati...\n",
      "\n",
      "   --- Scenario: gemma-2-9b-it -> Llama-3.1-8B-Instruct ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: MLP → gemma-2-9b-it ← Llama-3.1-8B-Instruct\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "\n",
      "   --- Scenario: gemma-2-9b-it -> Llama-3.1-8B-Instruct ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: MLP → gemma-2-9b-it ← Llama-3.1-8B-Instruct\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   Acc teacher: 0.9537, AUROC: 0.9912\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Acc teacher: 0.9537, AUROC: 0.9912\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch   1/1000 | Train Loss: 0.825053 | Val Loss: 0.535767\n",
      "   Epoch   1/1000 | Train Loss: 0.825053 | Val Loss: 0.535767\n",
      "   Epoch  10/1000 | Train Loss: 0.161145 | Val Loss: 0.448596\n",
      "   Epoch  10/1000 | Train Loss: 0.161145 | Val Loss: 0.448596\n",
      "   Epoch  20/1000 | Train Loss: 0.080563 | Val Loss: 0.462746\n",
      "   Epoch  20/1000 | Train Loss: 0.080563 | Val Loss: 0.462746\n",
      "   Epoch  30/1000 | Train Loss: 0.064779 | Val Loss: 0.478145\n",
      "   Epoch  30/1000 | Train Loss: 0.064779 | Val Loss: 0.478145\n",
      "   Epoch  40/1000 | Train Loss: 0.053985 | Val Loss: 0.457373\n",
      "   Epoch  40/1000 | Train Loss: 0.053985 | Val Loss: 0.457373\n",
      "   Epoch  50/1000 | Train Loss: 0.050955 | Val Loss: 0.446064\n",
      "   Epoch  50/1000 | Train Loss: 0.050955 | Val Loss: 0.446064\n",
      "   Early stopping at epoch 57. Best Val Loss: 0.407517\n",
      "   Early stopping at epoch 57. Best Val Loss: 0.407517\n",
      "   ✓ Alignment network saved: alignment_models/mlp/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9537, AUROC: 0.9912\n",
      "   Student → Teacher Acc: 0.9324, AUROC: 0.9814\n",
      "   Transfer gap        : 0.0214\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_mlp_Teacher_gemma-2-9b-it.png\n",
      "   ✓ Alignment network saved: alignment_models/mlp/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9537, AUROC: 0.9912\n",
      "   Student → Teacher Acc: 0.9324, AUROC: 0.9814\n",
      "   Transfer gap        : 0.0214\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_mlp_Teacher_gemma-2-9b-it.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_mlp_Llama-3_1-8B-Instruct_on_gemma-2-9b-it.png\n",
      "\n",
      "   --- Scenario: Llama-3.1-8B-Instruct -> gemma-2-9b-it ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: MLP → Llama-3.1-8B-Instruct ← gemma-2-9b-it\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_mlp_Llama-3_1-8B-Instruct_on_gemma-2-9b-it.png\n",
      "\n",
      "   --- Scenario: Llama-3.1-8B-Instruct -> gemma-2-9b-it ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: MLP → Llama-3.1-8B-Instruct ← gemma-2-9b-it\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   Acc teacher: 0.9715, AUROC: 0.9961\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Acc teacher: 0.9715, AUROC: 0.9961\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch   1/1000 | Train Loss: 0.719801 | Val Loss: 0.521444\n",
      "   Epoch   1/1000 | Train Loss: 0.719801 | Val Loss: 0.521444\n",
      "   Epoch  10/1000 | Train Loss: 0.213980 | Val Loss: 0.411389\n",
      "   Epoch  10/1000 | Train Loss: 0.213980 | Val Loss: 0.411389\n",
      "   Epoch  20/1000 | Train Loss: 0.141085 | Val Loss: 0.397485\n",
      "   Epoch  20/1000 | Train Loss: 0.141085 | Val Loss: 0.397485\n",
      "   Epoch  30/1000 | Train Loss: 0.106721 | Val Loss: 0.392348\n",
      "   Epoch  30/1000 | Train Loss: 0.106721 | Val Loss: 0.392348\n",
      "   Epoch  40/1000 | Train Loss: 0.084700 | Val Loss: 0.419163\n",
      "   Epoch  40/1000 | Train Loss: 0.084700 | Val Loss: 0.419163\n",
      "   Epoch  50/1000 | Train Loss: 0.072557 | Val Loss: 0.417662\n",
      "   Epoch  50/1000 | Train Loss: 0.072557 | Val Loss: 0.417662\n",
      "   Epoch  60/1000 | Train Loss: 0.062276 | Val Loss: 0.406773\n",
      "   Epoch  60/1000 | Train Loss: 0.062276 | Val Loss: 0.406773\n",
      "   Early stopping at epoch 65. Best Val Loss: 0.346048\n",
      "   Early stopping at epoch 65. Best Val Loss: 0.346048\n",
      "   ✓ Alignment network saved: alignment_models/mlp/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9715, AUROC: 0.9961\n",
      "   Student → Teacher Acc: 0.9644, AUROC: 0.9958\n",
      "   Transfer gap        : 0.0071\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_mlp_Teacher_Llama-3_1-8B-Instruct.png\n",
      "   ✓ Alignment network saved: alignment_models/mlp/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9715, AUROC: 0.9961\n",
      "   Student → Teacher Acc: 0.9644, AUROC: 0.9958\n",
      "   Transfer gap        : 0.0071\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_mlp_Teacher_Llama-3_1-8B-Instruct.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_mlp_gemma-2-9b-it_on_Llama-3_1-8B-Instruct.png\n",
      "   Memoria liberata per mlp.\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: HIDDEN\n",
      "========================================\n",
      " Caricamento IN-MEMORY gemma-2-9b-it [hidden]: layers [23, 26, 34]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 23 (new structure)...    ✓ Salvato: confusion_matrices/confusion_matrix_mlp_gemma-2-9b-it_on_Llama-3_1-8B-Instruct.png\n",
      "   Memoria liberata per mlp.\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: HIDDEN\n",
      "========================================\n",
      " Caricamento IN-MEMORY gemma-2-9b-it [hidden]: layers [23, 26, 34]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 23 (new structure)... done ((934, 3584))\n",
      "  Loading layer 26 (new structure)... done ((934, 3584))\n",
      "  Loading layer 26 (new structure)... done ((934, 3584))\n",
      "  Loading layer 34 (new structure)... done ((934, 3584))\n",
      "  Loading layer 34 (new structure)... done ((934, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 10752), Test: (281, 10752)\n",
      " Caricamento IN-MEMORY Llama-3.1-8B-Instruct [hidden]: layers [14, 15, 16]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 14 (new structure)... done ((934, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 10752), Test: (281, 10752)\n",
      " Caricamento IN-MEMORY Llama-3.1-8B-Instruct [hidden]: layers [14, 15, 16]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 14 (new structure)... done ((934, 4096))\n",
      "  Loading layer 15 (new structure)... done ((934, 4096))\n",
      "  Loading layer 15 (new structure)... done ((934, 4096))\n",
      "  Loading layer 16 (new structure)... done ((934, 4096))\n",
      "  Loading layer 16 (new structure)... done ((934, 4096))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 12288), Test: (281, 12288)\n",
      "   Normalizzazione dati...\n",
      "done ((934, 4096))\n",
      " Concatenating layers...\n",
      " Completato! Train: (653, 12288), Test: (281, 12288)\n",
      "   Normalizzazione dati...\n",
      "\n",
      "   --- Scenario: gemma-2-9b-it -> Llama-3.1-8B-Instruct ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: HIDDEN → gemma-2-9b-it ← Llama-3.1-8B-Instruct\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "\n",
      "   --- Scenario: gemma-2-9b-it -> Llama-3.1-8B-Instruct ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: HIDDEN → gemma-2-9b-it ← Llama-3.1-8B-Instruct\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   Acc teacher: 0.9537, AUROC: 0.9909\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Acc teacher: 0.9537, AUROC: 0.9909\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch   1/1000 | Train Loss: 0.834614 | Val Loss: 0.551277\n",
      "   Epoch   1/1000 | Train Loss: 0.834614 | Val Loss: 0.551277\n",
      "   Epoch  10/1000 | Train Loss: 0.188254 | Val Loss: 0.421249\n",
      "   Epoch  10/1000 | Train Loss: 0.188254 | Val Loss: 0.421249\n",
      "   Epoch  20/1000 | Train Loss: 0.098354 | Val Loss: 0.450836\n",
      "   Epoch  20/1000 | Train Loss: 0.098354 | Val Loss: 0.450836\n",
      "   Epoch  30/1000 | Train Loss: 0.071479 | Val Loss: 0.478644\n",
      "   Epoch  30/1000 | Train Loss: 0.071479 | Val Loss: 0.478644\n",
      "   Epoch  40/1000 | Train Loss: 0.066747 | Val Loss: 0.476782\n",
      "   Epoch  40/1000 | Train Loss: 0.066747 | Val Loss: 0.476782\n",
      "   Epoch  50/1000 | Train Loss: 0.054996 | Val Loss: 0.474332\n",
      "   Epoch  50/1000 | Train Loss: 0.054996 | Val Loss: 0.474332\n",
      "   Early stopping at epoch 57. Best Val Loss: 0.402084\n",
      "   Early stopping at epoch 57. Best Val Loss: 0.402084\n",
      "   ✓ Alignment network saved: alignment_models/hidden/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9537, AUROC: 0.9909\n",
      "   Student → Teacher Acc: 0.9288, AUROC: 0.9849\n",
      "   Transfer gap        : 0.0249\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_hidden_Teacher_gemma-2-9b-it.png\n",
      "   ✓ Alignment network saved: alignment_models/hidden/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9537, AUROC: 0.9909\n",
      "   Student → Teacher Acc: 0.9288, AUROC: 0.9849\n",
      "   Transfer gap        : 0.0249\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_hidden_Teacher_gemma-2-9b-it.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_hidden_Llama-3_1-8B-Instruct_on_gemma-2-9b-it.png\n",
      "\n",
      "   --- Scenario: Llama-3.1-8B-Instruct -> gemma-2-9b-it ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: HIDDEN → Llama-3.1-8B-Instruct ← gemma-2-9b-it\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_hidden_Llama-3_1-8B-Instruct_on_gemma-2-9b-it.png\n",
      "\n",
      "   --- Scenario: Llama-3.1-8B-Instruct -> gemma-2-9b-it ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: HIDDEN → Llama-3.1-8B-Instruct ← gemma-2-9b-it\n",
      "============================================================\n",
      "Using device: cuda:2\n",
      "1. Training teacher probe on FULL training set...\n",
      "   Acc teacher: 0.9751, AUROC: 0.9950\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Acc teacher: 0.9751, AUROC: 0.9950\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch   1/1000 | Train Loss: 0.706355 | Val Loss: 0.512236\n",
      "   Epoch   1/1000 | Train Loss: 0.706355 | Val Loss: 0.512236\n",
      "   Epoch  10/1000 | Train Loss: 0.201374 | Val Loss: 0.417389\n",
      "   Epoch  10/1000 | Train Loss: 0.201374 | Val Loss: 0.417389\n",
      "   Epoch  20/1000 | Train Loss: 0.132022 | Val Loss: 0.380592\n",
      "   Epoch  20/1000 | Train Loss: 0.132022 | Val Loss: 0.380592\n",
      "   Epoch  30/1000 | Train Loss: 0.095110 | Val Loss: 0.386090\n",
      "   Epoch  30/1000 | Train Loss: 0.095110 | Val Loss: 0.386090\n",
      "   Epoch  40/1000 | Train Loss: 0.075840 | Val Loss: 0.415265\n",
      "   Epoch  40/1000 | Train Loss: 0.075840 | Val Loss: 0.415265\n",
      "   Epoch  50/1000 | Train Loss: 0.071062 | Val Loss: 0.419554\n",
      "   Epoch  50/1000 | Train Loss: 0.071062 | Val Loss: 0.419554\n",
      "   Epoch  60/1000 | Train Loss: 0.062144 | Val Loss: 0.402165\n",
      "   Epoch  60/1000 | Train Loss: 0.062144 | Val Loss: 0.402165\n",
      "   Early stopping at epoch 65. Best Val Loss: 0.350129\n",
      "   Early stopping at epoch 65. Best Val Loss: 0.350129\n",
      "   ✓ Alignment network saved: alignment_models/hidden/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9751, AUROC: 0.9950\n",
      "   Student → Teacher Acc: 0.9537, AUROC: 0.9928\n",
      "   Transfer gap        : 0.0214\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_hidden_Teacher_Llama-3_1-8B-Instruct.png\n",
      "   ✓ Alignment network saved: alignment_models/hidden/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9751, AUROC: 0.9950\n",
      "   Student → Teacher Acc: 0.9537, AUROC: 0.9928\n",
      "   Transfer gap        : 0.0214\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_hidden_Teacher_Llama-3_1-8B-Instruct.png\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_hidden_gemma-2-9b-it_on_Llama-3_1-8B-Instruct.png\n",
      "   Memoria liberata per hidden.\n",
      "\n",
      "============================================================\n",
      "✓ Risultati salvati in: results_metrics/hybrid_adapter_logreg_results.json\n",
      "============================================================\n",
      "   ✓ Salvato: confusion_matrices/confusion_matrix_hidden_gemma-2-9b-it_on_Llama-3_1-8B-Instruct.png\n",
      "   Memoria liberata per hidden.\n",
      "\n",
      "============================================================\n",
      "✓ Risultati salvati in: results_metrics/hybrid_adapter_logreg_results.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FASE 1: SPLIT DEI DATI BILANCIATI (70% train / 30% test)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Ora lavoriamo sugli indici LOCALI (0..len(balanced_indices)-1)\n",
    "n_balanced = len(balanced_indices)\n",
    "rng = np.random.RandomState(SEED)\n",
    "shuffled_local_indices = rng.permutation(n_balanced)\n",
    "split_idx = int(0.7 * n_balanced)\n",
    "\n",
    "train_indices = shuffled_local_indices[:split_idx]\n",
    "test_indices = shuffled_local_indices[split_idx:]\n",
    "\n",
    "print(f\"Campioni bilanciati totali: {n_balanced}\")\n",
    "print(f\"Train: {len(train_indices)}, Test: {len(test_indices)}\")\n",
    "print(f\"Label train - Hall: {np.sum(balanced_labels[train_indices]==1)}, Non-Hall: {np.sum(balanced_labels[train_indices]==0)}\")\n",
    "print(f\"Label test  - Hall: {np.sum(balanced_labels[test_indices]==1)}, Non-Hall: {np.sum(balanced_labels[test_indices]==0)}\")\n",
    "\n",
    "# Definisci gli scenari di esperimento usando le costanti\n",
    "scenarios = [\n",
    "    {\"teacher_model\": MODEL_A, \"student_model\": MODEL_B},\n",
    "    {\"teacher_model\": MODEL_B, \"student_model\": MODEL_A}\n",
    "]\n",
    "\n",
    "# Struttura per raccogliere i risultati mantenendo l'ordine degli scenari\n",
    "scenario_results_map = {0: [], 1: []}\n",
    "\n",
    "# Loop sui layer types (Carica -> Esegui -> Libera Memoria)\n",
    "for layer_type in ['attn', 'mlp', 'hidden']:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"PROCESSING LAYER TYPE: {layer_type.upper()}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    try:\n",
    "        # 1. CARICAMENTO E SPLITTING CON INDICI BILANCIATI\n",
    "        X_model_a_train, X_model_a_test, y_model_a_train, y_model_a_test = load_and_split_layers(\n",
    "            MODEL_A, DATASET_NAME, \n",
    "            LAYER_CONFIG[MODEL_A][layer_type], \n",
    "            layer_type,\n",
    "            balanced_indices, balanced_labels,\n",
    "            train_indices, test_indices\n",
    "        )\n",
    "\n",
    "        X_model_b_train, X_model_b_test, y_model_b_train, y_model_b_test = load_and_split_layers(\n",
    "            MODEL_B, DATASET_NAME, \n",
    "            LAYER_CONFIG[MODEL_B][layer_type], \n",
    "            layer_type,\n",
    "            balanced_indices, balanced_labels,\n",
    "            train_indices, test_indices\n",
    "        )\n",
    "        \n",
    "        # 2. SCALING (con cast esplicito a float32 per risparmiare memoria)\n",
    "        print(\"   Normalizzazione dati...\")\n",
    "        scaler_model_a = StandardScaler()\n",
    "        X_model_a_train = scaler_model_a.fit_transform(X_model_a_train).astype(np.float32)\n",
    "        X_model_a_test = scaler_model_a.transform(X_model_a_test).astype(np.float32)\n",
    "        \n",
    "        scaler_model_b = StandardScaler()\n",
    "        X_model_b_train = scaler_model_b.fit_transform(X_model_b_train).astype(np.float32)\n",
    "        X_model_b_test = scaler_model_b.transform(X_model_b_test).astype(np.float32)\n",
    "        \n",
    "        # Organizza i dati per l'uso (le label sono le stesse per entrambi i modelli!)\n",
    "        current_data = {\n",
    "            \"model_a\": {\"X_train\": X_model_a_train, \"X_test\": X_model_a_test, \"y_train\": y_model_a_train, \"y_test\": y_model_a_test},\n",
    "            \"model_b\": {\"X_train\": X_model_b_train, \"X_test\": X_model_b_test, \"y_train\": y_model_b_train, \"y_test\": y_model_b_test}\n",
    "        }\n",
    "\n",
    "        # 3. ESECUZIONE ESPERIMENTI PER ENTRAMBI GLI SCENARI\n",
    "        for i, scenario in enumerate(scenarios):\n",
    "            print(f\"\\n   --- Scenario: {scenario['teacher_model']} -> {scenario['student_model']} ---\")\n",
    "            \n",
    "            if scenario['teacher_model'] == MODEL_A:\n",
    "                X_teacher_data = current_data['model_a']\n",
    "                X_student_data = current_data['model_b']\n",
    "            else:\n",
    "                X_teacher_data = current_data['model_b']\n",
    "                X_student_data = current_data['model_a']\n",
    "            \n",
    "            res = run_experiment_pipeline_cached(\n",
    "                X_teacher_data, X_teacher_data, scenario['teacher_model'],\n",
    "                X_student_data, X_student_data, scenario['student_model'],\n",
    "                layer_type, \"CONFIG1\"\n",
    "            )\n",
    "            scenario_results_map[i].append(res)\n",
    "            \n",
    "            # Plot confusion matrices\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"Teacher_{scenario['teacher_model'].replace('.', '_')}\"\n",
    "            )\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['student_on_teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"{scenario['student_model'].replace('.', '_')}_on_{scenario['teacher_model'].replace('.', '_')}\"\n",
    "            )\n",
    "\n",
    "        # 4. PULIZIA MEMORIA\n",
    "        del current_data, X_model_a_train, X_model_a_test, X_model_b_train, X_model_b_test\n",
    "        del scaler_model_a, scaler_model_b\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"   Memoria liberata per {layer_type}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore critico nel layer {layer_type}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# Ricostruisci la struttura all_results per il salvataggio JSON\n",
    "all_results = []\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    all_results.append({\n",
    "        \"scenario\": f\"{scenario['teacher_model']} (teacher) → {scenario['student_model']} (student)\",\n",
    "        \"results\": scenario_results_map[i]\n",
    "    })\n",
    "\n",
    "# Salva tutti i risultati in JSON\n",
    "os.makedirs(\"results_metrics\", exist_ok=True)\n",
    "metrics_file = \"results_metrics/hybrid_adapter_logreg_results.json\"\n",
    "\n",
    "all_results_json = []\n",
    "for scenario_data in all_results:\n",
    "    scenario_results = []\n",
    "    for r in scenario_data['results']:\n",
    "        config = r['alignment_model']['config']\n",
    "        probe_cfg = r['probe_config']\n",
    "        \n",
    "        scenario_results.append({\n",
    "            \"layer_type\": r['type'],\n",
    "            \"teacher_model\": r['teacher_name'],\n",
    "            \"student_model\": r['student_name'],\n",
    "            \"data_info\": {\n",
    "                \"total_balanced_samples\": int(n_balanced),\n",
    "                \"train_samples\": int(len(train_indices)),\n",
    "                \"test_samples\": int(len(test_indices)),\n",
    "                \"concordant_undersampling\": True\n",
    "            },\n",
    "            \"alignment_model_info\": {\n",
    "                \"architecture\": \"AlignmentNetwork\",\n",
    "                \"input_dim\": r['alignment_model']['input_dim'],\n",
    "                \"output_dim\": r['alignment_model']['output_dim'],\n",
    "                \"hidden_dim\": config['hidden_dim'],\n",
    "                \"dropout\": config['dropout'],\n",
    "                \"activation\": \"GELU\",\n",
    "                \"normalization\": \"LayerNorm\",\n",
    "                \"residual_connection\": True,\n",
    "                \"initialization\": \"zero_init\"\n",
    "            },\n",
    "            \"training_hyperparameters\": {\n",
    "                \"optimizer\": config['optimizer'],\n",
    "                \"learning_rate\": config['learning_rate'],\n",
    "                \"weight_decay\": config['weight_decay'],\n",
    "                \"batch_size\": config['batch_size'],\n",
    "                \"max_epochs\": config['max_epochs'],\n",
    "                \"scheduler\": config['scheduler'],\n",
    "                \"gradient_clip_max_norm\": config['gradient_clip_max_norm'],\n",
    "                \"early_stopping_patience\": config['early_stopping_patience'],\n",
    "                \"early_stopping_min_delta\": config['early_stopping_min_delta']\n",
    "            },\n",
    "            \"loss_function\": {\n",
    "                \"type\": \"MixedLoss\",\n",
    "                \"mse_weight\": config['loss_alpha'],\n",
    "                \"cosine_weight\": config['loss_beta']\n",
    "            },\n",
    "            \"training_results\": {\n",
    "                \"alignment_network\": {\n",
    "                    \"best_val_loss\": round(r['alignment_model']['best_val_loss'], 6),\n",
    "                    \"epochs_trained\": r['alignment_model']['epochs_trained'],\n",
    "                    \"model_saved_path\": r['alignment_model']['model_path']\n",
    "                }\n",
    "            },\n",
    "            \"teacher_probe\": {\n",
    "                \"type\": probe_cfg['type'],\n",
    "                \"max_iter\": probe_cfg['max_iter'],\n",
    "                \"class_weight\": probe_cfg['class_weight'],\n",
    "                \"solver\": probe_cfg['solver']\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"teacher\": {\n",
    "                    \"accuracy\": round(r['teacher']['accuracy'], 4),\n",
    "                    \"precision\": round(r['teacher']['precision'], 4),\n",
    "                    \"recall\": round(r['teacher']['recall'], 4),\n",
    "                    \"f1_score\": round(r['teacher']['f1'], 4),\n",
    "                    \"auroc\": round(r['teacher']['auroc'], 4),\n",
    "                    \"confusion_matrix\": {\n",
    "                        \"TN\": int(r['teacher']['confusion_matrix'][0][0]),\n",
    "                        \"FP\": int(r['teacher']['confusion_matrix'][0][1]),\n",
    "                        \"FN\": int(r['teacher']['confusion_matrix'][1][0]),\n",
    "                        \"TP\": int(r['teacher']['confusion_matrix'][1][1])\n",
    "                    }\n",
    "                },\n",
    "                \"student_on_teacher\": {\n",
    "                    \"accuracy\": round(r['student_on_teacher']['accuracy'], 4),\n",
    "                    \"precision\": round(r['student_on_teacher']['precision'], 4),\n",
    "                    \"recall\": round(r['student_on_teacher']['recall'], 4),\n",
    "                    \"f1_score\": round(r['student_on_teacher']['f1'], 4),\n",
    "                    \"auroc\": round(r['student_on_teacher']['auroc'], 4),\n",
    "                    \"confusion_matrix\": {\n",
    "                        \"TN\": int(r['student_on_teacher']['confusion_matrix'][0][0]),\n",
    "                        \"FP\": int(r['student_on_teacher']['confusion_matrix'][0][1]),\n",
    "                        \"FN\": int(r['student_on_teacher']['confusion_matrix'][1][0]),\n",
    "                        \"TP\": int(r['student_on_teacher']['confusion_matrix'][1][1])\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    all_results_json.append({\n",
    "        \"scenario\": scenario_data['scenario'],\n",
    "        \"results\": scenario_results\n",
    "    })\n",
    "\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(all_results_json, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "\n",
    "print(f\"✓ Risultati salvati in: {metrics_file}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4e0da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(all_results_json, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
