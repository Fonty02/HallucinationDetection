{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac38c47",
   "metadata": {},
   "source": [
    "# In this notebook a LogisticRegression model for each type of layer is trained and evaluated.\n",
    "\n",
    "This is useful to understand which layers are more suitable for the hallucination detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7845cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52978296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "HF_DEFAULT_HOME = os.environ.get(\"HF_HOME\", \"~\\\\.cache\\\\huggingface\\\\hub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492453b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per statistiche compatibile con la nuova struttura\n",
    "# La nuova struttura ha le attivazioni separate in cartelle hallucinated/ e not_hallucinated/\n",
    "# invece di un file hallucination_labels.json\n",
    "\n",
    "def stats_per_json(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Versione originale per la vecchia struttura con hallucination_labels.json\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\"generations\",\"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    allucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_items': allucinated_items,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "def stats_from_new_structure(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Nuova funzione per la struttura con cartelle hallucinated/ e not_hallucinated/\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    not_hallucinated_path = os.path.join(base_path, \"not_hallucinated\")\n",
    "    \n",
    "    # Carica gli instance_ids da un layer (layer0) per contare i campioni\n",
    "    hall_ids_path = os.path.join(hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    not_hall_ids_path = os.path.join(not_hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    \n",
    "    with open(hall_ids_path, 'r') as f:\n",
    "        hallucinated_ids = json.load(f)\n",
    "    with open(not_hall_ids_path, 'r') as f:\n",
    "        not_hallucinated_ids = json.load(f)\n",
    "    \n",
    "    total = len(hallucinated_ids) + len(not_hallucinated_ids)\n",
    "    hallucinations = len(hallucinated_ids)\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'not_hallucinations': len(not_hallucinated_ids),\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_ids': hallucinated_ids,\n",
    "        'not_hallucinated_ids': not_hallucinated_ids,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "def detect_structure_type(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Rileva automaticamente se la struttura è vecchia o nuova.\n",
    "    Ritorna 'new' se esistono le cartelle hallucinated/not_hallucinated,\n",
    "    altrimenti 'old'.\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    if os.path.isdir(hallucinated_path):\n",
    "        return 'new'\n",
    "    return 'old'\n",
    "\n",
    "def get_stats(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Funzione wrapper che rileva automaticamente la struttura e chiama la funzione appropriata.\n",
    "    \"\"\"\n",
    "    structure = detect_structure_type(model_name, dataset_name)\n",
    "    if structure == 'new':\n",
    "        return stats_from_new_structure(model_name, dataset_name)\n",
    "    else:\n",
    "        return stats_per_json(model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elenca i modelli e dataset disponibili\n",
    "available_models = os.listdir(os.path.join(PROJECT_ROOT, CACHE_DIR_NAME))\n",
    "print(\"Modelli disponibili:\", available_models)\n",
    "\n",
    "# Scegli modello e dataset\n",
    "MODEL_NAME = \"Llama-3.1-8B-Instruct\"  # Cambia secondo necessità\n",
    "DATASET_NAME = \"belief_bank_facts\"      # Cambia secondo necessità\n",
    "\n",
    "# Verifica la struttura\n",
    "structure_type = detect_structure_type(MODEL_NAME, DATASET_NAME)\n",
    "print(f\"Struttura dati rilevata per {MODEL_NAME}/{DATASET_NAME}: {structure_type}\")\n",
    "\n",
    "# Ottieni statistiche\n",
    "stats = get_stats(MODEL_NAME, DATASET_NAME)\n",
    "print(f\"\\nStatistiche per {MODEL_NAME}:\")\n",
    "print(f\"  Totale campioni: {stats['total']}\")\n",
    "print(f\"  Allucinazioni: {stats['hallucinations']} ({stats['percent_hallucinations']:.2f}%)\")\n",
    "\n",
    "# Se vuoi confrontare più modelli\n",
    "if \"gemma-2-9b-it\" in available_models:\n",
    "    gemma_stats = get_stats(\"gemma-2-9b-it\", DATASET_NAME)\n",
    "    print(f\"\\nStatistiche per gemma-2-9b-it:\")\n",
    "    print(f\"  Totale campioni: {gemma_stats['total']}\")\n",
    "    print(f\"  Allucinazioni: {gemma_stats['hallucinations']} ({gemma_stats['percent_hallucinations']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_in_model(model, dataset=None):\n",
    "    \"\"\"\n",
    "    Conta il numero di layer nel modello.\n",
    "    Supporta sia la vecchia che la nuova struttura.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model)\n",
    "    \n",
    "    # Se non viene specificato il dataset, prendi il primo disponibile\n",
    "    if dataset is None:\n",
    "        subdirs = [d for d in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, d))]\n",
    "        if not subdirs:\n",
    "            raise ValueError(f\"No subdirectories found in {file_path}\")\n",
    "        dataset = subdirs[0]\n",
    "    \n",
    "    layer_dir = os.path.join(file_path, dataset, \"activation_attn\")\n",
    "    \n",
    "    # Controlla se è la nuova struttura (con cartelle hallucinated/not_hallucinated)\n",
    "    hallucinated_path = os.path.join(layer_dir, \"hallucinated\")\n",
    "    if os.path.isdir(hallucinated_path):\n",
    "        # Nuova struttura: conta i file layer*_activations.pt nella cartella hallucinated\n",
    "        layer_files = [f for f in os.listdir(hallucinated_path) if f.endswith('_activations.pt')]\n",
    "        return len(layer_files)\n",
    "    else:\n",
    "        # Vecchia struttura: conta i file layer*_activations.pt direttamente\n",
    "        layer_files = [f for f in os.listdir(layer_dir) if f.endswith('_activations.pt')]\n",
    "        return len(layer_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activations_and_labels(model_name, dataset_name, layer, layer_type):\n",
    "    \"\"\"\n",
    "    Carica le attivazioni e le label per un dato layer e tipo.\n",
    "    Supporta sia la vecchia che la nuova struttura dati.\n",
    "    \n",
    "    IMPORTANTE: Per la nuova struttura, le attivazioni vengono ordinate\n",
    "    in base agli instance_ids per garantire la corretta corrispondenza\n",
    "    tra campioni di diversi layer/tipi.\n",
    "    \n",
    "    Returns:\n",
    "        X: numpy array delle attivazioni (n_samples, hidden_dim) - ordinate per instance_id\n",
    "        y: numpy array delle label (n_samples,) - 1=hallucination, 0=correct\n",
    "        instance_ids: numpy array degli instance_ids (n_samples,) - ordinati\n",
    "    \"\"\"\n",
    "    structure = detect_structure_type(model_name, dataset_name)\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, f\"activation_{layer_type}\")\n",
    "    \n",
    "    if structure == 'new':\n",
    "        # Nuova struttura: carica da hallucinated/ e not_hallucinated/\n",
    "        hall_act_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer}_activations.pt\")\n",
    "        hall_ids_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer}_instance_ids.json\")\n",
    "        not_hall_act_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer}_activations.pt\")\n",
    "        not_hall_ids_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer}_instance_ids.json\")\n",
    "        \n",
    "        # Carica attivazioni\n",
    "        hall_activations = torch.load(hall_act_path)\n",
    "        not_hall_activations = torch.load(not_hall_act_path)\n",
    "        \n",
    "        # Carica instance_ids\n",
    "        with open(hall_ids_path, 'r') as f:\n",
    "            hall_ids = json.load(f)\n",
    "        with open(not_hall_ids_path, 'r') as f:\n",
    "            not_hall_ids = json.load(f)\n",
    "        \n",
    "        # Converti in numpy\n",
    "        if isinstance(hall_activations, torch.Tensor):\n",
    "            hall_activations = hall_activations.cpu().numpy().astype(np.float32)\n",
    "        if isinstance(not_hall_activations, torch.Tensor):\n",
    "            not_hall_activations = not_hall_activations.cpu().numpy().astype(np.float32)\n",
    "        \n",
    "        # Concatena attivazioni, label e ids\n",
    "        X_concat = np.vstack([hall_activations, not_hall_activations])\n",
    "        y_concat = np.concatenate([\n",
    "            np.ones(hall_activations.shape[0], dtype=int),\n",
    "            np.zeros(not_hall_activations.shape[0], dtype=int)\n",
    "        ])\n",
    "        ids_concat = np.array(hall_ids + not_hall_ids)\n",
    "        \n",
    "        # Ordina tutto in base agli instance_ids\n",
    "        sort_indices = np.argsort(ids_concat)\n",
    "        X = X_concat[sort_indices]\n",
    "        y = y_concat[sort_indices]\n",
    "        instance_ids = ids_concat[sort_indices]\n",
    "        \n",
    "        return X, y, instance_ids\n",
    "    \n",
    "    else:\n",
    "        # Vecchia struttura: carica tutto insieme e usa hallucination_labels.json\n",
    "        file_path = os.path.join(base_path, f\"layer{layer}_activations.pt\")\n",
    "        activations = torch.load(file_path)\n",
    "        \n",
    "        if isinstance(activations, torch.Tensor):\n",
    "            X = activations.cpu().numpy().astype(np.float32)\n",
    "        else:\n",
    "            X = activations.astype(np.float32)\n",
    "        \n",
    "        # Carica le label dal JSON\n",
    "        labels_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \n",
    "                                   \"generations\", \"hallucination_labels.json\")\n",
    "        with open(labels_path, 'r') as f:\n",
    "            labels_data = json.load(f)\n",
    "        \n",
    "        y = np.array([item['is_hallucination'] for item in labels_data], dtype=int)\n",
    "        instance_ids = np.arange(len(y))  # IDs sequenziali per la vecchia struttura\n",
    "        \n",
    "        return X, y, instance_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica che le attivazioni siano ordinate correttamente per instance_id\n",
    "def verify_ordering(model_name, dataset_name, layer=0, layer_type=\"attn\"):\n",
    "    \"\"\"\n",
    "    Verifica che le attivazioni siano ordinate per instance_id.\n",
    "    \"\"\"\n",
    "    X, y, instance_ids = load_activations_and_labels(model_name, dataset_name, layer, layer_type)\n",
    "    \n",
    "    print(f\"=== Verifica ordinamento per {model_name}/{dataset_name} ===\")\n",
    "    print(f\"Layer: {layer}, Tipo: {layer_type}\")\n",
    "    print(f\"Numero di campioni: {len(instance_ids)}\")\n",
    "    print(f\"\\nPrimi 20 instance_ids: {instance_ids[:20].tolist()}\")\n",
    "    print(f\"Ultime 20 instance_ids: {instance_ids[-20:].tolist()}\")\n",
    "    \n",
    "    # Verifica se sono ordinati\n",
    "    is_sorted = np.all(instance_ids[:-1] <= instance_ids[1:])\n",
    "    print(f\"\\nGli instance_ids sono ordinati in ordine crescente: {is_sorted}\")\n",
    "    \n",
    "    # Verifica corrispondenza label\n",
    "    print(f\"\\nPrime 20 label (y): {y[:20].tolist()}\")\n",
    "    print(f\"Ultime 20 label (y): {y[-20:].tolist()}\")\n",
    "    \n",
    "    # Statistiche sulle label\n",
    "    print(f\"\\nDistribuzione label:\")\n",
    "    print(f\"  Hallucination (y=1): {np.sum(y == 1)}\")\n",
    "    print(f\"  Not hallucination (y=0): {np.sum(y == 0)}\")\n",
    "    \n",
    "    return X, y, instance_ids\n",
    "\n",
    "# Esegui verifica\n",
    "X_test, y_test, ids_test = verify_ordering(MODEL_NAME, DATASET_NAME, layer=0, layer_type=\"attn\")\n",
    "\n",
    "# Verifica anche che diversi layer/tipi abbiano lo stesso ordinamento\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Verifica consistenza tra diversi layer/tipi...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "_, y_attn, ids_attn = load_activations_and_labels(MODEL_NAME, DATASET_NAME, 0, \"attn\")\n",
    "_, y_mlp, ids_mlp = load_activations_and_labels(MODEL_NAME, DATASET_NAME, 0, \"mlp\")\n",
    "_, y_hidden, ids_hidden = load_activations_and_labels(MODEL_NAME, DATASET_NAME, 0, \"hidden\")\n",
    "\n",
    "print(f\"IDs attn == IDs mlp: {np.array_equal(ids_attn, ids_mlp)}\")\n",
    "print(f\"IDs attn == IDs hidden: {np.array_equal(ids_attn, ids_hidden)}\")\n",
    "print(f\"Labels attn == Labels mlp: {np.array_equal(y_attn, y_mlp)}\")\n",
    "print(f\"Labels attn == Labels hidden: {np.array_equal(y_attn, y_hidden)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione\n",
    "MODELS_TO_ANALYZE = [MODEL_NAME]  # Aggiungi altri modelli se necessario\n",
    "if \"gemma-2-9b-it\" in available_models:\n",
    "    MODELS_TO_ANALYZE.append(\"gemma-2-9b-it\")\n",
    "\n",
    "DATASET = DATASET_NAME\n",
    "\n",
    "# Inizializza i risultati\n",
    "results = {model: {\"attn\": {}, \"mlp\": {}, \"hidden\": {}} for model in MODELS_TO_ANALYZE}\n",
    "\n",
    "# Per ogni modello\n",
    "for model in MODELS_TO_ANALYZE:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Elaborazione modello: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    num_layers = layers_in_model(model, DATASET)\n",
    "    print(f\"Numero di layer rilevati: {num_layers}\")\n",
    "    \n",
    "    # Carica le attivazioni per il primo layer per determinare il numero di campioni\n",
    "    X_sample, y_sample, instance_ids_sample = load_activations_and_labels(model, DATASET, 0, \"attn\")\n",
    "    n_samples = X_sample.shape[0]\n",
    "    print(f\"Numero di campioni: {n_samples}\")\n",
    "    print(f\"Instance IDs range: {instance_ids_sample.min()} - {instance_ids_sample.max()}\")\n",
    "    \n",
    "    # Shuffle e split\n",
    "    rng = np.random.RandomState(42)\n",
    "    shuffled_indices = rng.permutation(n_samples)\n",
    "    split_idx = int(0.7 * n_samples)\n",
    "    train_indices = shuffled_indices[:split_idx]\n",
    "    test_indices = shuffled_indices[split_idx:]\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        for layer_type in [\"attn\", \"mlp\", \"hidden\"]:\n",
    "            # Carica attivazioni e label (ora ordinate per instance_id)\n",
    "            X_layer, y, _ = load_activations_and_labels(model, DATASET, layer, layer_type)\n",
    "            \n",
    "            # Split train/test\n",
    "            X_train = X_layer[train_indices]\n",
    "            y_train = y[train_indices]\n",
    "            X_test = X_layer[test_indices]\n",
    "            y_test = y[test_indices]\n",
    "            \n",
    "            # Normalizzazione\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            print(f\"Training Logistic Regression per {model} - Layer {layer} - Type {layer_type}\")\n",
    "            \n",
    "            # Addestramento\n",
    "            clf = LogisticRegression(max_iter=10000, class_weight='balanced', solver='lbfgs', n_jobs=-1)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Metriche\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            # Salva i risultati\n",
    "            results[model][layer_type][layer] = (accuracy, f1)\n",
    "            \n",
    "            # Libera memoria\n",
    "            del X_layer\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completato!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per ordinare tutti i layer per accuracy e salvare su JSON\n",
    "def sort_and_save_all_results(results, output_file=\"sorted_results.json\"):\n",
    "    \"\"\"\n",
    "    Ordina tutti i layer per accuracy in ordine decrescente e salva su JSON.\n",
    "    \n",
    "    Args:\n",
    "        results: dizionario completo dei risultati nel formato:\n",
    "                 {model_name: {layer_type: {layer_num: (accuracy, f1)}}}\n",
    "        output_file: path del file JSON di output\n",
    "    \n",
    "    Returns:\n",
    "        dizionario con tutti i risultati ordinati\n",
    "    \"\"\"\n",
    "    sorted_results = {}\n",
    "    \n",
    "    for model_name, layer_types in results.items():\n",
    "        sorted_results[model_name] = {}\n",
    "        \n",
    "        for layer_type, layer_data in layer_types.items():\n",
    "            # Ordina i layer per accuracy decrescente\n",
    "            sorted_layers = sorted(\n",
    "                [(layer, acc, f1) for layer, (acc, f1) in layer_data.items()],\n",
    "                key=lambda x: x[1],  # ordina per accuracy\n",
    "                reverse=True  # ordine decrescente\n",
    "            )\n",
    "            \n",
    "            # Salva in formato lista ordinata\n",
    "            sorted_results[model_name][layer_type] = [\n",
    "                {\n",
    "                    \"layer\": layer,\n",
    "                    \"accuracy\": acc,\n",
    "                    \"f1_score\": f1\n",
    "                }\n",
    "                for layer, acc, f1 in sorted_layers\n",
    "            ]\n",
    "    \n",
    "    # Salva su JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(sorted_results, f, indent=4)\n",
    "    print(f\"Tutti i risultati ordinati salvati in {output_file}\")\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "# Salva tutti i risultati ordinati\n",
    "sorted_all = sort_and_save_all_results(results, \"all_layers_sorted.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dabd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_from_json(json_data, model_name=None, dataset=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Genera un grafico dell'accuracy per layer.\n",
    "    \n",
    "    Args:\n",
    "        json_data (dict): Il dizionario caricato dal file JSON.\n",
    "        model_name (str): Il nome del modello da plottare.\n",
    "                          Se None, prende il primo modello trovato nel JSON.\n",
    "        dataset (str): Nome del dataset per il titolo del file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Selezione del modello\n",
    "    if model_name is None:\n",
    "        model_name = list(json_data.keys())[0]\n",
    "    \n",
    "    if model_name not in json_data:\n",
    "        print(f\"Errore: Modello '{model_name}' non trovato nel JSON.\")\n",
    "        return\n",
    "\n",
    "    data = json_data[model_name]\n",
    "\n",
    "    # 2. Configurazione dello Stile\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.weight\": \"bold\",\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"axes.labelsize\": 24,\n",
    "        \"xtick.labelsize\": 18,\n",
    "        \"ytick.labelsize\": 18,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"legend.title_fontsize\": 14,\n",
    "        \"lines.linewidth\": 2\n",
    "    })\n",
    "\n",
    "    # Creazione della figura\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Mappatura colori\n",
    "    styles = {\n",
    "        \"hidden\": {\"color\": \"red\", \"label\": \"hidden\"},\n",
    "        \"mlp\":    {\"color\": \"blue\", \"label\": \"mlp\"},\n",
    "        \"attn\":   {\"color\": \"green\", \"label\": \"attn\"}\n",
    "    }\n",
    "\n",
    "    # 3. Estrazione e Ordinamento dei dati\n",
    "    for key in [\"hidden\", \"mlp\", \"attn\"]:\n",
    "        if key in data:\n",
    "            points = data[key]\n",
    "            sorted_points = sorted(points, key=lambda x: x['layer'])\n",
    "            layers = [item['layer'] for item in sorted_points]\n",
    "            accuracies = [item['accuracy'] for item in sorted_points]\n",
    "            ax.plot(layers, accuracies, \n",
    "                    color=styles[key][\"color\"], \n",
    "                    label=styles[key][\"label\"])\n",
    "\n",
    "    # 4. Rifinitura Grafica\n",
    "    ax.set_xlabel(\"Layer\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(f\"{model_name} - {dataset}\")\n",
    "    ax.grid(True, linestyle='-', alpha=1.0)\n",
    "    \n",
    "    legend = ax.legend(title=\"activation\", loc=\"upper left\", frameon=True)\n",
    "    plt.setp(legend.get_title(), fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Crea la cartella img se non esiste\n",
    "    os.makedirs(\"img\", exist_ok=True)\n",
    "    plt.savefig(f\"img/{model_name}_{dataset}_activations.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Carica e visualizza i risultati\n",
    "content = json.load(open('all_layers_sorted.json'))\n",
    "\n",
    "for model_name in content.keys():\n",
    "    plot_accuracy_from_json(content, model_name, DATASET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
