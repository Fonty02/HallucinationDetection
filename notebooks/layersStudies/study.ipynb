{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac38c47",
   "metadata": {},
   "source": [
    "# In this notebook a LogisticRegression model for each type of layer is trained and evaluated.\n",
    "\n",
    "This is useful to understand which layers are more suitable for the hallucination detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7845cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "# ==================================================================\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# ==================================================================\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "def get_balanced_indices(y, seed=SEED):\n",
    "    \"\"\"\n",
    "    Calculate indices to balance the dataset via undersampling.\n",
    "    This function is DETERMINISTIC given the same seed and the same labels.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array of labels\n",
    "        seed: seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        balanced_indices: numpy array of selected indices (sorted)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    unique_classes, counts = np.unique(y, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "    \n",
    "    selected_indices = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        \n",
    "        if len(cls_indices) > min_count:\n",
    "            sampled = rng.choice(cls_indices, size=min_count, replace=False)\n",
    "            selected_indices.extend(sampled)\n",
    "        else:\n",
    "            selected_indices.extend(cls_indices)\n",
    "    \n",
    "    return np.sort(np.array(selected_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52978296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "HF_DEFAULT_HOME = os.environ.get(\"HF_HOME\", \"~\\\\.cache\\\\huggingface\\\\hub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492453b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for statistics compatible with the new structure\n",
    "# The new structure has activations separated in hallucinated/ and not_hallucinated/ folders\n",
    "# instead of a hallucination_labels.json file\n",
    "\n",
    "def stats_per_json(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Original version for the old structure with hallucination_labels.json\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\"generations\",\"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    allucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_items': allucinated_items,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "def stats_from_new_structure(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    New function for the structure with hallucinated/ and not_hallucinated/ folders\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    not_hallucinated_path = os.path.join(base_path, \"not_hallucinated\")\n",
    "    \n",
    "    # Load instance_ids from a layer (layer0) to count samples\n",
    "    hall_ids_path = os.path.join(hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    not_hall_ids_path = os.path.join(not_hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    \n",
    "    with open(hall_ids_path, 'r') as f:\n",
    "        hallucinated_ids = json.load(f)\n",
    "    with open(not_hall_ids_path, 'r') as f:\n",
    "        not_hallucinated_ids = json.load(f)\n",
    "    \n",
    "    total = len(hallucinated_ids) + len(not_hallucinated_ids)\n",
    "    hallucinations = len(hallucinated_ids)\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'not_hallucinations': len(not_hallucinated_ids),\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_ids': hallucinated_ids,\n",
    "        'not_hallucinated_ids': not_hallucinated_ids,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "def detect_structure_type(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Automatically detect whether the structure is old or new.\n",
    "    Returns 'new' if hallucinated/not_hallucinated folders exist,\n",
    "    otherwise 'old'.\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    if os.path.isdir(hallucinated_path):\n",
    "        return 'new'\n",
    "    return 'old'\n",
    "\n",
    "def get_stats(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Wrapper function that automatically detects the structure and calls the appropriate function.\n",
    "    \"\"\"\n",
    "    structure = detect_structure_type(model_name, dataset_name)\n",
    "    if structure == 'new':\n",
    "        return stats_from_new_structure(model_name, dataset_name)\n",
    "    else:\n",
    "        return stats_per_json(model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models and datasets\n",
    "available_models = os.listdir(os.path.join(PROJECT_ROOT, CACHE_DIR_NAME))\n",
    "print(\"Available models:\", available_models)\n",
    "\n",
    "# Choose model and dataset\n",
    "MODEL_NAME = \"gemma-2-9b-it\"  # Change as needed\n",
    "DATASET_NAME = \"belief_bank_constraints\"      # Change as needed\n",
    "\n",
    "# Verify the structure\n",
    "structure_type = detect_structure_type(MODEL_NAME, DATASET_NAME)\n",
    "print(f\"Data structure detected for {MODEL_NAME}/{DATASET_NAME}: {structure_type}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = get_stats(MODEL_NAME, DATASET_NAME)\n",
    "print(f\"\\nStatistics for {MODEL_NAME}:\")\n",
    "print(f\"  Total samples: {stats['total']}\")\n",
    "print(f\"  Hallucinations: {stats['hallucinations']} ({stats['percent_hallucinations']:.2f}%)\")\n",
    "\n",
    "# If you want to compare multiple models\n",
    "if \"Llama-3.1-8B-Instruct\" in available_models:\n",
    "    gemma_stats = get_stats(\"Llama-3.1-8B-Instruct\", DATASET_NAME)\n",
    "    print(f\"\\nStatistics for Llama-3.1-8B-Instruct\")\n",
    "    print(f\"  Total samples: {gemma_stats['total']}\")\n",
    "    print(f\"  Hallucinations: {gemma_stats['hallucinations']} ({gemma_stats['percent_hallucinations']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_in_model(model, dataset=None):\n",
    "    \"\"\"\n",
    "    Count the number of layers in the model.\n",
    "    Supports both old and new structure.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model)\n",
    "    \n",
    "    # If no dataset is specified, take the first available one\n",
    "    if dataset is None:\n",
    "        subdirs = [d for d in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, d))]\n",
    "        if not subdirs:\n",
    "            raise ValueError(f\"No subdirectories found in {file_path}\")\n",
    "        dataset = subdirs[0]\n",
    "    \n",
    "    layer_dir = os.path.join(file_path, dataset, \"activation_attn\")\n",
    "    \n",
    "    # Check if it's the new structure (with hallucinated/not_hallucinated folders)\n",
    "    hallucinated_path = os.path.join(layer_dir, \"hallucinated\")\n",
    "    if os.path.isdir(hallucinated_path):\n",
    "        # New structure: count layer*_activations.pt files in the hallucinated folder\n",
    "        layer_files = [f for f in os.listdir(hallucinated_path) if f.endswith('_activations.pt')]\n",
    "        return len(layer_files)\n",
    "    else:\n",
    "        # Old structure: count layer*_activations.pt files directly\n",
    "        layer_files = [f for f in os.listdir(layer_dir) if f.endswith('_activations.pt')]\n",
    "        return len(layer_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activations_and_labels(model_name, dataset_name, layer, layer_type):\n",
    "    \"\"\"\n",
    "    Load activations and labels for a given layer and type.\n",
    "    Supports both old and new data structures.\n",
    "    \n",
    "    IMPORTANT: For the new structure, activations are sorted\n",
    "    by instance_ids to ensure correct correspondence\n",
    "    between samples from different layers/types.\n",
    "    \n",
    "    Returns:\n",
    "        X: numpy array of activations (n_samples, hidden_dim) - sorted by instance_id\n",
    "        y: numpy array of labels (n_samples,) - 1=hallucination, 0=correct\n",
    "        instance_ids: numpy array of instance_ids (n_samples,) - sorted\n",
    "    \"\"\"\n",
    "    structure = detect_structure_type(model_name, dataset_name)\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, f\"activation_{layer_type}\")\n",
    "    \n",
    "    if structure == 'new':\n",
    "        # New structure: load from hallucinated/ and not_hallucinated/\n",
    "        hall_act_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer}_activations.pt\")\n",
    "        hall_ids_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer}_instance_ids.json\")\n",
    "        not_hall_act_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer}_activations.pt\")\n",
    "        not_hall_ids_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer}_instance_ids.json\")\n",
    "        \n",
    "        # Load activations\n",
    "        hall_activations = torch.load(hall_act_path)\n",
    "        not_hall_activations = torch.load(not_hall_act_path)\n",
    "        \n",
    "        # Load instance_ids\n",
    "        with open(hall_ids_path, 'r') as f:\n",
    "            hall_ids = json.load(f)\n",
    "        with open(not_hall_ids_path, 'r') as f:\n",
    "            not_hall_ids = json.load(f)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        if isinstance(hall_activations, torch.Tensor):\n",
    "            hall_activations = hall_activations.cpu().numpy().astype(np.float32)\n",
    "        if isinstance(not_hall_activations, torch.Tensor):\n",
    "            not_hall_activations = not_hall_activations.cpu().numpy().astype(np.float32)\n",
    "        \n",
    "        # Concatenate activations, labels and ids\n",
    "        X_concat = np.vstack([hall_activations, not_hall_activations])\n",
    "        y_concat = np.concatenate([\n",
    "            np.ones(hall_activations.shape[0], dtype=int),\n",
    "            np.zeros(not_hall_activations.shape[0], dtype=int)\n",
    "        ])\n",
    "        ids_concat = np.array(hall_ids + not_hall_ids)\n",
    "        \n",
    "        # Sort everything by instance_ids\n",
    "        sort_indices = np.argsort(ids_concat)\n",
    "        X = X_concat[sort_indices]\n",
    "        y = y_concat[sort_indices]\n",
    "        instance_ids = ids_concat[sort_indices]\n",
    "        \n",
    "        return X, y, instance_ids\n",
    "    \n",
    "    else:\n",
    "        # Old structure: load everything together and use hallucination_labels.json\n",
    "        file_path = os.path.join(base_path, f\"layer{layer}_activations.pt\")\n",
    "        activations = torch.load(file_path)\n",
    "        \n",
    "        if isinstance(activations, torch.Tensor):\n",
    "            X = activations.cpu().numpy().astype(np.float32)\n",
    "        else:\n",
    "            X = activations.astype(np.float32)\n",
    "        \n",
    "        # Load labels from JSON\n",
    "        labels_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \n",
    "                                   \"generations\", \"hallucination_labels.json\")\n",
    "        with open(labels_path, 'r') as f:\n",
    "            labels_data = json.load(f)\n",
    "        \n",
    "        y = np.array([item['is_hallucination'] for item in labels_data], dtype=int)\n",
    "        instance_ids = np.arange(len(y))  # Sequential IDs for old structure\n",
    "        \n",
    "        return X, y, instance_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that activations are correctly sorted by instance_id\n",
    "def verify_ordering(model_name, dataset_name, layer=0, layer_type=\"attn\"):\n",
    "    \"\"\"\n",
    "    Verify that activations are sorted by instance_id.\n",
    "    \"\"\"\n",
    "    X, y, instance_ids = load_activations_and_labels(model_name, dataset_name, layer, layer_type)\n",
    "    \n",
    "    print(f\"=== Ordering verification for {model_name}/{dataset_name} ===\")\n",
    "    print(f\"Layer: {layer}, Type: {layer_type}\")\n",
    "    print(f\"Number of samples: {len(instance_ids)}\")\n",
    "    print(f\"\\nFirst 20 instance_ids: {instance_ids[:20].tolist()}\")\n",
    "    print(f\"Last 20 instance_ids: {instance_ids[-20:].tolist()}\")\n",
    "    \n",
    "    # Verify if sorted\n",
    "    is_sorted = np.all(instance_ids[:-1] <= instance_ids[1:])\n",
    "    print(f\"\\nThe instance_ids are sorted in ascending order: {is_sorted}\")\n",
    "    \n",
    "    # Verify label correspondence\n",
    "    print(f\"\\nFirst 20 labels (y): {y[:20].tolist()}\")\n",
    "    print(f\"Last 20 labels (y): {y[-20:].tolist()}\")\n",
    "    \n",
    "    # Statistics on labels\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(f\"  Hallucination (y=1): {np.sum(y == 1)}\")\n",
    "    print(f\"  Not hallucination (y=0): {np.sum(y == 0)}\")\n",
    "    \n",
    "    return X, y, instance_ids\n",
    "\n",
    "# Execute verification\n",
    "X_test, y_test, ids_test = verify_ordering(MODEL_NAME, DATASET_NAME, layer=0, layer_type=\"attn\")\n",
    "\n",
    "# Also verify that different layers/types have the same ordering\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Verifying consistency across different layers/types...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "_, y_attn, ids_attn = load_activations_and_labels(MODEL_NAME, DATASET_NAME, 0, \"attn\")\n",
    "_, y_mlp, ids_mlp = load_activations_and_labels(MODEL_NAME, DATASET_NAME, 0, \"mlp\")\n",
    "_, y_hidden, ids_hidden = load_activations_and_labels(MODEL_NAME, DATASET_NAME, 0, \"hidden\")\n",
    "\n",
    "print(f\"IDs attn == IDs mlp: {np.array_equal(ids_attn, ids_mlp)}\")\n",
    "print(f\"IDs attn == IDs hidden: {np.array_equal(ids_attn, ids_hidden)}\")\n",
    "print(f\"Labels attn == Labels mlp: {np.array_equal(y_attn, y_mlp)}\")\n",
    "print(f\"Labels attn == Labels hidden: {np.array_equal(y_attn, y_hidden)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione\n",
    "MODELS_TO_ANALYZE = [MODEL_NAME]  # Aggiungi altri modelli se necessario\n",
    "if \"Llama-3.1-8B-Instruct\" in available_models:\n",
    "    MODELS_TO_ANALYZE.append(\"Llama-3.1-8B-Instruct\")\n",
    "\n",
    "DATASET = DATASET_NAME\n",
    "\n",
    "# Inizializza i risultati\n",
    "results = {model: {\"attn\": {}, \"mlp\": {}, \"hidden\": {}} for model in MODELS_TO_ANALYZE}\n",
    "\n",
    "# Per ogni modello\n",
    "for model in MODELS_TO_ANALYZE:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Elaborazione modello: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    num_layers = layers_in_model(model, DATASET)\n",
    "    print(f\"Numero di layer rilevati: {num_layers}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # CALCOLA INDICI UNA SOLA VOLTA\n",
    "    # ============================================\n",
    "    X_sample, y_sample, _ = load_activations_and_labels(model, DATASET, 0, \"attn\")\n",
    "    n_samples = X_sample.shape[0]\n",
    "    print(f\"Numero di campioni originali: {n_samples}\")\n",
    "    print(f\"Distribuzione originale: {np.bincount(y_sample)}\")\n",
    "    \n",
    "    del X_sample  # Libera subito\n",
    "    \n",
    "    # Undersampling globale\n",
    "    balanced_indices = get_balanced_indices(y_sample, seed=SEED)\n",
    "    y_balanced = y_sample[balanced_indices]\n",
    "    print(f\"Dopo undersampling: {len(balanced_indices)} campioni\")\n",
    "    print(f\"Distribuzione bilanciata: {np.bincount(y_balanced)}\")\n",
    "    \n",
    "    # Split stratificato sui dati bilanciati\n",
    "    train_rel_idx, test_rel_idx = train_test_split(\n",
    "        np.arange(len(balanced_indices)),\n",
    "        test_size=0.3,\n",
    "        random_state=SEED,\n",
    "        stratify=y_balanced\n",
    "    )\n",
    "    \n",
    "    # Converti in indici assoluti\n",
    "    train_indices = balanced_indices[train_rel_idx]\n",
    "    test_indices = balanced_indices[test_rel_idx]\n",
    "    \n",
    "    print(f\"Train: {len(train_indices)}, Test: {len(test_indices)}\")\n",
    "    print(f\"Train dist: {np.bincount(y_sample[train_indices])}, Test dist: {np.bincount(y_sample[test_indices])}\")\n",
    "    \n",
    "    del y_sample, y_balanced\n",
    "    gc.collect()\n",
    "    \n",
    "    # ============================================\n",
    "    # LOOP SUI LAYER\n",
    "    # ============================================\n",
    "    for layer in range(num_layers):\n",
    "        for layer_type in [\"attn\", \"mlp\", \"hidden\"]:\n",
    "            # Carica attivazioni e label\n",
    "            X_layer, y, _ = load_activations_and_labels(model, DATASET, layer, layer_type)\n",
    "            \n",
    "            # Applica gli indici PRE-CALCOLATI\n",
    "            X_train = X_layer[train_indices]\n",
    "            y_train = y[train_indices]\n",
    "            X_test = X_layer[test_indices]\n",
    "            y_test = y[test_indices]\n",
    "            \n",
    "            del X_layer, y  # Libera memoria subito\n",
    "            \n",
    "            # Normalizzazione\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Addestramento (lbfgs è più veloce per dataset piccoli)\n",
    "            clf = LogisticRegression(max_iter=10000, class_weight='balanced', solver='lbfgs', n_jobs=-1)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_proba = clf.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Metriche\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            auroc = roc_auc_score(y_test, y_proba)\n",
    "            \n",
    "            # Salva i risultati\n",
    "            results[model][layer_type][layer] = (accuracy, f1, auroc)\n",
    "            \n",
    "            print(f\"  Layer {layer} {layer_type}: Acc={accuracy:.4f}, F1={f1:.4f}, AUROC={auroc:.4f}\")\n",
    "            \n",
    "            del X_train, X_test, y_train, y_test, scaler, clf\n",
    "            gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training completato!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per ordinare tutti i layer per accuracy e salvare su JSON\n",
    "def sort_and_save_all_results(results, output_file=\"sorted_results.json\"):\n",
    "    \"\"\"\n",
    "    Ordina tutti i layer per accuracy in ordine decrescente e salva su JSON.\n",
    "    \n",
    "    Args:\n",
    "        results: dizionario completo dei risultati nel formato:\n",
    "                 {model_name: {layer_type: {layer_num: (accuracy, f1, auroc)}}}\n",
    "        output_file: path del file JSON di output\n",
    "    \n",
    "    Returns:\n",
    "        dizionario con tutti i risultati ordinati\n",
    "    \"\"\"\n",
    "    sorted_results = {}\n",
    "    \n",
    "    for model_name, layer_types in results.items():\n",
    "        sorted_results[model_name] = {}\n",
    "        \n",
    "        for layer_type, layer_data in layer_types.items():\n",
    "            # Ordina i layer per accuracy decrescente\n",
    "            sorted_layers = sorted(\n",
    "                [(layer, acc, f1, auroc) for layer, (acc, f1, auroc) in layer_data.items()],\n",
    "                key=lambda x: x[1],  # ordina per accuracy\n",
    "                reverse=True  # ordine decrescente\n",
    "            )\n",
    "            \n",
    "            # Salva in formato lista ordinata\n",
    "            sorted_results[model_name][layer_type] = [\n",
    "                {\n",
    "                    \"layer\": layer,\n",
    "                    \"accuracy\": round(acc, 4),\n",
    "                    \"f1_score\": round(f1, 4),\n",
    "                    \"auroc\": round(auroc, 4)\n",
    "                }\n",
    "                for layer, acc, f1, auroc in sorted_layers\n",
    "            ]\n",
    "    \n",
    "    # Salva su JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(sorted_results, f, indent=4)\n",
    "    print(f\"Tutti i risultati ordinati salvati in {output_file}\")\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "# Salva tutti i risultati ordinati\n",
    "sorted_all = sort_and_save_all_results(results, \"all_layers_sorted.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dabd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_from_json(json_data, model_name=None, dataset=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Genera un grafico dell'accuracy per layer.\n",
    "    \n",
    "    Args:\n",
    "        json_data (dict): Il dizionario caricato dal file JSON.\n",
    "        model_name (str): Il nome del modello da plottare.\n",
    "                          Se None, prende il primo modello trovato nel JSON.\n",
    "        dataset (str): Nome del dataset per il titolo del file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Selezione del modello\n",
    "    if model_name is None:\n",
    "        model_name = list(json_data.keys())[0]\n",
    "    \n",
    "    if model_name not in json_data:\n",
    "        print(f\"Errore: Modello '{model_name}' non trovato nel JSON.\")\n",
    "        return\n",
    "\n",
    "    data = json_data[model_name]\n",
    "\n",
    "    # 2. Configurazione dello Stile\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.weight\": \"bold\",\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"axes.labelsize\": 24,\n",
    "        \"xtick.labelsize\": 18,\n",
    "        \"ytick.labelsize\": 18,\n",
    "        \"legend.fontsize\": 12,\n",
    "        \"legend.title_fontsize\": 14,\n",
    "        \"lines.linewidth\": 2\n",
    "    })\n",
    "\n",
    "    # Creazione della figura\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Mappatura colori\n",
    "    styles = {\n",
    "        \"hidden\": {\"color\": \"red\", \"label\": \"hidden\"},\n",
    "        \"mlp\":    {\"color\": \"blue\", \"label\": \"mlp\"},\n",
    "        \"attn\":   {\"color\": \"green\", \"label\": \"attn\"}\n",
    "    }\n",
    "\n",
    "    # 3. Estrazione e Ordinamento dei dati\n",
    "    for key in [\"hidden\", \"mlp\", \"attn\"]:\n",
    "        if key in data:\n",
    "            points = data[key]\n",
    "            sorted_points = sorted(points, key=lambda x: x['layer'])\n",
    "            layers = [item['layer'] for item in sorted_points]\n",
    "            accuracies = [item['accuracy'] for item in sorted_points]\n",
    "            ax.plot(layers, accuracies, \n",
    "                    color=styles[key][\"color\"], \n",
    "                    label=styles[key][\"label\"])\n",
    "\n",
    "    # 4. Rifinitura Grafica\n",
    "    ax.set_xlabel(\"Layer\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.grid(True, linestyle='-', alpha=1.0)\n",
    "    \n",
    "    legend = ax.legend(title=\"activation\", loc=\"upper left\", frameon=True)\n",
    "    plt.setp(legend.get_title(), fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Crea la cartella img se non esiste\n",
    "    os.makedirs(\"img\", exist_ok=True)\n",
    "    plt.savefig(f\"img/{model_name}_{dataset}_activations.pdf\")\n",
    "    #plt.show()\n",
    "\n",
    "# Carica e visualizza i risultati\n",
    "content = json.load(open('all_layers_sorted_GEMMA_LLAMA_BBF.json', 'r'))\n",
    "DATASET = DATASET_NAME\n",
    "for model_name in content.keys():\n",
    "    plot_accuracy_from_json(content, model_name, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7738bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_single_model_multi_dataset(json_files, dataset_names, model_name, output_filename=None):\n",
    "    \"\"\"\n",
    "    Genera una figura con 1 riga e 3 colonne (una per dataset) per un singolo modello.\n",
    "    \n",
    "    Args:\n",
    "        json_files: lista dei path ai file JSON\n",
    "        dataset_names: lista dei nomi dei dataset (per i titoli)\n",
    "        model_name: nome del modello da plottare (es. \"gemma\" o \"llama\")\n",
    "        output_filename: nome del file di output (se None, viene generato automaticamente)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carica tutti i dati JSON\n",
    "    all_data = {}\n",
    "    for json_file, dataset_name in zip(json_files, dataset_names):\n",
    "        with open(json_file, 'r') as f:\n",
    "            all_data[dataset_name] = json.load(f)\n",
    "    \n",
    "    # Configurazione dello Stile (allineata a plot_accuracy_from_json)\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.weight\": \"bold\",\n",
    "        \"axes.labelweight\": \"bold\",\n",
    "        \"axes.labelsize\": 14,\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.titleweight\": \"bold\",\n",
    "        \"xtick.labelsize\": 12,\n",
    "        \"ytick.labelsize\": 12,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"legend.title_fontsize\": 11,\n",
    "        \"lines.linewidth\": 2\n",
    "    })\n",
    "    \n",
    "    # Mappatura colori\n",
    "    styles = {\n",
    "        \"hidden\": {\"color\": \"red\", \"label\": \"hidden\"},\n",
    "        \"mlp\":    {\"color\": \"blue\", \"label\": \"mlp\"},\n",
    "        \"attn\":   {\"color\": \"green\", \"label\": \"attn\"}\n",
    "    }\n",
    "    \n",
    "    # Creazione della figura: dimensioni compatte\n",
    "    fig, axes = plt.subplots(1, len(dataset_names), figsize=(15, 4))\n",
    "    \n",
    "    # Trova il nome completo del modello dal primo JSON\n",
    "    full_model_name = None\n",
    "    for key in all_data[dataset_names[0]].keys():\n",
    "        if model_name.lower() in key.lower():\n",
    "            full_model_name = key\n",
    "            break\n",
    "    \n",
    "    if full_model_name is None:\n",
    "        print(f\"Modello '{model_name}' non trovato nei dati.\")\n",
    "        return\n",
    "    \n",
    "    for col_idx, dataset_name in enumerate(dataset_names):\n",
    "        ax = axes[col_idx]\n",
    "        \n",
    "        # Cerca il modello nei dati del dataset\n",
    "        data = all_data[dataset_name]\n",
    "        \n",
    "        # Trova il nome del modello nel JSON\n",
    "        matching_model = None\n",
    "        for key in data.keys():\n",
    "            if model_name.lower() in key.lower():\n",
    "                matching_model = key\n",
    "                break\n",
    "        \n",
    "        if matching_model is None:\n",
    "            ax.set_title(dataset_name)\n",
    "            ax.text(0.5, 0.5, \"Model not found\", ha='center', va='center', transform=ax.transAxes)\n",
    "            continue\n",
    "        \n",
    "        model_data = data[matching_model]\n",
    "        \n",
    "        # Plotta le curve per ogni tipo di layer\n",
    "        for key in [\"hidden\", \"mlp\", \"attn\"]:\n",
    "            if key in model_data:\n",
    "                points = model_data[key]\n",
    "                sorted_points = sorted(points, key=lambda x: x['layer'])\n",
    "                layers = [item['layer'] for item in sorted_points]\n",
    "                accuracies = [item['accuracy'] for item in sorted_points]\n",
    "                ax.plot(layers, accuracies, \n",
    "                        color=styles[key][\"color\"], \n",
    "                        label=styles[key][\"label\"])\n",
    "        \n",
    "        # Titolo: nome del dataset\n",
    "        ax.set_title(dataset_name)\n",
    "        \n",
    "        # Etichette assi\n",
    "        ax.set_xlabel(\"Layer\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        \n",
    "        # Griglia\n",
    "        ax.grid(True, linestyle='-', alpha=1.0)\n",
    "        \n",
    "        # Legenda in ogni subplot\n",
    "        legend = ax.legend(title=\"activation\", loc=\"upper left\", frameon=True)\n",
    "        plt.setp(legend.get_title(), fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Crea la cartella img se non esiste\n",
    "    os.makedirs(\"img\", exist_ok=True)\n",
    "    \n",
    "    if output_filename is None:\n",
    "        output_filename = f\"{model_name}_3datasets_comparison.pdf\"\n",
    "    \n",
    "    plt.savefig(f\"img/{output_filename}\")\n",
    "    plt.show()\n",
    "    print(f\"Figura salvata in img/{output_filename}\")\n",
    "\n",
    "# File JSON disponibili\n",
    "json_files = [\n",
    "    \"all_layers_sorted_GEMMA_LLAMA_BBC.json\",\n",
    "    \"all_layers_sorted_GEMMA_LLAMA_BBF.json\",\n",
    "    \"all_layers_sorted_GEMMA_LLAMA_HE.json\"\n",
    "]\n",
    "\n",
    "# Nomi dei dataset (per i titoli delle colonne)\n",
    "dataset_names = [\n",
    "    \"Belief Bank Constraints\",\n",
    "    \"Belief Bank Facts\", \n",
    "    \"HaluEval\"\n",
    "]\n",
    "\n",
    "# Genera un grafico per Gemma\n",
    "plot_single_model_multi_dataset(json_files, dataset_names, \"gemma\", \"gemma_3datasets_comparison.pdf\")\n",
    "\n",
    "# Genera un grafico per Llama\n",
    "plot_single_model_multi_dataset(json_files, dataset_names, \"llama\", \"llama_3datasets_comparison.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
