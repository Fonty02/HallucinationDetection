{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb4f80d",
   "metadata": {},
   "source": [
    "# This notebook contains a preliminary analysis of the Universal Prober for LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619f151",
   "metadata": {},
   "source": [
    "### Libraries import and defintion of constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import traceback\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "HF_DEFAULT_HOME = os.environ.get(\"HF_HOME\", \"~\\\\.cache\\\\huggingface\\\\hub\")\n",
    "LAYER_CONFIG = {\n",
    "    \"Qwen2.5-7B\": [16,18,19],     \n",
    "    \"Falcon3-7B-Base\": [26,27]  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55019cc2",
   "metadata": {},
   "source": [
    "## Dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_per_json(model_name, dataset_name):\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\"generations\",\"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    allucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_items': allucinated_items,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_stats=stats_per_json(\"Qwen2.5-7B\", \"belief_bank\")\n",
    "falcon_stats=stats_per_json(\"Falcon3-7B-Base\", \"belief_bank\")\n",
    "print(\"Qwen2.5-7B Hallucination Stats:\", qwen_stats)\n",
    "print(\"Falcon-7B Hallucination Stats:\", falcon_stats)\n",
    "common_hallucinated = set(item for item in qwen_stats['hallucinated_items']).intersection(\n",
    "    set(item for item in falcon_stats['hallucinated_items'])\n",
    ")\n",
    "print(\"Number of common hallucinated instances between Qwen2.5-7B and Falcon-7B:\", len(common_hallucinated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d771399",
   "metadata": {},
   "source": [
    "## Model and activations stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3167f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_in_model(model):\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model)\n",
    "    #open the first subdirectory found in file_path\n",
    "    subdirs = [d for d in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, d))]\n",
    "    if not subdirs:\n",
    "        raise ValueError(f\"No subdirectories found in {file_path}\")\n",
    "    first_subdir = subdirs[0]\n",
    "    layer_files = os.path.join(file_path,first_subdir, \"activation_attn\")\n",
    "    #return number of files / 2\n",
    "    return len(os.listdir(layer_files)) // 2\n",
    "\n",
    "qwen_layers = layers_in_model(\"Qwen2.5-7B\")\n",
    "falcon_layers = layers_in_model(\"Falcon3-7B-Base\")\n",
    "print(\"Number of layers in Qwen2.5-7B:\", qwen_layers)\n",
    "print(\"Number of layers in Falcon-7B:\", falcon_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11912745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubplots(model, dataset, num_layers, type, model_stats, dim_type,directory_to_save):\n",
    "    # Calculate grid dimensions\n",
    "    cols = 4\n",
    "    rows = (num_layers + cols - 1) // cols\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(32, 8*rows))\n",
    "    axs = axs.flatten()  # Flatten to handle single row/col cases\n",
    "    \n",
    "    # Pre-calcola gli indici allucinati per efficienza\n",
    "    hallucinated_indices = set(model_stats['hallucinated_items'])\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model, dataset, \"activation_\"+type, f\"layer{layer}_activations.pt\")\n",
    "        \n",
    "        # Caricamento attivazioni\n",
    "        try:\n",
    "            activations = torch.load(file_path)\n",
    "            # Se è un tensor GPU, portalo su CPU e convertilo in numpy\n",
    "            if isinstance(activations, torch.Tensor):\n",
    "                activations = activations.cpu().numpy()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File non trovato: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        activations_2d = None\n",
    "        var_text = \"\"\n",
    "\n",
    "        # --- IMPLEMENTAZIONE DIMENSION REDUCTION ---\n",
    "        if dim_type == \"PCA\":\n",
    "            pca = PCA(n_components=2)\n",
    "            activations_2d = pca.fit_transform(activations)\n",
    "            var_text = f'(Var: {pca.explained_variance_ratio_[0]:.2%}, {pca.explained_variance_ratio_[1]:.2%})'\n",
    "        \n",
    "        \n",
    "\n",
    "        # --- PLOTTING ---\n",
    "        if activations_2d is not None:\n",
    "            colors = ['red' if i in hallucinated_indices else 'blue' for i in range(activations_2d.shape[0])]\n",
    "            \n",
    "            # Scatter plot\n",
    "            # Aumentato leggermente 's' (dimensione punti) e ridotto alpha per vedere meglio la densità\n",
    "            axs[layer].scatter(activations_2d[:, 0], activations_2d[:, 1], c=colors, alpha=0.5, s=10)\n",
    "            \n",
    "            axs[layer].set_title(f'Layer {layer} {var_text}', fontsize=12, fontweight='bold')\n",
    "            axs[layer].set_xlabel(f'{dim_type} 1', fontsize=10)\n",
    "            axs[layer].set_ylabel(f'{dim_type} 2', fontsize=10)\n",
    "            axs[layer].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Leave unused subplots empty\n",
    "    for i in range(num_layers, len(axs)):\n",
    "        axs[i].axis('off')\n",
    "    \n",
    "    # Add legend to figure (top right corner of the entire figure)\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='red', label='Hallucinated'),\n",
    "                       Patch(facecolor='blue', label='Non-hallucinated')]\n",
    "    fig.legend(handles=legend_elements, loc='upper right', fontsize=12, bbox_to_anchor=(0.98, 0.98))\n",
    "    \n",
    "    fig.suptitle(f'Activations {dim_type} for {model} - {type} layers\\n(Red: Hallucinated, Blue: Non-hallucinated)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(directory_to_save, f'{model}_{dataset}_{type}_activations_{dim_type}.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Salvato plot per {model} - {type} - {dim_type}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim_type in [\"PCA\"]:\n",
    "    directory_to_save = f\"activation_plots_{dim_type}\"\n",
    "    os.makedirs(directory_to_save, exist_ok=True)\n",
    "    for type in ['attn', 'mlp',\"hidden\"]:\n",
    "            createSubplots(\"Qwen2.5-7B\", \"belief_bank\", qwen_layers, type, qwen_stats, dim_type,directory_to_save)\n",
    "            createSubplots(\"Falcon3-7B-Base\", \"belief_bank\", falcon_layers, type, falcon_stats, dim_type,directory_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46feb8a3",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_concatenated_layers(model_name, dataset_name, layer_indices, type_layer, stats):\n",
    "    \"\"\"\n",
    "    Carica multipli layer e li concatena.\n",
    "    \"\"\"\n",
    "    print(f\"   Caricamento {model_name} [{type_layer}]: layers {layer_indices}...\")\n",
    "    combined_features = []\n",
    "    y = None\n",
    "    \n",
    "    total_samples = stats['total']\n",
    "    hallucinated_set = set(stats['hallucinated_items'])\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_\"+type_layer, f\"layer{layer_idx}_activations.pt\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: Layer {layer_idx} non trovato in {file_path}. Salto.\")\n",
    "            continue\n",
    "            \n",
    "        activations = torch.load(file_path)\n",
    "        if isinstance(activations, torch.Tensor):\n",
    "            X_layer = activations.cpu().numpy().astype(np.float32)\n",
    "        else:\n",
    "            X_layer = activations.astype(np.float32)\n",
    "            \n",
    "        if X_layer.shape[0] > total_samples:\n",
    "            X_layer = X_layer[:total_samples]\n",
    "            \n",
    "        combined_features.append(X_layer)\n",
    "        \n",
    "        if y is None:\n",
    "            y = np.zeros(X_layer.shape[0], dtype=int)\n",
    "            for i in range(len(y)):\n",
    "                if i in hallucinated_set:\n",
    "                    y[i] = 1\n",
    "\n",
    "    if not combined_features:\n",
    "        raise ValueError(f\"Nessun layer caricato per {model_name}\")\n",
    "\n",
    "    X_final = np.concatenate(combined_features, axis=1)\n",
    "    return X_final, y\n",
    "\n",
    "def run_experiment_pipeline_cached(X_teacher, y_teacher, teacher_name, \n",
    "                                   X_student, y_student, student_name, layer_type):\n",
    "    \"\"\"\n",
    "    Esegue l'esperimento con dati già splittati e normalizzati.\n",
    "    (X_teacher, y_teacher, X_student, y_student sono già train/test split e normalizzati)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== EXPERIMENT: {layer_type.upper()} LAYERS ({teacher_name} → {student_name}) ===\")\n",
    "    print(f\"Teacher Input Shape ({teacher_name}): Train={X_teacher['X_train'].shape}, Test={X_teacher['X_test'].shape}\")\n",
    "    print(f\"Student Input Shape ({student_name}): Train={X_student['X_train'].shape}, Test={X_student['X_test'].shape}\")\n",
    "    \n",
    "    X_A_train = X_teacher['X_train']\n",
    "    X_A_test = X_teacher['X_test']\n",
    "    y_A_train = y_teacher['y_train']\n",
    "    y_A_test = y_teacher['y_test']\n",
    "    \n",
    "    X_B_train = X_student['X_train']\n",
    "    X_B_test = X_student['X_test']\n",
    "    y_B_train = y_student['y_train']\n",
    "    y_B_test = y_student['y_test']\n",
    "\n",
    "    # --- STEP 1: Teacher Probing ---\n",
    "    print(f\"1. Training Teacher Probe ({teacher_name})...\")\n",
    "    probe_teacher = LogisticRegression(max_iter=1000, class_weight='balanced', solver='lbfgs', n_jobs=-1)\n",
    "    probe_teacher.fit(X_A_train, y_A_train)\n",
    "    \n",
    "    # --- METRICHE TEACHER ---\n",
    "    y_pred_teacher = probe_teacher.predict(X_A_test)\n",
    "    cm_teacher = confusion_matrix(y_A_test, y_pred_teacher)\n",
    "    acc_teacher = accuracy_score(y_A_test, y_pred_teacher)\n",
    "    prec_teacher = precision_score(y_A_test, y_pred_teacher)\n",
    "    rec_teacher = recall_score(y_A_test, y_pred_teacher)\n",
    "    f1_teacher = f1_score(y_A_test, y_pred_teacher)\n",
    "    \n",
    "\n",
    "    # --- STEP 2: Alignment ---\n",
    "    print(f\"2. Learning Linear Projection ({student_name} → {teacher_name})...\")\n",
    "    aligner = Ridge(alpha=1000.0, fit_intercept=False) \n",
    "    aligner.fit(X_B_train, X_A_train) \n",
    "    \n",
    "    # --- STEP 3: StudentOnTeacher (Cross-Model) ---\n",
    "    print(f\"3. Projecting {student_name} & Testing with {teacher_name} Probe...\")\n",
    "    X_B_test_projected = aligner.predict(X_B_test)\n",
    "    y_pred_cross = probe_teacher.predict(X_B_test_projected)\n",
    "    \n",
    "    # --- METRICHE CROSS-MODEL ---\n",
    "    cm_cross = confusion_matrix(y_B_test, y_pred_cross)\n",
    "    acc_cross = accuracy_score(y_B_test, y_pred_cross)\n",
    "    prec_cross = precision_score(y_B_test, y_pred_cross)\n",
    "    rec_cross = recall_score(y_B_test, y_pred_cross)\n",
    "    f1_cross = f1_score(y_B_test, y_pred_cross)\n",
    "    \n",
    "    print(f\"   -> {student_name} on {teacher_name} Accuracy: {acc_cross:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"type\": layer_type,\n",
    "        \"teacher_name\": teacher_name,\n",
    "        \"student_name\": student_name,\n",
    "        \"teacher\": {\n",
    "            \"accuracy\": acc_teacher,\n",
    "            \"precision\": prec_teacher,\n",
    "            \"recall\": rec_teacher,\n",
    "            \"f1\": f1_teacher,\n",
    "            \"confusion_matrix\": cm_teacher.tolist()\n",
    "        },\n",
    "        \"student_on_teacher\": {\n",
    "            \"accuracy\": acc_cross,\n",
    "            \"precision\": prec_cross,\n",
    "            \"recall\": rec_cross,\n",
    "            \"f1\": f1_cross,\n",
    "            \"confusion_matrix\": cm_cross.tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm, layer_type, model_name=\"\", save_dir=\"confusion_matrices\"):\n",
    "    \"\"\"\n",
    "    Plotta e salva la confusion matrix come immagine.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,\n",
    "                xticklabels=['Non-Hallucinated', 'Hallucinated'],\n",
    "                yticklabels=['Non-Hallucinated', 'Hallucinated'])\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    title = f'Confusion Matrix - {layer_type.upper()} Layers'\n",
    "    if model_name:\n",
    "        title += f' ({model_name})'\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = os.path.join(save_dir, f'confusion_matrix_{layer_type}_{model_name}.png' if model_name else f'confusion_matrix_{layer_type}.png')\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   ✓ Salvato: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dddcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_CONFIG = {\n",
    "    \"Qwen2.5-7B\": list(range(19, 24)),      \n",
    "    \"Falcon3-7B-Base\": list(range(23, 27))  \n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 1: PRE-CARICAMENTO E SPLITTING DEI DATI (stessi indici shuffled per TUTTI i layer type)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "n_samples = qwen_stats['total'] \n",
    "rng = np.random.RandomState(42)\n",
    "shuffled_indices = rng.permutation(n_samples)\n",
    "split_idx = int(0.7 * n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:split_idx]\n",
    "test_indices = shuffled_indices[split_idx:]\n",
    "\n",
    "\n",
    "data_splits = {}\n",
    "for layer_type in ['attn', 'mlp', 'hidden']:\n",
    "    gc.collect()\n",
    "    \n",
    "    # Carica Qwen\n",
    "    X_qwen, y_qwen = load_concatenated_layers(\n",
    "        \"Qwen2.5-7B\", \"belief_bank\", \n",
    "        LAYER_CONFIG[\"Qwen2.5-7B\"], \n",
    "        layer_type, qwen_stats\n",
    "    )\n",
    "    \n",
    "    # Carica Falcon\n",
    "    X_falcon, y_falcon = load_concatenated_layers(\n",
    "        \"Falcon3-7B-Base\", \"belief_bank\", \n",
    "        LAYER_CONFIG[\"Falcon3-7B-Base\"], \n",
    "        layer_type, falcon_stats\n",
    "    )\n",
    "    \n",
    "    # Applica gli STESSI indici a entrambi i modelli\n",
    "    X_qwen_train, X_qwen_test = X_qwen[train_indices], X_qwen[test_indices]\n",
    "    y_qwen_train, y_qwen_test = y_qwen[train_indices], y_qwen[test_indices]\n",
    "    \n",
    "    X_falcon_train, X_falcon_test = X_falcon[train_indices], X_falcon[test_indices]\n",
    "    y_falcon_train, y_falcon_test = y_falcon[train_indices], y_falcon[test_indices]\n",
    "    \n",
    "    # Normalizza una sola volta\n",
    "    scaler_qwen = StandardScaler()\n",
    "    X_qwen_train = scaler_qwen.fit_transform(X_qwen_train)\n",
    "    X_qwen_test = scaler_qwen.transform(X_qwen_test)\n",
    "    \n",
    "    scaler_falcon = StandardScaler()\n",
    "    X_falcon_train = scaler_falcon.fit_transform(X_falcon_train)\n",
    "    X_falcon_test = scaler_falcon.transform(X_falcon_test)\n",
    "    \n",
    "    # Salva in un dizionario le informazioni\n",
    "    data_splits[layer_type] = {\n",
    "        \"qwen\": {\n",
    "            \"X_train\": X_qwen_train,\n",
    "            \"X_test\": X_qwen_test,\n",
    "            \"y_train\": y_qwen_train,\n",
    "            \"y_test\": y_qwen_test\n",
    "        },\n",
    "        \"falcon\": {\n",
    "            \"X_train\": X_falcon_train,\n",
    "            \"X_test\": X_falcon_test,\n",
    "            \"y_train\": y_falcon_train,\n",
    "            \"y_test\": y_falcon_test\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #li cancello poichè ho tutto quello che mi serve nel dizionario\n",
    "    del X_qwen, y_qwen, X_falcon, y_falcon\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FASE 2: ESECUZIONE ESPERIMENTI SU ENTRAMBI GLI SCENARI\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Definisci gli scenari di esperimento\n",
    "scenarios = [\n",
    "    {\n",
    "        \"teacher_model\": \"Qwen2.5-7B\",\n",
    "        \"student_model\": \"Falcon3-7B-Base\",\n",
    "    },\n",
    "    {\n",
    "        \"teacher_model\": \"Falcon3-7B-Base\",\n",
    "        \"student_model\": \"Qwen2.5-7B\",\n",
    "    }\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Loop su entrambi gli scenari\n",
    "for scenario_idx, scenario in enumerate(scenarios, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SCENARIO {scenario_idx}: {scenario['teacher_model']} → {scenario['student_model']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Loop sui 3 tipi di layer richiesti\n",
    "    for layer_type in ['attn', 'mlp', 'hidden']:\n",
    "        \n",
    "        try:\n",
    "            # Recupera i dati pre-splittati e normalizzati\n",
    "            if scenario['teacher_model'] == \"Qwen2.5-7B\":\n",
    "                X_teacher_data = data_splits[layer_type]['qwen']\n",
    "                X_student_data = data_splits[layer_type]['falcon']\n",
    "            else:\n",
    "                X_teacher_data = data_splits[layer_type]['falcon']\n",
    "                X_student_data = data_splits[layer_type]['qwen']\n",
    "            \n",
    "            # Esegui pipeline con dati della cache\n",
    "            res = run_experiment_pipeline_cached(\n",
    "                X_teacher_data, X_teacher_data, scenario['teacher_model'],\n",
    "                X_student_data, X_student_data, scenario['student_model'],\n",
    "                layer_type\n",
    "            )\n",
    "            results.append(res)\n",
    "            \n",
    "            # 4. Plotta confusion matrices\n",
    "            print(f\"\\n   Creazione visualizzazioni confusion matrices...\")\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"Teacher_{scenario['teacher_model'].split('.')[0]}\"\n",
    "            )\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['student_on_teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"{scenario['student_model'].split('.')[0]}_on_{scenario['teacher_model'].split('.')[0]}\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore critico nel layer {layer_type}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    all_results.append({\n",
    "        \"scenario\": f\"{scenario['teacher_model']} (teacher) → {scenario['student_model']} (student)\",\n",
    "        \"results\": results\n",
    "    })\n",
    "\n",
    "# Salva tutti i risultati in JSON\n",
    "os.makedirs(\"results_metrics\", exist_ok=True)\n",
    "metrics_file = \"results_metrics/experiment_results_all_scenarios.json\"\n",
    "\n",
    "all_results_json = []\n",
    "for scenario_data in all_results:\n",
    "    scenario_results = []\n",
    "    for r in scenario_data['results']:\n",
    "        scenario_results.append({\n",
    "            \"layer_type\": r['type'],\n",
    "            \"teacher_model\": r['teacher_name'],\n",
    "            \"student_model\": r['student_name'],\n",
    "            \"teacher\": {\n",
    "                \"accuracy\": round(r['teacher']['accuracy'], 4),\n",
    "                \"precision\": round(r['teacher']['precision'], 4),\n",
    "                \"recall\": round(r['teacher']['recall'], 4),\n",
    "                \"f1_score\": round(r['teacher']['f1'], 4),\n",
    "                \"confusion_matrix\": {\n",
    "                    \"TN\": int(r['teacher']['confusion_matrix'][0][0]),\n",
    "                    \"FP\": int(r['teacher']['confusion_matrix'][0][1]),\n",
    "                    \"FN\": int(r['teacher']['confusion_matrix'][1][0]),\n",
    "                    \"TP\": int(r['teacher']['confusion_matrix'][1][1])\n",
    "                }\n",
    "            },\n",
    "            \"student_on_teacher\": {\n",
    "                \"accuracy\": round(r['student_on_teacher']['accuracy'], 4),\n",
    "                \"precision\": round(r['student_on_teacher']['precision'], 4),\n",
    "                \"recall\": round(r['student_on_teacher']['recall'], 4),\n",
    "                \"f1_score\": round(r['student_on_teacher']['f1'], 4),\n",
    "                \"confusion_matrix\": {\n",
    "                    \"TN\": int(r['student_on_teacher']['confusion_matrix'][0][0]),\n",
    "                    \"FP\": int(r['student_on_teacher']['confusion_matrix'][0][1]),\n",
    "                    \"FN\": int(r['student_on_teacher']['confusion_matrix'][1][0]),\n",
    "                    \"TP\": int(r['student_on_teacher']['confusion_matrix'][1][1])\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    all_results_json.append({\n",
    "        \"scenario\": scenario_data['scenario'],\n",
    "        \"results\": scenario_results\n",
    "    })\n",
    "\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(all_results_json, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Risultati salvati in: {metrics_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ef4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensione dei modelli\n",
    "\n",
    "print(\"=== Controllo Dimensioni Vettori ===\")\n",
    "\n",
    "dataset = \"belief_bank\"\n",
    "types = [\"attn\", \"mlp\", \"hidden\"]\n",
    "\n",
    "for model_name, layers in LAYER_CONFIG.items():\n",
    "    print(f\"\\nModello: {model_name}\")\n",
    "    # Prendiamo il primo layer disponibile nella configurazione\n",
    "    layer_idx = layers[0]\n",
    "    \n",
    "    for type_layer in types:\n",
    "        file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset, \"activation_\"+type_layer, f\"layer{layer_idx}_activations.pt\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(file_path):\n",
    "                activations = torch.load(file_path)\n",
    "                # Gestione sia per Tensor che per Numpy array\n",
    "                shape = activations.shape if hasattr(activations, 'shape') else \"Unknown\"\n",
    "                print(f\"  - Type: {type_layer:<10} | Layer: {layer_idx} | Shape: {shape}\")\n",
    "            else:\n",
    "                print(f\"  - Type: {type_layer:<10} | Layer: {layer_idx} | File non trovato\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Type: {type_layer:<10} | Layer: {layer_idx} | Errore: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
