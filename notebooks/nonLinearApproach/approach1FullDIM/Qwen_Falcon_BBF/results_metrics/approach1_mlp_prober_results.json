[
  {
    "scenario": "Qwen2.5-7B \u2192 Falcon3-7B-Base",
    "results": [
      {
        "layer_type": "attn",
        "teacher_model": "Qwen2.5-7B",
        "student_model": "Falcon3-7B-Base",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "alignment_model_info": {
          "architecture": "AlignmentNetwork",
          "input_dim": 9216,
          "output_dim": 10752,
          "hidden_dim": 128,
          "dropout": 0.5,
          "activation": "GELU",
          "normalization": "LayerNorm",
          "residual_connection": true
        },
        "alignment_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.1,
          "batch_size": 32,
          "max_epochs": 1000,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 50,
          "early_stopping_min_delta": 0.0001
        },
        "alignment_loss_function": {
          "type": "MixedLoss",
          "mse_weight": 0.01,
          "cosine_weight": 1.0
        },
        "prober_model_info": {
          "architecture": "MLPProber",
          "input_dim": 10752,
          "hidden_dim": 64,
          "dropout": 0.5
        },
        "prober_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.01,
          "batch_size": 64,
          "max_epochs": 200,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 30,
          "early_stopping_min_delta": 0.0001,
          "loss_function": "BCEWithLogitsLoss",
          "use_class_weights": true
        },
        "training_results": {
          "alignment_network": {
            "best_val_loss": 0.137682,
            "epochs_trained": 119,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\alignment_models\\alignment_attn_Falcon3-7B-Base_to_Qwen2.5-7B.pt"
          },
          "mlp_prober": {
            "best_val_acc": 0.9933,
            "epochs_trained": 25,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\prober_models\\mlp_prober_attn_Qwen2.5-7B.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9925,
            "precision": 0.9945,
            "recall": 0.9909,
            "f1_score": 0.9927,
            "auroc": 0.9996,
            "confusion_matrix": {
              "TN": 1035,
              "FP": 6,
              "FN": 10,
              "TP": 1088
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9812,
            "precision": 0.9845,
            "recall": 0.9769,
            "f1_score": 0.9807,
            "auroc": 0.9944,
            "confusion_matrix": {
              "TN": 2274,
              "FP": 34,
              "FN": 51,
              "TP": 2160
            }
          }
        }
      },
      {
        "layer_type": "mlp",
        "teacher_model": "Qwen2.5-7B",
        "student_model": "Falcon3-7B-Base",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "alignment_model_info": {
          "architecture": "AlignmentNetwork",
          "input_dim": 9216,
          "output_dim": 10752,
          "hidden_dim": 128,
          "dropout": 0.5,
          "activation": "GELU",
          "normalization": "LayerNorm",
          "residual_connection": true
        },
        "alignment_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.1,
          "batch_size": 32,
          "max_epochs": 1000,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 50,
          "early_stopping_min_delta": 0.0001
        },
        "alignment_loss_function": {
          "type": "MixedLoss",
          "mse_weight": 0.01,
          "cosine_weight": 1.0
        },
        "prober_model_info": {
          "architecture": "MLPProber",
          "input_dim": 10752,
          "hidden_dim": 64,
          "dropout": 0.5
        },
        "prober_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.01,
          "batch_size": 64,
          "max_epochs": 200,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 30,
          "early_stopping_min_delta": 0.0001,
          "loss_function": "BCEWithLogitsLoss",
          "use_class_weights": true
        },
        "training_results": {
          "alignment_network": {
            "best_val_loss": 0.120942,
            "epochs_trained": 112,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\alignment_models\\alignment_mlp_Falcon3-7B-Base_to_Qwen2.5-7B.pt"
          },
          "mlp_prober": {
            "best_val_acc": 0.9947,
            "epochs_trained": 42,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\prober_models\\mlp_prober_mlp_Qwen2.5-7B.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9939,
            "precision": 0.9963,
            "recall": 0.9918,
            "f1_score": 0.9941,
            "auroc": 0.9998,
            "confusion_matrix": {
              "TN": 1037,
              "FP": 4,
              "FN": 9,
              "TP": 1089
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9774,
            "precision": 0.9813,
            "recall": 0.9724,
            "f1_score": 0.9768,
            "auroc": 0.994,
            "confusion_matrix": {
              "TN": 2267,
              "FP": 41,
              "FN": 61,
              "TP": 2150
            }
          }
        }
      },
      {
        "layer_type": "hidden",
        "teacher_model": "Qwen2.5-7B",
        "student_model": "Falcon3-7B-Base",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "alignment_model_info": {
          "architecture": "AlignmentNetwork",
          "input_dim": 9216,
          "output_dim": 10752,
          "hidden_dim": 128,
          "dropout": 0.5,
          "activation": "GELU",
          "normalization": "LayerNorm",
          "residual_connection": true
        },
        "alignment_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.1,
          "batch_size": 32,
          "max_epochs": 1000,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 50,
          "early_stopping_min_delta": 0.0001
        },
        "alignment_loss_function": {
          "type": "MixedLoss",
          "mse_weight": 0.01,
          "cosine_weight": 1.0
        },
        "prober_model_info": {
          "architecture": "MLPProber",
          "input_dim": 10752,
          "hidden_dim": 64,
          "dropout": 0.5
        },
        "prober_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.01,
          "batch_size": 64,
          "max_epochs": 200,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 30,
          "early_stopping_min_delta": 0.0001,
          "loss_function": "BCEWithLogitsLoss",
          "use_class_weights": true
        },
        "training_results": {
          "alignment_network": {
            "best_val_loss": 0.114773,
            "epochs_trained": 119,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\alignment_models\\alignment_hidden_Falcon3-7B-Base_to_Qwen2.5-7B.pt"
          },
          "mlp_prober": {
            "best_val_acc": 0.9947,
            "epochs_trained": 50,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\prober_models\\mlp_prober_hidden_Qwen2.5-7B.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9916,
            "precision": 0.9954,
            "recall": 0.9882,
            "f1_score": 0.9918,
            "auroc": 0.9999,
            "confusion_matrix": {
              "TN": 1036,
              "FP": 5,
              "FN": 13,
              "TP": 1085
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9805,
            "precision": 0.9858,
            "recall": 0.9742,
            "f1_score": 0.98,
            "auroc": 0.9958,
            "confusion_matrix": {
              "TN": 2277,
              "FP": 31,
              "FN": 57,
              "TP": 2154
            }
          }
        }
      }
    ]
  },
  {
    "scenario": "Falcon3-7B-Base \u2192 Qwen2.5-7B",
    "results": [
      {
        "layer_type": "attn",
        "teacher_model": "Falcon3-7B-Base",
        "student_model": "Qwen2.5-7B",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "alignment_model_info": {
          "architecture": "AlignmentNetwork",
          "input_dim": 10752,
          "output_dim": 9216,
          "hidden_dim": 128,
          "dropout": 0.5,
          "activation": "GELU",
          "normalization": "LayerNorm",
          "residual_connection": true
        },
        "alignment_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.1,
          "batch_size": 32,
          "max_epochs": 1000,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 50,
          "early_stopping_min_delta": 0.0001
        },
        "alignment_loss_function": {
          "type": "MixedLoss",
          "mse_weight": 0.01,
          "cosine_weight": 1.0
        },
        "prober_model_info": {
          "architecture": "MLPProber",
          "input_dim": 9216,
          "hidden_dim": 64,
          "dropout": 0.5
        },
        "prober_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.01,
          "batch_size": 64,
          "max_epochs": 200,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 30,
          "early_stopping_min_delta": 0.0001,
          "loss_function": "BCEWithLogitsLoss",
          "use_class_weights": true
        },
        "training_results": {
          "alignment_network": {
            "best_val_loss": 0.362954,
            "epochs_trained": 84,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\alignment_models\\alignment_attn_Qwen2.5-7B_to_Falcon3-7B-Base.pt"
          },
          "mlp_prober": {
            "best_val_acc": 0.9956,
            "epochs_trained": 69,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\prober_models\\mlp_prober_attn_Falcon3-7B-Base.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9916,
            "precision": 0.9919,
            "recall": 0.991,
            "f1_score": 0.9914,
            "auroc": 0.9994,
            "confusion_matrix": {
              "TN": 2290,
              "FP": 18,
              "FN": 20,
              "TP": 2191
            }
          },
          "student_on_teacher": {
            "accuracy": 0.8897,
            "precision": 0.8767,
            "recall": 0.9135,
            "f1_score": 0.8947,
            "auroc": 0.9383,
            "confusion_matrix": {
              "TN": 900,
              "FP": 141,
              "FN": 95,
              "TP": 1003
            }
          }
        }
      },
      {
        "layer_type": "mlp",
        "teacher_model": "Falcon3-7B-Base",
        "student_model": "Qwen2.5-7B",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "alignment_model_info": {
          "architecture": "AlignmentNetwork",
          "input_dim": 10752,
          "output_dim": 9216,
          "hidden_dim": 128,
          "dropout": 0.5,
          "activation": "GELU",
          "normalization": "LayerNorm",
          "residual_connection": true
        },
        "alignment_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.1,
          "batch_size": 32,
          "max_epochs": 1000,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 50,
          "early_stopping_min_delta": 0.0001
        },
        "alignment_loss_function": {
          "type": "MixedLoss",
          "mse_weight": 0.01,
          "cosine_weight": 1.0
        },
        "prober_model_info": {
          "architecture": "MLPProber",
          "input_dim": 9216,
          "hidden_dim": 64,
          "dropout": 0.5
        },
        "prober_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.01,
          "batch_size": 64,
          "max_epochs": 200,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 30,
          "early_stopping_min_delta": 0.0001,
          "loss_function": "BCEWithLogitsLoss",
          "use_class_weights": true
        },
        "training_results": {
          "alignment_network": {
            "best_val_loss": 0.347877,
            "epochs_trained": 82,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\alignment_models\\alignment_mlp_Qwen2.5-7B_to_Falcon3-7B-Base.pt"
          },
          "mlp_prober": {
            "best_val_acc": 0.9943,
            "epochs_trained": 57,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\prober_models\\mlp_prober_mlp_Falcon3-7B-Base.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9876,
            "precision": 0.99,
            "recall": 0.9846,
            "f1_score": 0.9873,
            "auroc": 0.9991,
            "confusion_matrix": {
              "TN": 2286,
              "FP": 22,
              "FN": 34,
              "TP": 2177
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9411,
            "precision": 0.9746,
            "recall": 0.9089,
            "f1_score": 0.9406,
            "auroc": 0.988,
            "confusion_matrix": {
              "TN": 1015,
              "FP": 26,
              "FN": 100,
              "TP": 998
            }
          }
        }
      },
      {
        "layer_type": "hidden",
        "teacher_model": "Falcon3-7B-Base",
        "student_model": "Qwen2.5-7B",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "alignment_model_info": {
          "architecture": "AlignmentNetwork",
          "input_dim": 10752,
          "output_dim": 9216,
          "hidden_dim": 128,
          "dropout": 0.5,
          "activation": "GELU",
          "normalization": "LayerNorm",
          "residual_connection": true
        },
        "alignment_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.1,
          "batch_size": 32,
          "max_epochs": 1000,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 50,
          "early_stopping_min_delta": 0.0001
        },
        "alignment_loss_function": {
          "type": "MixedLoss",
          "mse_weight": 0.01,
          "cosine_weight": 1.0
        },
        "prober_model_info": {
          "architecture": "MLPProber",
          "input_dim": 9216,
          "hidden_dim": 64,
          "dropout": 0.5
        },
        "prober_training_hyperparameters": {
          "optimizer": "AdamW",
          "learning_rate": 0.001,
          "weight_decay": 0.01,
          "batch_size": 64,
          "max_epochs": 200,
          "scheduler": "CosineAnnealingLR",
          "gradient_clip_max_norm": 1.0,
          "early_stopping_patience": 30,
          "early_stopping_min_delta": 0.0001,
          "loss_function": "BCEWithLogitsLoss",
          "use_class_weights": true
        },
        "training_results": {
          "alignment_network": {
            "best_val_loss": 0.361099,
            "epochs_trained": 82,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\alignment_models\\alignment_hidden_Qwen2.5-7B_to_Falcon3-7B-Base.pt"
          },
          "mlp_prober": {
            "best_val_acc": 0.9962,
            "epochs_trained": 130,
            "model_saved_path": "c:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach1FullDIM\\prober_models\\mlp_prober_hidden_Falcon3-7B-Base.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9905,
            "precision": 0.9909,
            "recall": 0.9896,
            "f1_score": 0.9903,
            "auroc": 0.9988,
            "confusion_matrix": {
              "TN": 2288,
              "FP": 20,
              "FN": 23,
              "TP": 2188
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9313,
            "precision": 0.9694,
            "recall": 0.8944,
            "f1_score": 0.9304,
            "auroc": 0.9847,
            "confusion_matrix": {
              "TN": 1010,
              "FP": 31,
              "FN": 116,
              "TP": 982
            }
          }
        }
      }
    ]
  }
]