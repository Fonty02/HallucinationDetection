{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca74b2b6",
   "metadata": {},
   "source": [
    "# Approach 1: Non-linear adaptation with AdapterMLP and MLP Prober\n",
    "\n",
    "In this notebook a second non-linear approach is tested. We take all the activations of both LLMs, we train 3 MLP classifiers (1 per type of layer) for the Teacher model, with an AdapterMLP we try to adapt the Student latent space to the Teacher one. Finally we test the adapted Student activations with the Teacher MLP classifiers.\n",
    "\n",
    "**Difference from Approach 1:** Here we use an MLP instead of Logistic Regression as the probing classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import traceback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "# ==================================================================\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# ==================================================================\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def get_generator(seed=SEED):\n",
    "    \"\"\"Create a reproducible generator for DataLoader\"\"\"\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return g\n",
    "\n",
    "# Set seeds at import time\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3df4d828",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "HF_DEFAULT_HOME = os.environ.get(\"HF_HOME\", \"~\\\\.cache\\\\huggingface\\\\hub\")\n",
    "\n",
    "# We test the same layers as in the linear approach\n",
    "LAYER_CONFIG = {\n",
    "    \"Qwen2.5-7B\": \n",
    "    {\n",
    "        \"attn\": [15,16,18],\n",
    "        \"mlp\":[16,18,20],\n",
    "        \"hidden\": [18,19,20]\n",
    "    },    \n",
    "    \"Falcon3-7B-Base\": \n",
    "    {\n",
    "        \"attn\": [2,7,12],\n",
    "        \"mlp\":[10,11,12],\n",
    "        \"hidden\": [2,3,19]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41961315",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c619539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_per_json(model_name, dataset_name):\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\"generations\",\"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    allucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_items': allucinated_items,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "\n",
    "qwen_stats=stats_per_json(\"Qwen2.5-7B\", \"belief_bank\")\n",
    "falcon_stats=stats_per_json(\"Falcon3-7B-Base\", \"belief_bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e23f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. Dataset class for Alignment\n",
    "# ------------------------------------------------------------------\n",
    "class AlignmentDataset(Dataset):\n",
    "    def __init__(self, x_source: torch.Tensor, x_target: torch.Tensor):\n",
    "        self.x_source = x_source\n",
    "        self.x_target = x_target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_source.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_source[idx], self.x_target[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Dataset class for Classification\n",
    "# ------------------------------------------------------------------\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. AlignmentNetwork (same as approach1)\n",
    "# ------------------------------------------------------------------\n",
    "class AlignmentNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 128, dropout: float = 0.5):\n",
    "        \"\"\"\n",
    "        Architettura a \"Clessidra\" (Bottleneck) molto stretto.\n",
    "        hidden_dim=128 su 10k input costringe a imparare solo le feature principali.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Proiezione Lineare di base (Identity se dim uguali)\n",
    "        if input_dim != output_dim:\n",
    "            self.input_proj = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        else:\n",
    "            self.input_proj = nn.Identity()\n",
    "\n",
    "        # Ramo Non-Lineare (Bottleneck Estremo)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim), # Compressione forte (es. 10000 -> 128)\n",
    "            nn.LayerNorm(hidden_dim),          # Normalizzazione\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),               # Dropout aggressivo (0.5)\n",
    "            nn.Linear(hidden_dim, output_dim), # Decompressione\n",
    "            nn.Dropout(dropout)                # Dropout finale\n",
    "        )\n",
    "        \n",
    "        # Zero-Init per partire come una funzione lineare pura\n",
    "        self._init_zero()\n",
    "\n",
    "    def _init_zero(self):\n",
    "        # L'ultimo layer del residuo parte da zero.\n",
    "        # Al passo 0, la rete è ESATTAMENTE lineare (solo self.input_proj).\n",
    "        nn.init.zeros_(self.net[-2].weight) # -2 perché c'è il dropout alla fine\n",
    "        if self.net[-2].bias is not None:\n",
    "            nn.init.zeros_(self.net[-2].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x_base è la proiezione lineare\n",
    "        x_base = self.input_proj(x)\n",
    "        # Aggiungiamo il residuo non-lineare\n",
    "        return x_base + self.net(x_base)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. MLP Prober (replaces LogisticRegression)\n",
    "# ------------------------------------------------------------------\n",
    "class MLPProber(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 256, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        MLP classifier for hallucination detection.\n",
    "        Architecture: Input -> Hidden -> Hidden/2 -> Output (binary)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Returns predicted class labels (0 or 1)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return (torch.sigmoid(logits) > 0.5).long()\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"Returns probability of class 1\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return torch.sigmoid(logits)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. MixedLoss for Alignment\n",
    "# ------------------------------------------------------------------\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.01, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Peso per MSE\n",
    "        self.beta = beta    # Peso per Cosine\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # 1. MSE Loss (Magnitudine e Posizione esatta)\n",
    "        loss_mse = self.mse(pred, target)\n",
    "        \n",
    "        # 2. Cosine Loss (Direzione/Angolo)\n",
    "        cosine_sim = F.cosine_similarity(pred, target, dim=1).mean()\n",
    "        loss_cosine = 1 - cosine_sim\n",
    "        \n",
    "        # Loss combinata\n",
    "        return self.alpha * loss_mse + self.beta * loss_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f83547db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_layers(model_name, dataset_name, layer_indices, type_layer, stats, train_indices, test_indices):\n",
    "    \"\"\"\n",
    "    Caricamento standard in RAM (senza memmap).\n",
    "    \"\"\"\n",
    "    print(f\" Caricamento IN-MEMORY {model_name} [{type_layer}]: layers {layer_indices}...\")\n",
    "\n",
    "    total_samples = stats['total']\n",
    "    hallucinated_set = set(stats['hallucinated_items'])\n",
    "\n",
    "    # Label\n",
    "    y_full = np.zeros(total_samples, dtype=np.int8)\n",
    "    y_full[list(hallucinated_set)] = 1\n",
    "    y_train = y_full[train_indices]\n",
    "    y_test  = y_full[test_indices]\n",
    "\n",
    "    # Load and concatenate\n",
    "    all_features = []\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\n",
    "                                 \"activation_\"+type_layer, f\"layer{layer_idx}_activations.pt\")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\" Warning: Layer {layer_idx} non trovato. Salto.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Loading layer {layer_idx}...\", end=\" \")\n",
    "        acts = torch.load(file_path, map_location='cpu')\n",
    "        \n",
    "        if acts.shape[0] > total_samples:\n",
    "            acts = acts[:total_samples]\n",
    "\n",
    "        # Convert to numpy\n",
    "        if isinstance(acts, torch.Tensor):\n",
    "            X_layer = acts.float().numpy() \n",
    "        else:\n",
    "            X_layer = acts.astype(np.float32)\n",
    "\n",
    "        # Flatten\n",
    "        if X_layer.ndim > 2:\n",
    "            X_layer = X_layer.reshape(X_layer.shape[0], -1)\n",
    "            \n",
    "        all_features.append(X_layer)\n",
    "        print(f\"done ({X_layer.shape})\")\n",
    "        \n",
    "        del acts\n",
    "        gc.collect()\n",
    "\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"Nessun layer valido trovato per {model_name}\")\n",
    "\n",
    "    print(\" Concatenating layers...\")\n",
    "    X_full = np.concatenate(all_features, axis=1)\n",
    "    \n",
    "    X_train = X_full[train_indices]\n",
    "    X_test  = X_full[test_indices]\n",
    "    \n",
    "    print(f\" Completato! Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Train MLP Prober\n",
    "# ==================================================================\n",
    "def train_mlp_prober(X_train, y_train, X_val, y_val, input_dim, device, \n",
    "                     hidden_dim=64, dropout=0.5, epochs=200, patience=30, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Train MLP prober with early stopping based on validation F1 score.\n",
    "    \"\"\"\n",
    "    prober = MLPProber(input_dim=input_dim, hidden_dim=hidden_dim, dropout=dropout).to(device)\n",
    "    \n",
    "    # Compute class weights for imbalanced data\n",
    "    n_pos = y_train.sum()\n",
    "    n_neg = len(y_train) - n_pos\n",
    "    pos_weight = torch.tensor([n_neg / n_pos]).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.AdamW(prober.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = ClassificationDataset(X_train, y_train)\n",
    "    val_dataset = ClassificationDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        prober.train()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #proviamo ad aggiungere rumore per non far overfit. Lo aggiungiamo solo se il teacher ha dimensione < student\n",
    "            noise = torch.randn_like(X_batch) * 0.05 if X_batch.shape[1] <  X_val.shape[1] else 0.0\n",
    "\n",
    "\n",
    "            logits = prober(X_batch+noise)\n",
    "            loss = criterion(logits, y_batch.float())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(prober.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        prober.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = prober.predict(X_batch)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        val_f1 = f1_score(all_labels, all_preds)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"   Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early Stopping based on acc\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_f1\n",
    "            patience_counter = 0\n",
    "            best_model_state = prober.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"   Early stopping at epoch {epoch+1}. Best Val ACC: {best_val_acc:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        prober.load_state_dict(best_model_state)\n",
    "    #save best model on disk\n",
    "    \n",
    "    \n",
    "    \n",
    "    return prober, best_val_acc\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Main Experiment Pipeline\n",
    "# ==================================================================\n",
    "def run_experiment_pipeline_cached(X_teacher, y_teacher, teacher_name,\n",
    "                                   X_student, y_student, student_name, layer_type, config_name,\n",
    "                                   patience=50, min_delta=1e-4):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPERIMENT: {layer_type.upper()} → {teacher_name} ← {student_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Dati già splittati (numpy per sklearn)\n",
    "    X_A_train_full, X_A_test = X_teacher['X_train'], X_teacher['X_test']\n",
    "    y_A_train_full, y_A_test = y_teacher['y_train'], y_teacher['y_test']\n",
    "    X_B_train_full, X_B_test = X_student['X_train'], X_student['X_test']\n",
    "    y_B_train_full, y_B_test = y_student['y_train'], y_student['y_test']\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Teacher MLP Probing (con validation split interno)\n",
    "    # --------------------------------------------------\n",
    "    print(\"1. Training teacher MLP prober...\")\n",
    "    \n",
    "    # Split interno per validation del prober\n",
    "    num_train = len(X_A_train_full)\n",
    "    indices = np.arange(num_train)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    prober_val_size = int(num_train * 0.15)\n",
    "    prober_train_idx = indices[prober_val_size:]\n",
    "    prober_val_idx = indices[:prober_val_size]\n",
    "    \n",
    "    X_A_prober_train = torch.from_numpy(X_A_train_full[prober_train_idx]).float().to(device)\n",
    "    y_A_prober_train = torch.from_numpy(y_A_train_full[prober_train_idx]).long().to(device)\n",
    "    X_A_prober_val = torch.from_numpy(X_A_train_full[prober_val_idx]).float().to(device)\n",
    "    y_A_prober_val = torch.from_numpy(y_A_train_full[prober_val_idx]).long().to(device)\n",
    "    \n",
    "    probe_teacher, best_prober_acc = train_mlp_prober(\n",
    "        X_A_prober_train, y_A_prober_train,\n",
    "        X_A_prober_val, y_A_prober_val,\n",
    "        input_dim=X_A_train_full.shape[1],\n",
    "        device=device,\n",
    "        epochs=200,\n",
    "        patience=30\n",
    "    )\n",
    "    print(f\"   Best prober validation F1: {best_prober_acc:.4f}\")\n",
    "\n",
    "    #save best model on disk\n",
    "    model_save_dir = os.path.join(\"prober_models\", layer_type)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    model_filename = os.path.join(model_save_dir, f\"{config_name}_prober_{student_name}_to_{teacher_name}.pt\")\n",
    "    \n",
    "    # --- METRICHE TEACHER ---\n",
    "    probe_teacher.eval()\n",
    "    X_A_test_t = torch.from_numpy(X_A_test).float().to(device)\n",
    "    y_pred_teacher = probe_teacher.predict(X_A_test_t).cpu().numpy()\n",
    "    \n",
    "    cm_teacher = confusion_matrix(y_A_test, y_pred_teacher)\n",
    "    acc_teacher = accuracy_score(y_A_test, y_pred_teacher)\n",
    "    prec_teacher = precision_score(y_A_test, y_pred_teacher)\n",
    "    rec_teacher = recall_score(y_A_test, y_pred_teacher)\n",
    "    f1_teacher = f1_score(y_A_test, y_pred_teacher)\n",
    "    print(f\"   Teacher Test Acc: {acc_teacher:.4f}, F1: {f1_teacher:.4f}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Alignment Training (Student → Teacher space)\n",
    "    # --------------------------------------------------\n",
    "    print(\"2. Training alignment network (with 90/10 validation split)...\")\n",
    "    \n",
    "    # Preconversion a torch.Tensor UNA VOLTA SOLA\n",
    "    X_A_train_full_t = torch.from_numpy(X_A_train_full).float()\n",
    "    X_A_test_t = torch.from_numpy(X_A_test).float()\n",
    "    X_B_train_full_t = torch.from_numpy(X_B_train_full).float()\n",
    "    X_B_test_t = torch.from_numpy(X_B_test).float()\n",
    "\n",
    "    # Create Validation Split (10%) per l'alignment network SOLTANTO\n",
    "    num_train = len(X_B_train_full)\n",
    "    indices = np.arange(num_train)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    val_size = int(num_train * 0.1)\n",
    "    train_indices = indices[val_size:]\n",
    "    val_indices = indices[:val_size]\n",
    "\n",
    "    # Slice diretta sui tensori\n",
    "    X_B_align_train = X_B_train_full_t[train_indices]\n",
    "    X_A_align_train = X_A_train_full_t[train_indices]\n",
    "    \n",
    "    X_B_val = X_B_train_full_t[val_indices]\n",
    "    X_A_val = X_A_train_full_t[val_indices]\n",
    "\n",
    "    train_dataset = AlignmentDataset(X_B_align_train.to(device), X_A_align_train.to(device))\n",
    "    val_dataset = AlignmentDataset(X_B_val.to(device), X_A_val.to(device))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False)\n",
    "    \n",
    "    criterion = MixedLoss().to(device)\n",
    "\n",
    "    aligner = AlignmentNetwork(\n",
    "        input_dim=X_B_align_train.shape[1],\n",
    "        output_dim=X_A_align_train.shape[1],\n",
    "    ).to(device)\n",
    "    epochs = 1000\n",
    "    optimizer = optim.AdamW(aligner.parameters(), lr=1e-3, weight_decay=0.1)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        aligner.train()\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            projected = aligner(data)\n",
    "            loss = criterion(projected, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(aligner.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        aligner.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                projected = aligner(data)\n",
    "                loss = criterion(projected, target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"   Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "            \n",
    "        # Early Stopping Check\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = aligner.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"   Early stopping triggered at epoch {epoch+1}. Best Val Loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "            \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        aligner.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Save the best alignment network to disk\n",
    "    model_save_dir = os.path.join(\"alignment_models\", layer_type)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    model_filename = os.path.join(model_save_dir, f\"{config_name}_aligner_{student_name}_to_{teacher_name}.pt\")\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': aligner.state_dict(),\n",
    "        'input_dim': X_B_align_train.shape[1],\n",
    "        'output_dim': X_A_align_train.shape[1],\n",
    "        'dropout': 0.1,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'layer_type': layer_type,\n",
    "        'student_model': student_name,\n",
    "        'teacher_model': teacher_name,\n",
    "    }, model_filename)\n",
    "    print(f\"   ✓ Alignment network saved: {model_filename}\")\n",
    "    \n",
    "    # Save the MLP prober too\n",
    "    prober_filename = os.path.join(model_save_dir, f\"{config_name}_mlp_prober_{teacher_name}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': probe_teacher.state_dict(),\n",
    "        'input_dim': X_A_train_full.shape[1],\n",
    "        'hidden_dim': 256,\n",
    "        'dropout': 0.3,\n",
    "        'layer_type': layer_type,\n",
    "        'teacher_model': teacher_name,\n",
    "    }, prober_filename)\n",
    "    print(f\"   ✓ MLP prober saved: {prober_filename}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Evaluation: Student projected → Teacher MLP probe\n",
    "    # --------------------------------------------------\n",
    "    print(\"3. Projecting student test set & evaluating...\")\n",
    "    aligner.eval()\n",
    "    with torch.no_grad():\n",
    "        X_B_projected = aligner(X_B_test_t.to(device))\n",
    "    \n",
    "    y_pred_cross = probe_teacher.predict(X_B_projected).cpu().numpy()\n",
    "    \n",
    "    # --- METRICHE CROSS-MODEL ---\n",
    "    cm_cross = confusion_matrix(y_B_test, y_pred_cross)\n",
    "    acc_cross = accuracy_score(y_B_test, y_pred_cross)\n",
    "    prec_cross = precision_score(y_B_test, y_pred_cross)\n",
    "    rec_cross = recall_score(y_B_test, y_pred_cross)\n",
    "    f1_cross = f1_score(y_B_test, y_pred_cross)\n",
    "    \n",
    "    print(f\"\\nFINAL RESULT:\")\n",
    "    print(f\"   Teacher Acc         : {acc_teacher:.4f}, F1: {f1_teacher:.4f}\")\n",
    "    print(f\"   Student → Teacher Acc: {acc_cross:.4f}, F1: {f1_cross:.4f}\")\n",
    "    print(f\"   Transfer gap (Acc)  : {acc_teacher - acc_cross:.4f}\")\n",
    "    print(f\"   Transfer gap (F1)   : {f1_teacher - f1_cross:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"type\": layer_type,\n",
    "        \"teacher_name\": teacher_name,\n",
    "        \"student_name\": student_name,\n",
    "        \"teacher\": {\n",
    "            \"accuracy\": acc_teacher,\n",
    "            \"precision\": prec_teacher,\n",
    "            \"recall\": rec_teacher,\n",
    "            \"f1\": f1_teacher,\n",
    "            \"confusion_matrix\": cm_teacher.tolist()\n",
    "        },\n",
    "        \"student_on_teacher\": {\n",
    "            \"accuracy\": acc_cross,\n",
    "            \"precision\": prec_cross,\n",
    "            \"recall\": rec_cross,\n",
    "            \"f1\": f1_cross,\n",
    "            \"confusion_matrix\": cm_cross.tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, layer_type, model_name=\"\", save_dir=\"confusion_matrices\"):\n",
    "    \"\"\"\n",
    "    Plotta e salva la confusion matrix come immagine.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,\n",
    "                xticklabels=['Non-Hallucinated', 'Hallucinated'],\n",
    "                yticklabels=['Non-Hallucinated', 'Hallucinated'])\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    title = f'Confusion Matrix - {layer_type.upper()} Layers'\n",
    "    if model_name:\n",
    "        title += f' ({model_name})'\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = os.path.join(save_dir, f'confusion_matrix_{layer_type}_{model_name}.png' if model_name else f'confusion_matrix_{layer_type}.png')\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   ✓ Salvato: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b8b6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FASE 1: PRE-CARICAMENTO E SPLITTING DEI DATI (stessi indici shuffled per TUTTI i layer type)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: ATTN\n",
      "========================================\n",
      " Caricamento IN-MEMORY Qwen2.5-7B [attn]: layers [15, 16, 18]...\n",
      "  Loading layer 15... done ((27416, 3584))\n",
      "  Loading layer 16... done ((27416, 3584))\n",
      "  Loading layer 18... done ((27416, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (19191, 10752), Test: (8225, 10752)\n",
      " Caricamento IN-MEMORY Falcon3-7B-Base [attn]: layers [2, 7, 12]...\n",
      "  Loading layer 2... done ((27416, 3072))\n",
      "  Loading layer 7... done ((27416, 3072))\n",
      "  Loading layer 12... done ((27416, 3072))\n",
      " Concatenating layers...\n",
      " Completato! Train: (19191, 9216), Test: (8225, 9216)\n",
      "   Normalizzazione dati...\n",
      "\n",
      "   --- Scenario: Qwen2.5-7B -> Falcon3-7B-Base ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: ATTN → Qwen2.5-7B ← Falcon3-7B-Base\n",
      "============================================================\n",
      "Using device: cuda\n",
      "1. Training teacher MLP prober...\n",
      "   Epoch  20/200 | Train Loss: 0.0337 | Val F1: 0.9891 | Val Acc: 0.9875\n",
      "   Epoch  40/200 | Train Loss: 0.0183 | Val F1: 0.9936 | Val Acc: 0.9927\n",
      "   Epoch  60/200 | Train Loss: 0.0135 | Val F1: 0.9957 | Val Acc: 0.9951\n",
      "   Epoch  80/200 | Train Loss: 0.0079 | Val F1: 0.9960 | Val Acc: 0.9955\n",
      "   Early stopping at epoch 89. Best Val ACC: 0.9970\n",
      "   Best prober validation F1: 0.9970\n",
      "   Teacher Test Acc: 0.9902, F1: 0.9916\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch  50/1000 | Train Loss: 0.505178 | Val Loss: 0.671774\n",
      "   Early stopping triggered at epoch 60. Best Val Loss: 0.653763\n",
      "   ✓ Alignment network saved: alignment_models\\attn\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt\n",
      "   ✓ MLP prober saved: alignment_models\\attn\\CONFIG1_mlp_prober_Qwen2.5-7B.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9902, F1: 0.9916\n",
      "   Student → Teacher Acc: 0.7549, F1: 0.7999\n",
      "   Transfer gap (Acc)  : 0.2353\n",
      "   Transfer gap (F1)   : 0.1916\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_attn_Teacher_Qwen2.png\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_attn_Falcon3-7B-Base_on_Qwen2.png\n",
      "\n",
      "   --- Scenario: Falcon3-7B-Base -> Qwen2.5-7B ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: ATTN → Falcon3-7B-Base ← Qwen2.5-7B\n",
      "============================================================\n",
      "Using device: cuda\n",
      "1. Training teacher MLP prober...\n",
      "   Epoch  20/200 | Train Loss: 0.0901 | Val F1: 0.9341 | Val Acc: 0.9201\n",
      "   Epoch  40/200 | Train Loss: 0.0518 | Val F1: 0.9385 | Val Acc: 0.9256\n",
      "   Early stopping at epoch 58. Best Val ACC: 0.9367\n",
      "   Best prober validation F1: 0.9367\n",
      "   Teacher Test Acc: 0.9191, F1: 0.9329\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch  50/1000 | Train Loss: 0.543031 | Val Loss: 0.644939\n",
      "   Early stopping triggered at epoch 82. Best Val Loss: 0.628332\n",
      "   ✓ Alignment network saved: alignment_models\\attn\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt\n",
      "   ✓ MLP prober saved: alignment_models\\attn\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9191, F1: 0.9329\n",
      "   Student → Teacher Acc: 0.7629, F1: 0.7929\n",
      "   Transfer gap (Acc)  : 0.1562\n",
      "   Transfer gap (F1)   : 0.1400\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_attn_Teacher_Falcon3-7B-Base.png\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_attn_Qwen2_on_Falcon3-7B-Base.png\n",
      "   Memoria liberata per attn.\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: MLP\n",
      "========================================\n",
      " Caricamento IN-MEMORY Qwen2.5-7B [mlp]: layers [16, 18, 20]...\n",
      "  Loading layer 16... done ((27416, 3584))\n",
      "  Loading layer 18... done ((27416, 3584))\n",
      "  Loading layer 20... done ((27416, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (19191, 10752), Test: (8225, 10752)\n",
      " Caricamento IN-MEMORY Falcon3-7B-Base [mlp]: layers [10, 11, 12]...\n",
      "  Loading layer 10... done ((27416, 3072))\n",
      "  Loading layer 11... done ((27416, 3072))\n",
      "  Loading layer 12... done ((27416, 3072))\n",
      " Concatenating layers...\n",
      " Completato! Train: (19191, 9216), Test: (8225, 9216)\n",
      "   Normalizzazione dati...\n",
      "\n",
      "   --- Scenario: Qwen2.5-7B -> Falcon3-7B-Base ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: MLP → Qwen2.5-7B ← Falcon3-7B-Base\n",
      "============================================================\n",
      "Using device: cuda\n",
      "1. Training teacher MLP prober...\n",
      "   Epoch  20/200 | Train Loss: 0.0395 | Val F1: 0.9879 | Val Acc: 0.9861\n",
      "   Epoch  40/200 | Train Loss: 0.0251 | Val F1: 0.9881 | Val Acc: 0.9864\n",
      "   Epoch  60/200 | Train Loss: 0.0153 | Val F1: 0.9927 | Val Acc: 0.9917\n",
      "   Epoch  80/200 | Train Loss: 0.0111 | Val F1: 0.9936 | Val Acc: 0.9927\n",
      "   Epoch 100/200 | Train Loss: 0.0097 | Val F1: 0.9924 | Val Acc: 0.9913\n",
      "   Early stopping at epoch 107. Best Val ACC: 0.9954\n",
      "   Best prober validation F1: 0.9954\n",
      "   Teacher Test Acc: 0.9889, F1: 0.9905\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch  50/1000 | Train Loss: 0.460344 | Val Loss: 0.719703\n",
      "   Early stopping triggered at epoch 72. Best Val Loss: 0.701431\n",
      "   ✓ Alignment network saved: alignment_models\\mlp\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt\n",
      "   ✓ MLP prober saved: alignment_models\\mlp\\CONFIG1_mlp_prober_Qwen2.5-7B.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9889, F1: 0.9905\n",
      "   Student → Teacher Acc: 0.7167, F1: 0.7607\n",
      "   Transfer gap (Acc)  : 0.2722\n",
      "   Transfer gap (F1)   : 0.2298\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_mlp_Teacher_Qwen2.png\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_mlp_Falcon3-7B-Base_on_Qwen2.png\n",
      "\n",
      "   --- Scenario: Falcon3-7B-Base -> Qwen2.5-7B ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: MLP → Falcon3-7B-Base ← Qwen2.5-7B\n",
      "============================================================\n",
      "Using device: cuda\n",
      "1. Training teacher MLP prober...\n",
      "   Epoch  20/200 | Train Loss: 0.0705 | Val F1: 0.9282 | Val Acc: 0.9138\n",
      "   Early stopping at epoch 36. Best Val ACC: 0.9233\n",
      "   Best prober validation F1: 0.9233\n",
      "   Teacher Test Acc: 0.9080, F1: 0.9249\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch  50/1000 | Train Loss: 0.552983 | Val Loss: 0.681241\n",
      "   Epoch 100/1000 | Train Loss: 0.548393 | Val Loss: 0.680997\n",
      "   Epoch 150/1000 | Train Loss: 0.544333 | Val Loss: 0.684407\n",
      "   Early stopping triggered at epoch 160. Best Val Loss: 0.659861\n",
      "   ✓ Alignment network saved: alignment_models\\mlp\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt\n",
      "   ✓ MLP prober saved: alignment_models\\mlp\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9080, F1: 0.9249\n",
      "   Student → Teacher Acc: 0.7932, F1: 0.8161\n",
      "   Transfer gap (Acc)  : 0.1148\n",
      "   Transfer gap (F1)   : 0.1088\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_mlp_Teacher_Falcon3-7B-Base.png\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_mlp_Qwen2_on_Falcon3-7B-Base.png\n",
      "   Memoria liberata per mlp.\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: HIDDEN\n",
      "========================================\n",
      " Caricamento IN-MEMORY Qwen2.5-7B [hidden]: layers [18, 19, 20]...\n",
      "  Loading layer 18... done ((27416, 3584))\n",
      "  Loading layer 19... done ((27416, 3584))\n",
      "  Loading layer 20... done ((27416, 3584))\n",
      " Concatenating layers...\n",
      " Completato! Train: (19191, 10752), Test: (8225, 10752)\n",
      " Caricamento IN-MEMORY Falcon3-7B-Base [hidden]: layers [2, 3, 19]...\n",
      "  Loading layer 2... done ((27416, 3072))\n",
      "  Loading layer 3... done ((27416, 3072))\n",
      "  Loading layer 19... done ((27416, 3072))\n",
      " Concatenating layers...\n",
      " Completato! Train: (19191, 9216), Test: (8225, 9216)\n",
      "   Normalizzazione dati...\n",
      "\n",
      "   --- Scenario: Qwen2.5-7B -> Falcon3-7B-Base ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: HIDDEN → Qwen2.5-7B ← Falcon3-7B-Base\n",
      "============================================================\n",
      "Using device: cuda\n",
      "1. Training teacher MLP prober...\n",
      "   Epoch  20/200 | Train Loss: 0.0460 | Val F1: 0.9882 | Val Acc: 0.9864\n",
      "   Epoch  40/200 | Train Loss: 0.0244 | Val F1: 0.9903 | Val Acc: 0.9889\n",
      "   Epoch  60/200 | Train Loss: 0.0189 | Val F1: 0.9912 | Val Acc: 0.9899\n",
      "   Epoch  80/200 | Train Loss: 0.0146 | Val F1: 0.9918 | Val Acc: 0.9906\n",
      "   Epoch 100/200 | Train Loss: 0.0118 | Val F1: 0.9939 | Val Acc: 0.9931\n",
      "   Early stopping at epoch 105. Best Val ACC: 0.9951\n",
      "   Best prober validation F1: 0.9951\n",
      "   Teacher Test Acc: 0.9906, F1: 0.9920\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch  50/1000 | Train Loss: 0.479150 | Val Loss: 0.688826\n",
      "   Early stopping triggered at epoch 60. Best Val Loss: 0.674172\n",
      "   ✓ Alignment network saved: alignment_models\\hidden\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt\n",
      "   ✓ MLP prober saved: alignment_models\\hidden\\CONFIG1_mlp_prober_Qwen2.5-7B.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9906, F1: 0.9920\n",
      "   Student → Teacher Acc: 0.7385, F1: 0.7883\n",
      "   Transfer gap (Acc)  : 0.2522\n",
      "   Transfer gap (F1)   : 0.2036\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_hidden_Teacher_Qwen2.png\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_hidden_Falcon3-7B-Base_on_Qwen2.png\n",
      "\n",
      "   --- Scenario: Falcon3-7B-Base -> Qwen2.5-7B ---\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: HIDDEN → Falcon3-7B-Base ← Qwen2.5-7B\n",
      "============================================================\n",
      "Using device: cuda\n",
      "1. Training teacher MLP prober...\n",
      "   Epoch  20/200 | Train Loss: 0.0884 | Val F1: 0.9196 | Val Acc: 0.9048\n",
      "   Early stopping at epoch 36. Best Val ACC: 0.9189\n",
      "   Best prober validation F1: 0.9189\n",
      "   Teacher Test Acc: 0.9055, F1: 0.9219\n",
      "2. Training alignment network (with 90/10 validation split)...\n",
      "   Epoch  50/1000 | Train Loss: 0.574038 | Val Loss: 0.684646\n",
      "   Epoch 100/1000 | Train Loss: 0.569420 | Val Loss: 0.691386\n",
      "   Epoch 150/1000 | Train Loss: 0.563888 | Val Loss: 0.663472\n",
      "   Epoch 200/1000 | Train Loss: 0.558036 | Val Loss: 0.675817\n",
      "   Epoch 250/1000 | Train Loss: 0.549794 | Val Loss: 0.660079\n",
      "   Epoch 300/1000 | Train Loss: 0.542348 | Val Loss: 0.667960\n",
      "   Early stopping triggered at epoch 304. Best Val Loss: 0.643107\n",
      "   ✓ Alignment network saved: alignment_models\\hidden\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt\n",
      "   ✓ MLP prober saved: alignment_models\\hidden\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt\n",
      "3. Projecting student test set & evaluating...\n",
      "\n",
      "FINAL RESULT:\n",
      "   Teacher Acc         : 0.9055, F1: 0.9219\n",
      "   Student → Teacher Acc: 0.8107, F1: 0.8387\n",
      "   Transfer gap (Acc)  : 0.0948\n",
      "   Transfer gap (F1)   : 0.0832\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_hidden_Teacher_Falcon3-7B-Base.png\n",
      "   ✓ Salvato: confusion_matrices\\confusion_matrix_hidden_Qwen2_on_Falcon3-7B-Base.png\n",
      "   Memoria liberata per hidden.\n",
      "\n",
      "✓ Risultati salvati in: results_metrics/experiment_results_all_scenarios_mlp_prober.json\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FASE 1: PRE-CARICAMENTO E SPLITTING DEI DATI (stessi indici shuffled per TUTTI i layer type)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "n_samples = qwen_stats['total'] \n",
    "rng = np.random.RandomState(42)\n",
    "shuffled_indices = rng.permutation(n_samples)\n",
    "split_idx = int(0.7 * n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:split_idx]\n",
    "test_indices = shuffled_indices[split_idx:]\n",
    "\n",
    "# Definisci gli scenari di esperimento\n",
    "scenarios = [\n",
    "    {\"teacher_model\": \"Qwen2.5-7B\", \"student_model\": \"Falcon3-7B-Base\"},\n",
    "    {\"teacher_model\": \"Falcon3-7B-Base\", \"student_model\": \"Qwen2.5-7B\"}\n",
    "]\n",
    "\n",
    "# Struttura per raccogliere i risultati mantenendo l'ordine degli scenari\n",
    "scenario_results_map = {0: [], 1: []}\n",
    "\n",
    "# Loop sui layer types (Carica -> Esegui -> Libera Memoria)\n",
    "for layer_type in ['attn', 'mlp', 'hidden']:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"PROCESSING LAYER TYPE: {layer_type.upper()}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    gc.collect()\n",
    "    \n",
    "    try:\n",
    "        # 1. CARICAMENTO E SPLITTING STANDARD\n",
    "        X_qwen_train, X_qwen_test, y_qwen_train, y_qwen_test = load_and_split_layers(\n",
    "            \"Qwen2.5-7B\", \"belief_bank\", \n",
    "            LAYER_CONFIG[\"Qwen2.5-7B\"][layer_type], \n",
    "            layer_type, qwen_stats,\n",
    "            train_indices, test_indices\n",
    "        )\n",
    "\n",
    "\n",
    "        X_falcon_train, X_falcon_test, y_falcon_train, y_falcon_test = load_and_split_layers(\n",
    "            \"Falcon3-7B-Base\", \"belief_bank\", \n",
    "            LAYER_CONFIG[\"Falcon3-7B-Base\"][layer_type], \n",
    "            layer_type, falcon_stats,\n",
    "            train_indices, test_indices\n",
    "        )\n",
    "        \n",
    "        # 2. SCALING (con cast esplicito a float32 per risparmiare memoria)\n",
    "        print(\"   Normalizzazione dati...\")\n",
    "        scaler_qwen = StandardScaler()\n",
    "        X_qwen_train = scaler_qwen.fit_transform(X_qwen_train)\n",
    "        X_qwen_test = scaler_qwen.transform(X_qwen_test)\n",
    "        \n",
    "        scaler_falcon = StandardScaler()\n",
    "        X_falcon_train = scaler_falcon.fit_transform(X_falcon_train)\n",
    "        X_falcon_test = scaler_falcon.transform(X_falcon_test)\n",
    "        \n",
    "        # Organizza i dati per l'uso\n",
    "        current_data = {\n",
    "            \"qwen\": {\"X_train\": X_qwen_train, \"X_test\": X_qwen_test, \"y_train\": y_qwen_train, \"y_test\": y_qwen_test},\n",
    "            \"falcon\": {\"X_train\": X_falcon_train, \"X_test\": X_falcon_test, \"y_train\": y_falcon_train, \"y_test\": y_falcon_test}\n",
    "        }\n",
    "\n",
    "        # 3. ESECUZIONE ESPERIMENTI PER ENTRAMBI GLI SCENARI\n",
    "        for i, scenario in enumerate(scenarios):\n",
    "            print(f\"\\n   --- Scenario: {scenario['teacher_model']} -> {scenario['student_model']} ---\")\n",
    "            \n",
    "            if scenario['teacher_model'] == \"Qwen2.5-7B\":\n",
    "                X_teacher_data = current_data['qwen']\n",
    "                X_student_data = current_data['falcon']\n",
    "            else:\n",
    "                X_teacher_data = current_data['falcon']\n",
    "                X_student_data = current_data['qwen']\n",
    "            \n",
    "            res = run_experiment_pipeline_cached(\n",
    "                X_teacher_data, X_teacher_data, scenario['teacher_model'],\n",
    "                X_student_data, X_student_data, scenario['student_model'],\n",
    "                layer_type, \"CONFIG1\"\n",
    "            )\n",
    "            scenario_results_map[i].append(res)\n",
    "            \n",
    "            # Plot confusion matrices\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"Teacher_{scenario['teacher_model'].split('.')[0]}\"\n",
    "            )\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['student_on_teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"{scenario['student_model'].split('.')[0]}_on_{scenario['teacher_model'].split('.')[0]}\"\n",
    "            )\n",
    "\n",
    "        # 4. PULIZIA MEMORIA\n",
    "        del current_data, X_qwen_train, X_qwen_test, X_falcon_train, X_falcon_test\n",
    "        del scaler_qwen, scaler_falcon\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"   Memoria liberata per {layer_type}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore critico nel layer {layer_type}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "\n",
    "# Ricostruisci la struttura all_results per il salvataggio JSON\n",
    "all_results = []\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    all_results.append({\n",
    "        \"scenario\": f\"{scenario['teacher_model']} (teacher) → {scenario['student_model']} (student)\",\n",
    "        \"results\": scenario_results_map[i]\n",
    "    })\n",
    "\n",
    "# Salva tutti i risultati in JSON\n",
    "os.makedirs(\"results_metrics\", exist_ok=True)\n",
    "metrics_file = \"results_metrics/experiment_results_all_scenarios_mlp_prober.json\"\n",
    "\n",
    "all_results_json = []\n",
    "for scenario_data in all_results:\n",
    "    scenario_results = []\n",
    "    for r in scenario_data['results']:\n",
    "        scenario_results.append({\n",
    "            \"layer_type\": r['type'],\n",
    "            \"teacher_model\": r['teacher_name'],\n",
    "            \"student_model\": r['student_name'],\n",
    "            \"teacher\": {\n",
    "                \"accuracy\": round(r['teacher']['accuracy'], 4),\n",
    "                \"precision\": round(r['teacher']['precision'], 4),\n",
    "                \"recall\": round(r['teacher']['recall'], 4),\n",
    "                \"f1_score\": round(r['teacher']['f1'], 4),\n",
    "                \"confusion_matrix\": {\n",
    "                    \"TN\": int(r['teacher']['confusion_matrix'][0][0]),\n",
    "                    \"FP\": int(r['teacher']['confusion_matrix'][0][1]),\n",
    "                    \"FN\": int(r['teacher']['confusion_matrix'][1][0]),\n",
    "                    \"TP\": int(r['teacher']['confusion_matrix'][1][1])\n",
    "                }\n",
    "            },\n",
    "            \"student_on_teacher\": {\n",
    "                \"accuracy\": round(r['student_on_teacher']['accuracy'], 4),\n",
    "                \"precision\": round(r['student_on_teacher']['precision'], 4),\n",
    "                \"recall\": round(r['student_on_teacher']['recall'], 4),\n",
    "                \"f1_score\": round(r['student_on_teacher']['f1'], 4),\n",
    "                \"confusion_matrix\": {\n",
    "                    \"TN\": int(r['student_on_teacher']['confusion_matrix'][0][0]),\n",
    "                    \"FP\": int(r['student_on_teacher']['confusion_matrix'][0][1]),\n",
    "                    \"FN\": int(r['student_on_teacher']['confusion_matrix'][1][0]),\n",
    "                    \"TP\": int(r['student_on_teacher']['confusion_matrix'][1][1])\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    all_results_json.append({\n",
    "        \"scenario\": scenario_data['scenario'],\n",
    "        \"results\": scenario_results\n",
    "    })\n",
    "\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(all_results_json, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Risultati salvati in: {metrics_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
