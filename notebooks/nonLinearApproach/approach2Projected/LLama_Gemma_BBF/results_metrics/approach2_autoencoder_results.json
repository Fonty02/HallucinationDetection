[
  {
    "scenario": "gemma-2-9b-it (teacher) \u2192 Llama-3.1-8B-Instruct (student)",
    "results": [
      {
        "layer_type": "attn",
        "teacher_model": "gemma-2-9b-it",
        "student_model": "Llama-3.1-8B-Instruct",
        "data_info": {
          "alignment_samples_train": 653,
          "alignment_samples_val": 281,
          "model_a_train": 1122,
          "model_a_test": 482,
          "model_b_train": 2518,
          "model_b_test": 1080,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.206803,
            "epochs_trained": 243,
            "model_saved_path": "models/attn/CONFIG1_autoencoder_gemma-2-9b-it.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 12288,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.238854,
            "epochs_trained": 279,
            "model_saved_path": "models/attn/CONFIG1_autoencoder_Llama-3.1-8B-Instruct.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9643,
            "epochs_trained": 40,
            "model_saved_path": "models/attn/CONFIG1_mlp_prober_gemma-2-9b-it.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.513611,
            "epochs_trained": 30,
            "model_saved_path": "models/attn/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9627,
            "precision": 0.9676,
            "recall": 0.9598,
            "f1_score": 0.9637,
            "auroc": 0.9927,
            "confusion_matrix": {
              "TN": 225,
              "FP": 8,
              "FN": 10,
              "TP": 239
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9,
            "precision": 0.938,
            "recall": 0.8534,
            "f1_score": 0.8937,
            "auroc": 0.9701,
            "confusion_matrix": {
              "TN": 518,
              "FP": 30,
              "FN": 78,
              "TP": 454
            }
          }
        }
      },
      {
        "layer_type": "mlp",
        "teacher_model": "gemma-2-9b-it",
        "student_model": "Llama-3.1-8B-Instruct",
        "data_info": {
          "alignment_samples_train": 653,
          "alignment_samples_val": 281,
          "model_a_train": 1122,
          "model_a_test": 482,
          "model_b_train": 2518,
          "model_b_test": 1080,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.150357,
            "epochs_trained": 246,
            "model_saved_path": "models/mlp/CONFIG1_autoencoder_gemma-2-9b-it.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 12288,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.223809,
            "epochs_trained": 284,
            "model_saved_path": "models/mlp/CONFIG1_autoencoder_Llama-3.1-8B-Instruct.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9702,
            "epochs_trained": 35,
            "model_saved_path": "models/mlp/CONFIG1_mlp_prober_gemma-2-9b-it.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.538795,
            "epochs_trained": 23,
            "model_saved_path": "models/mlp/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9523,
            "precision": 0.9593,
            "recall": 0.9478,
            "f1_score": 0.9535,
            "auroc": 0.9878,
            "confusion_matrix": {
              "TN": 223,
              "FP": 10,
              "FN": 13,
              "TP": 236
            }
          },
          "student_on_teacher": {
            "accuracy": 0.888,
            "precision": 0.9185,
            "recall": 0.8477,
            "f1_score": 0.8817,
            "auroc": 0.9548,
            "confusion_matrix": {
              "TN": 508,
              "FP": 40,
              "FN": 81,
              "TP": 451
            }
          }
        }
      },
      {
        "layer_type": "hidden",
        "teacher_model": "gemma-2-9b-it",
        "student_model": "Llama-3.1-8B-Instruct",
        "data_info": {
          "alignment_samples_train": 653,
          "alignment_samples_val": 281,
          "model_a_train": 1122,
          "model_a_test": 482,
          "model_b_train": 2518,
          "model_b_test": 1080,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.155504,
            "epochs_trained": 254,
            "model_saved_path": "models/hidden/CONFIG1_autoencoder_gemma-2-9b-it.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 12288,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.216414,
            "epochs_trained": 271,
            "model_saved_path": "models/hidden/CONFIG1_autoencoder_Llama-3.1-8B-Instruct.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9643,
            "epochs_trained": 27,
            "model_saved_path": "models/hidden/CONFIG1_mlp_prober_gemma-2-9b-it.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.541864,
            "epochs_trained": 21,
            "model_saved_path": "models/hidden/CONFIG1_aligner_Llama-3.1-8B-Instruct_to_gemma-2-9b-it.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9398,
            "precision": 0.94,
            "recall": 0.9438,
            "f1_score": 0.9419,
            "auroc": 0.9876,
            "confusion_matrix": {
              "TN": 218,
              "FP": 15,
              "FN": 14,
              "TP": 235
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9037,
            "precision": 0.8877,
            "recall": 0.9211,
            "f1_score": 0.9041,
            "auroc": 0.9593,
            "confusion_matrix": {
              "TN": 486,
              "FP": 62,
              "FN": 42,
              "TP": 490
            }
          }
        }
      }
    ]
  },
  {
    "scenario": "Llama-3.1-8B-Instruct (teacher) \u2192 gemma-2-9b-it (student)",
    "results": [
      {
        "layer_type": "attn",
        "teacher_model": "Llama-3.1-8B-Instruct",
        "student_model": "gemma-2-9b-it",
        "data_info": {
          "alignment_samples_train": 653,
          "alignment_samples_val": 281,
          "model_a_train": 1122,
          "model_a_test": 482,
          "model_b_train": 2518,
          "model_b_test": 1080,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 12288,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.238854,
            "epochs_trained": 279,
            "model_saved_path": "models/attn/CONFIG1_autoencoder_Llama-3.1-8B-Instruct.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.206803,
            "epochs_trained": 243,
            "model_saved_path": "models/attn/CONFIG1_autoencoder_gemma-2-9b-it.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9867,
            "epochs_trained": 44,
            "model_saved_path": "models/attn/CONFIG1_mlp_prober_Llama-3.1-8B-Instruct.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.510613,
            "epochs_trained": 72,
            "model_saved_path": "models/attn/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9861,
            "precision": 0.985,
            "recall": 0.9868,
            "f1_score": 0.9859,
            "auroc": 0.9985,
            "confusion_matrix": {
              "TN": 540,
              "FP": 8,
              "FN": 7,
              "TP": 525
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9315,
            "precision": 0.9696,
            "recall": 0.8956,
            "f1_score": 0.9311,
            "auroc": 0.9827,
            "confusion_matrix": {
              "TN": 226,
              "FP": 7,
              "FN": 26,
              "TP": 223
            }
          }
        }
      },
      {
        "layer_type": "mlp",
        "teacher_model": "Llama-3.1-8B-Instruct",
        "student_model": "gemma-2-9b-it",
        "data_info": {
          "alignment_samples_train": 653,
          "alignment_samples_val": 281,
          "model_a_train": 1122,
          "model_a_test": 482,
          "model_b_train": 2518,
          "model_b_test": 1080,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 12288,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.223809,
            "epochs_trained": 284,
            "model_saved_path": "models/mlp/CONFIG1_autoencoder_Llama-3.1-8B-Instruct.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.150357,
            "epochs_trained": 246,
            "model_saved_path": "models/mlp/CONFIG1_autoencoder_gemma-2-9b-it.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9682,
            "epochs_trained": 51,
            "model_saved_path": "models/mlp/CONFIG1_mlp_prober_Llama-3.1-8B-Instruct.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.487944,
            "epochs_trained": 79,
            "model_saved_path": "models/mlp/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9704,
            "precision": 0.9596,
            "recall": 0.9812,
            "f1_score": 0.9703,
            "auroc": 0.995,
            "confusion_matrix": {
              "TN": 526,
              "FP": 22,
              "FN": 10,
              "TP": 522
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9232,
            "precision": 0.9344,
            "recall": 0.9157,
            "f1_score": 0.9249,
            "auroc": 0.9713,
            "confusion_matrix": {
              "TN": 217,
              "FP": 16,
              "FN": 21,
              "TP": 228
            }
          }
        }
      },
      {
        "layer_type": "hidden",
        "teacher_model": "Llama-3.1-8B-Instruct",
        "student_model": "gemma-2-9b-it",
        "data_info": {
          "alignment_samples_train": 653,
          "alignment_samples_val": 281,
          "model_a_train": 1122,
          "model_a_test": 482,
          "model_b_train": 2518,
          "model_b_test": 1080,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 12288,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.216414,
            "epochs_trained": 271,
            "model_saved_path": "models/hidden/CONFIG1_autoencoder_Llama-3.1-8B-Instruct.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.155504,
            "epochs_trained": 254,
            "model_saved_path": "models/hidden/CONFIG1_autoencoder_gemma-2-9b-it.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9682,
            "epochs_trained": 32,
            "model_saved_path": "models/hidden/CONFIG1_mlp_prober_Llama-3.1-8B-Instruct.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "optimizer": "AdamW",
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "scheduler": "CosineAnnealingLR",
            "gradient_clip_max_norm": 1.0,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.504224,
            "epochs_trained": 32,
            "model_saved_path": "models/hidden/CONFIG1_aligner_gemma-2-9b-it_to_Llama-3.1-8B-Instruct.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9639,
            "precision": 0.9556,
            "recall": 0.9718,
            "f1_score": 0.9637,
            "auroc": 0.9935,
            "confusion_matrix": {
              "TN": 524,
              "FP": 24,
              "FN": 15,
              "TP": 517
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9025,
            "precision": 0.9353,
            "recall": 0.8715,
            "f1_score": 0.9023,
            "auroc": 0.9693,
            "confusion_matrix": {
              "TN": 218,
              "FP": 15,
              "FN": 32,
              "TP": 217
            }
          }
        }
      }
    ]
  }
]