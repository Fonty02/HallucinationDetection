[
  {
    "scenario": "Qwen2.5-7B (teacher) \u2192 Falcon3-7B-Base (student)",
    "results": [
      {
        "layer_type": "attn",
        "teacher_model": "Qwen2.5-7B",
        "student_model": "Falcon3-7B-Base",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.09652,
            "epochs_trained": 287,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\attn\\CONFIG1_autoencoder_Qwen2.5-7B.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 9216,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.083049,
            "epochs_trained": 259,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\attn\\CONFIG1_autoencoder_Falcon3-7B-Base.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "type": "MLPProber",
            "hidden_dim": 64,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9933,
            "epochs_trained": 28,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\attn\\CONFIG1_mlp_prober_Qwen2.5-7B.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "hidden_dim": 256,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_alpha": 0.5,
            "loss_beta": 0.5
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.275919,
            "epochs_trained": 413,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\attn\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9916,
            "precision": 0.99,
            "recall": 0.9936,
            "f1_score": 0.9918,
            "auroc": 0.9994,
            "confusion_matrix": {
              "TN": 1030,
              "FP": 11,
              "FN": 7,
              "TP": 1091
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9706,
            "precision": 0.977,
            "recall": 0.9625,
            "f1_score": 0.9697,
            "auroc": 0.9953,
            "confusion_matrix": {
              "TN": 2258,
              "FP": 50,
              "FN": 83,
              "TP": 2128
            }
          }
        }
      },
      {
        "layer_type": "mlp",
        "teacher_model": "Qwen2.5-7B",
        "student_model": "Falcon3-7B-Base",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.084977,
            "epochs_trained": 276,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\mlp\\CONFIG1_autoencoder_Qwen2.5-7B.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 9216,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.054064,
            "epochs_trained": 242,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\mlp\\CONFIG1_autoencoder_Falcon3-7B-Base.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "type": "MLPProber",
            "hidden_dim": 64,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9933,
            "epochs_trained": 22,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\mlp\\CONFIG1_mlp_prober_Qwen2.5-7B.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "hidden_dim": 256,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_alpha": 0.5,
            "loss_beta": 0.5
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.281967,
            "epochs_trained": 271,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\mlp\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9897,
            "precision": 0.9856,
            "recall": 0.9945,
            "f1_score": 0.99,
            "auroc": 0.9997,
            "confusion_matrix": {
              "TN": 1025,
              "FP": 16,
              "FN": 6,
              "TP": 1092
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9608,
            "precision": 0.9757,
            "recall": 0.9435,
            "f1_score": 0.9593,
            "auroc": 0.9915,
            "confusion_matrix": {
              "TN": 2256,
              "FP": 52,
              "FN": 125,
              "TP": 2086
            }
          }
        }
      },
      {
        "layer_type": "hidden",
        "teacher_model": "Qwen2.5-7B",
        "student_model": "Falcon3-7B-Base",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 4991,
          "model_a_test": 2139,
          "model_b_train": 10543,
          "model_b_test": 4519,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.065256,
            "epochs_trained": 259,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\hidden\\CONFIG1_autoencoder_Qwen2.5-7B.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 9216,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.059189,
            "epochs_trained": 186,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\hidden\\CONFIG1_autoencoder_Falcon3-7B-Base.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "type": "MLPProber",
            "hidden_dim": 64,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9933,
            "epochs_trained": 24,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\hidden\\CONFIG1_mlp_prober_Qwen2.5-7B.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "hidden_dim": 256,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_alpha": 0.5,
            "loss_beta": 0.5
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.284021,
            "epochs_trained": 396,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\hidden\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9935,
            "precision": 0.9963,
            "recall": 0.9909,
            "f1_score": 0.9936,
            "auroc": 0.9997,
            "confusion_matrix": {
              "TN": 1037,
              "FP": 4,
              "FN": 10,
              "TP": 1088
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9586,
            "precision": 0.9756,
            "recall": 0.9389,
            "f1_score": 0.9569,
            "auroc": 0.9892,
            "confusion_matrix": {
              "TN": 2256,
              "FP": 52,
              "FN": 135,
              "TP": 2076
            }
          }
        }
      }
    ]
  },
  {
    "scenario": "Falcon3-7B-Base (teacher) \u2192 Qwen2.5-7B (student)",
    "results": [
      {
        "layer_type": "attn",
        "teacher_model": "Falcon3-7B-Base",
        "student_model": "Qwen2.5-7B",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 10543,
          "model_a_test": 4519,
          "model_b_train": 4991,
          "model_b_test": 2139,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 9216,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.083049,
            "epochs_trained": 259,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\attn\\CONFIG1_autoencoder_Falcon3-7B-Base.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.09652,
            "epochs_trained": 287,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\attn\\CONFIG1_autoencoder_Qwen2.5-7B.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "type": "MLPProber",
            "hidden_dim": 64,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9924,
            "epochs_trained": 36,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\attn\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "hidden_dim": 256,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_alpha": 0.5,
            "loss_beta": 0.5
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.282234,
            "epochs_trained": 177,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\attn\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9841,
            "precision": 0.9881,
            "recall": 0.9792,
            "f1_score": 0.9836,
            "auroc": 0.9983,
            "confusion_matrix": {
              "TN": 2282,
              "FP": 26,
              "FN": 46,
              "TP": 2165
            }
          },
          "student_on_teacher": {
            "accuracy": 0.8537,
            "precision": 0.8867,
            "recall": 0.8197,
            "f1_score": 0.8519,
            "auroc": 0.9367,
            "confusion_matrix": {
              "TN": 926,
              "FP": 115,
              "FN": 198,
              "TP": 900
            }
          }
        }
      },
      {
        "layer_type": "mlp",
        "teacher_model": "Falcon3-7B-Base",
        "student_model": "Qwen2.5-7B",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 10543,
          "model_a_test": 4519,
          "model_b_train": 4991,
          "model_b_test": 2139,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 9216,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.054064,
            "epochs_trained": 242,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\mlp\\CONFIG1_autoencoder_Falcon3-7B-Base.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.084977,
            "epochs_trained": 276,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\mlp\\CONFIG1_autoencoder_Qwen2.5-7B.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "type": "MLPProber",
            "hidden_dim": 64,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9873,
            "epochs_trained": 123,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\mlp\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "hidden_dim": 256,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_alpha": 0.5,
            "loss_beta": 0.5
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.303948,
            "epochs_trained": 173,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\mlp\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.9807,
            "precision": 0.9836,
            "recall": 0.9769,
            "f1_score": 0.9803,
            "auroc": 0.9971,
            "confusion_matrix": {
              "TN": 2272,
              "FP": 36,
              "FN": 51,
              "TP": 2160
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9411,
            "precision": 0.9746,
            "recall": 0.9089,
            "f1_score": 0.9406,
            "auroc": 0.9832,
            "confusion_matrix": {
              "TN": 1015,
              "FP": 26,
              "FN": 100,
              "TP": 998
            }
          }
        }
      },
      {
        "layer_type": "hidden",
        "teacher_model": "Falcon3-7B-Base",
        "student_model": "Qwen2.5-7B",
        "data_info": {
          "alignment_samples_train": 4030,
          "alignment_samples_val": 1728,
          "model_a_train": 10543,
          "model_a_test": 4519,
          "model_b_train": 4991,
          "model_b_test": 2139,
          "concordant_undersampling_for_alignment": true,
          "separate_undersampling_per_model": true
        },
        "teacher_autoencoder": {
          "architecture": {
            "input_dim": 9216,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.059189,
            "epochs_trained": 186,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\hidden\\CONFIG1_autoencoder_Falcon3-7B-Base.pt"
          }
        },
        "student_autoencoder": {
          "architecture": {
            "input_dim": 10752,
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2
          },
          "training_hyperparameters": {
            "latent_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.2,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 300,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "MSELoss"
          },
          "training_results": {
            "best_val_loss": 0.065256,
            "epochs_trained": 259,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\hidden\\CONFIG1_autoencoder_Qwen2.5-7B.pt"
          }
        },
        "prober_model": {
          "architecture": {
            "type": "MLPProber",
            "input_dim": 128,
            "hidden_dim": 64,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "type": "MLPProber",
            "hidden_dim": 64,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 64,
            "max_epochs": 200,
            "early_stopping_patience": 30,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_function": "BCEWithLogitsLoss",
            "use_class_weights": true
          },
          "training_results": {
            "best_val_acc": 0.9779,
            "epochs_trained": 56,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\hidden\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt"
          }
        },
        "alignment_model": {
          "architecture": {
            "type": "AlignmentNetwork",
            "input_dim": 128,
            "output_dim": 128,
            "hidden_dim": 256,
            "dropout": 0.3
          },
          "training_hyperparameters": {
            "hidden_dim": 256,
            "dropout": 0.3,
            "learning_rate": 0.001,
            "weight_decay": 0.01,
            "batch_size": 32,
            "max_epochs": 500,
            "early_stopping_patience": 50,
            "early_stopping_min_delta": 0.0001,
            "gradient_clip_max_norm": 1.0,
            "optimizer": "AdamW",
            "scheduler": "CosineAnnealingLR",
            "loss_alpha": 0.5,
            "loss_beta": 0.5
          },
          "loss_function": {
            "type": "MixedLoss",
            "mse_weight": 0.5,
            "cosine_weight": 0.5
          },
          "training_results": {
            "best_val_loss": 0.297232,
            "epochs_trained": 208,
            "model_saved_path": "C:\\Users\\fonta\\Desktop\\HallucinationDetection\\notebooks\\nonLinearApproach\\approach2Projected\\QWEN_FALCON_BBF\\models\\hidden\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt"
          }
        },
        "metrics": {
          "teacher": {
            "accuracy": 0.977,
            "precision": 0.9839,
            "recall": 0.9688,
            "f1_score": 0.9763,
            "auroc": 0.9952,
            "confusion_matrix": {
              "TN": 2273,
              "FP": 35,
              "FN": 69,
              "TP": 2142
            }
          },
          "student_on_teacher": {
            "accuracy": 0.9004,
            "precision": 0.9403,
            "recall": 0.8607,
            "f1_score": 0.8987,
            "auroc": 0.9503,
            "confusion_matrix": {
              "TN": 981,
              "FP": 60,
              "FN": 153,
              "TP": 945
            }
          }
        }
      }
    ]
  }
]