{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033f7dd2",
   "metadata": {},
   "source": [
    "# Approach 3: Autoencoder-based Alignment with MLP Prober\n",
    "\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Autoencoder for Teacher**: Learn to compress Teacher activations to latent dimension ($X_T \\to Z_T$)\n",
    "2. **Autoencoder for Student**: Learn to compress Student activations to the same latent dimension ($X_S \\to Z_S$)\n",
    "3. **MLP Prober on Teacher**: Train an MLP classifier on the reduced Teacher space\n",
    "4. **Alignment Network**: Learn to align Student's latent space to Teacher's latent space ($Z_S \\to Z_T$)\n",
    "5. **Evaluation**: Test the aligned Student representations on the Teacher's MLP prober\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733ba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "import traceback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "# ==================================================================\n",
    "# DEVICE CONFIGURATION\n",
    "# ==================================================================\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "# ==================================================================\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# ==================================================================\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set seeds at import time\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "HF_DEFAULT_HOME = os.environ.get(\"HF_HOME\", \"~\\\\.cache\\\\huggingface\\\\hub\")\n",
    "\n",
    "# Nomi dei modelli (usati come costanti in tutto il notebook)\n",
    "MODEL_A = \"gemma-2-9b-it\"\n",
    "MODEL_B = \"Llama-3.1-8B-Instruct\"\n",
    "\n",
    "LAYER_CONFIG = {\n",
    "    MODEL_A: \n",
    "    {\n",
    "        \"attn\": [23,27,33],\n",
    "        \"mlp\":[24,25,26],\n",
    "        \"hidden\": [23,24,27]\n",
    "    },    \n",
    "    MODEL_B: \n",
    "    {\n",
    "        \"attn\": [5,8,12],\n",
    "        \"mlp\":[13,14,15],\n",
    "        \"hidden\": [13,14,15]\n",
    "    }  \n",
    "}\n",
    "DATASET_NAME = \"belief_bank_constraints\"\n",
    "\n",
    "# ==================================================================\n",
    "# AUTOENCODER CONFIGURATION\n",
    "# ==================================================================\n",
    "AUTOENCODER_CONFIG = {\n",
    "    \"latent_dim\": 128,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"max_epochs\": 300,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"early_stopping_min_delta\": 1e-4,\n",
    "    \"gradient_clip_max_norm\": 1.0,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss_function\": \"MSELoss\"\n",
    "}\n",
    "\n",
    "# ==================================================================\n",
    "# ALIGNMENT CONFIGURATION\n",
    "# ==================================================================\n",
    "ALIGNMENT_CONFIG = {\n",
    "    \"hidden_dim\": 256,\n",
    "    \"dropout\": 0.3,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_epochs\": 500,\n",
    "    \"early_stopping_patience\": 50,\n",
    "    \"early_stopping_min_delta\": 1e-4,\n",
    "    \"gradient_clip_max_norm\": 1.0,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss_alpha\": 0.5,  # MSE weight\n",
    "    \"loss_beta\": 0.5    # Cosine weight\n",
    "}\n",
    "\n",
    "# ==================================================================\n",
    "# MLP PROBER CONFIGURATION\n",
    "# ==================================================================\n",
    "PROBER_CONFIG = {\n",
    "    \"type\": \"MLPProber\",\n",
    "    \"hidden_dim\": 64,\n",
    "    \"dropout\": 0.3,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"max_epochs\": 200,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"early_stopping_min_delta\": 1e-4,\n",
    "    \"gradient_clip_max_norm\": 1.0,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss_function\": \"BCEWithLogitsLoss\",\n",
    "    \"use_class_weights\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef8f91",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6127abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_per_json(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Versione originale per la vecchia struttura con hallucination_labels.json\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"generations\", \"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    hallucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_items': hallucinated_items,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "def stats_from_new_structure(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Nuova funzione per la struttura con cartelle hallucinated/ e not_hallucinated/\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    not_hallucinated_path = os.path.join(base_path, \"not_hallucinated\")\n",
    "    \n",
    "    hall_ids_path = os.path.join(hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    not_hall_ids_path = os.path.join(not_hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    \n",
    "    with open(hall_ids_path, 'r') as f:\n",
    "        hallucinated_ids = json.load(f)\n",
    "    with open(not_hall_ids_path, 'r') as f:\n",
    "        not_hallucinated_ids = json.load(f)\n",
    "    \n",
    "    total = len(hallucinated_ids) + len(not_hallucinated_ids)\n",
    "    hallucinations = len(hallucinated_ids)\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'not_hallucinations': len(not_hallucinated_ids),\n",
    "        'hallucinated_ids': hallucinated_ids,\n",
    "        'not_hallucinated_ids': not_hallucinated_ids,\n",
    "        'hallucinated_items': hallucinated_ids,  # Alias per compatibilità\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "def detect_structure_type(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Rileva automaticamente se la struttura è vecchia o nuova.\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    if os.path.isdir(hallucinated_path):\n",
    "        return 'new'\n",
    "    return 'old'\n",
    "\n",
    "def get_stats(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Funzione wrapper che rileva automaticamente la struttura e chiama la funzione appropriata.\n",
    "    \"\"\"\n",
    "    structure = detect_structure_type(model_name, dataset_name)\n",
    "    if structure == 'new':\n",
    "        return stats_from_new_structure(model_name, dataset_name)\n",
    "    else:\n",
    "        return stats_per_json(model_name, dataset_name)\n",
    "\n",
    "\n",
    "def get_balanced_indices(y, seed=SEED):\n",
    "    \"\"\"\n",
    "    Calcola gli indici per bilanciare il dataset tramite undersampling.\n",
    "    Questa funzione è DETERMINISTICA dato lo stesso seed e le stesse label.\n",
    "    \n",
    "    Args:\n",
    "        y: numpy array delle label\n",
    "        seed: seed per la riproducibilità\n",
    "    \n",
    "    Returns:\n",
    "        balanced_indices: numpy array degli indici selezionati (ordinati)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Trova le classi e i loro conteggi\n",
    "    unique_classes, counts = np.unique(y, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "    \n",
    "    selected_indices = []\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        \n",
    "        if len(cls_indices) > min_count:\n",
    "            # Undersampling: seleziona casualmente min_count campioni\n",
    "            sampled = rng.choice(cls_indices, size=min_count, replace=False)\n",
    "            selected_indices.extend(sampled)\n",
    "        else:\n",
    "            # Classe già al minimo, prendi tutti\n",
    "            selected_indices.extend(cls_indices)\n",
    "    \n",
    "    # Ordina gli indici per mantenere consistenza\n",
    "    return np.sort(np.array(selected_indices))\n",
    "\n",
    "\n",
    "def get_concordant_indices_and_undersample(stats_model1, stats_model2, seed=SEED):\n",
    "    \"\"\"\n",
    "    Trova gli indici dove ENTRAMBI i modelli concordano sull'etichetta,\n",
    "    poi applica undersampling per bilanciare le classi.\n",
    "    \n",
    "    Returns:\n",
    "        concordant_indices: array di indici concordanti e bilanciati\n",
    "        labels: array di label corrispondenti (0=non-hallucinated, 1=hallucinated)\n",
    "    \"\"\"\n",
    "    hall_set_1 = set(stats_model1['hallucinated_ids'])\n",
    "    hall_set_2 = set(stats_model2['hallucinated_ids'])\n",
    "    \n",
    "    # Trova tutti gli instance_id per ogni modello\n",
    "    all_ids_1 = set(stats_model1['hallucinated_ids'] + stats_model1.get('not_hallucinated_ids', []))\n",
    "    all_ids_2 = set(stats_model2['hallucinated_ids'] + stats_model2.get('not_hallucinated_ids', []))\n",
    "    \n",
    "    # Trova instance_id comuni\n",
    "    common_ids = all_ids_1.intersection(all_ids_2)\n",
    "    common_ids_sorted = sorted(common_ids)\n",
    "    \n",
    "    if not common_ids:\n",
    "        raise ValueError(\"Nessun instance_id comune trovato tra i due modelli.\")\n",
    "    \n",
    "    # Ottieni etichette per gli id comuni\n",
    "    y1_common = np.array([1 if id in hall_set_1 else 0 for id in common_ids_sorted])\n",
    "    y2_common = np.array([1 if id in hall_set_2 else 0 for id in common_ids_sorted])\n",
    "    \n",
    "    # Trova campioni CONCORDANTI (stessa label in entrambi i modelli)\n",
    "    concordant_mask = (y1_common == y2_common)\n",
    "    concordant_indices = np.array(common_ids_sorted)[concordant_mask]\n",
    "    concordant_labels = y1_common[concordant_mask]\n",
    "    \n",
    "    n_hall = np.sum(concordant_labels == 1)\n",
    "    n_non_hall = np.sum(concordant_labels == 0)\n",
    "    \n",
    "    print(f\"    - Hallucinated (concordanti): {n_hall}\")\n",
    "    print(f\"    - Non-hallucinated (concordanti): {n_non_hall}\")\n",
    "    \n",
    "    # Undersampling sulla classe maggioritaria\n",
    "    min_count = min(n_hall, n_non_hall)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    hall_concordant = concordant_indices[concordant_labels == 1]\n",
    "    non_hall_concordant = concordant_indices[concordant_labels == 0]\n",
    "    \n",
    "    hall_sampled = rng.choice(hall_concordant, size=min_count, replace=False)\n",
    "    non_hall_sampled = rng.choice(non_hall_concordant, size=min_count, replace=False)\n",
    "    \n",
    "    balanced_indices = np.concatenate([hall_sampled, non_hall_sampled])\n",
    "    balanced_labels = np.concatenate([np.ones(min_count, dtype=np.int8), np.zeros(min_count, dtype=np.int8)])\n",
    "    \n",
    "    shuffle_idx = rng.permutation(len(balanced_indices))\n",
    "    balanced_indices = balanced_indices[shuffle_idx]\n",
    "    balanced_labels = balanced_labels[shuffle_idx]\n",
    "    \n",
    "    print(f\"  Dopo undersampling: {len(balanced_indices)} campioni bilanciati ({min_count} per classe)\")\n",
    "    \n",
    "    return balanced_indices, balanced_labels\n",
    "\n",
    "\n",
    "def get_undersampled_indices_per_model(model_stats, seed=SEED):\n",
    "    \"\"\"\n",
    "    Applica undersampling al dataset di un singolo modello.\n",
    "    Usato per addestrare i prober su dati specifici del modello.\n",
    "    \n",
    "    Args:\n",
    "        model_stats: dizionario con statistiche del modello (da get_stats)\n",
    "        seed: seed per riproducibilità\n",
    "    \n",
    "    Returns:\n",
    "        balanced_idx: array di indici bilanciati\n",
    "        balanced_labels: array di label corrispondenti\n",
    "    \"\"\"\n",
    "    total = model_stats['total']\n",
    "    hall_set = set(model_stats['hallucinated_items'])\n",
    "    \n",
    "    y = np.array([1 if i in hall_set else 0 for i in range(total)])\n",
    "    balanced_idx = get_balanced_indices(y, seed)\n",
    "    balanced_labels = y[balanced_idx]\n",
    "    \n",
    "    return balanced_idx, balanced_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abef31",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49b09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. Dataset class for Autoencoder Training\n",
    "# ------------------------------------------------------------------\n",
    "class AutoencoderDataset(Dataset):\n",
    "    def __init__(self, X: torch.Tensor):\n",
    "        self.X = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Dataset class for Alignment\n",
    "# ------------------------------------------------------------------\n",
    "class AlignmentDataset(Dataset):\n",
    "    def __init__(self, x_source: torch.Tensor, x_target: torch.Tensor):\n",
    "        self.x_source = x_source\n",
    "        self.x_target = x_target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_source.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_source[idx], self.x_target[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Dataset class for Classification\n",
    "# ------------------------------------------------------------------\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Autoencoder for Dimensionality Reduction\n",
    "# ------------------------------------------------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 256, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, z\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. AlignmentNetwork\n",
    "# ------------------------------------------------------------------\n",
    "class AlignmentNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 256, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        if input_dim != output_dim:\n",
    "            self.input_proj = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        else:\n",
    "            self.input_proj = nn.Identity()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self._init_zero()\n",
    "\n",
    "    def _init_zero(self):\n",
    "        nn.init.zeros_(self.net[-2].weight)\n",
    "        if self.net[-2].bias is not None:\n",
    "            nn.init.zeros_(self.net[-2].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_base = self.input_proj(x)\n",
    "        return x_base + self.net(x_base)\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. MLP Prober\n",
    "# ------------------------------------------------------------------\n",
    "class MLPProber(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 512, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return (torch.sigmoid(logits) > 0.5).long()\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return torch.sigmoid(logits)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7. MixedLoss for Alignment\n",
    "# ------------------------------------------------------------------\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        loss_mse = self.mse(pred, target)\n",
    "        cosine_sim = F.cosine_similarity(pred, target, dim=1).mean()\n",
    "        loss_cosine = 1 - cosine_sim\n",
    "        return self.alpha * loss_mse + self.beta * loss_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff65ef9",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f050692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activations_and_labels(model_name, dataset_name, layer, layer_type):\n",
    "    \"\"\"\n",
    "    Carica le attivazioni e le label per un dato layer e tipo.\n",
    "    Supporta sia la vecchia che la nuova struttura dati.\n",
    "    \n",
    "    Returns:\n",
    "        X: numpy array delle attivazioni (n_samples, hidden_dim) - ordinate per instance_id\n",
    "        y: numpy array delle label (n_samples,) - 1=hallucination, 0=correct\n",
    "        instance_ids: numpy array degli instance_ids (n_samples,) - ordinati\n",
    "    \"\"\"\n",
    "    structure = detect_structure_type(model_name, dataset_name)\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, f\"activation_{layer_type}\")\n",
    "    \n",
    "    if structure == 'new':\n",
    "        hall_act_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer}_activations.pt\")\n",
    "        hall_ids_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer}_instance_ids.json\")\n",
    "        not_hall_act_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer}_activations.pt\")\n",
    "        not_hall_ids_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer}_instance_ids.json\")\n",
    "        \n",
    "        hall_activations = torch.load(hall_act_path, map_location=DEVICE)\n",
    "        not_hall_activations = torch.load(not_hall_act_path, map_location=DEVICE)\n",
    "        \n",
    "        with open(hall_ids_path, 'r') as f:\n",
    "            hall_ids = json.load(f)\n",
    "        with open(not_hall_ids_path, 'r') as f:\n",
    "            not_hall_ids = json.load(f)\n",
    "        \n",
    "        if isinstance(hall_activations, torch.Tensor):\n",
    "            hall_activations = hall_activations.cpu().numpy().astype(np.float32)\n",
    "        if isinstance(not_hall_activations, torch.Tensor):\n",
    "            not_hall_activations = not_hall_activations.cpu().numpy().astype(np.float32)\n",
    "        \n",
    "        X_concat = np.vstack([hall_activations, not_hall_activations])\n",
    "        y_concat = np.concatenate([\n",
    "            np.ones(hall_activations.shape[0], dtype=int),\n",
    "            np.zeros(not_hall_activations.shape[0], dtype=int)\n",
    "        ])\n",
    "        ids_concat = np.array(hall_ids + not_hall_ids)\n",
    "        \n",
    "        sort_indices = np.argsort(ids_concat)\n",
    "        X = X_concat[sort_indices]\n",
    "        y = y_concat[sort_indices]\n",
    "        instance_ids = ids_concat[sort_indices]\n",
    "        \n",
    "        return X, y, instance_ids\n",
    "    \n",
    "    else:\n",
    "        file_path = os.path.join(base_path, f\"layer{layer}_activations.pt\")\n",
    "        activations = torch.load(file_path, map_location=DEVICE)\n",
    "        \n",
    "        if isinstance(activations, torch.Tensor):\n",
    "            X = activations.cpu().numpy().astype(np.float32)\n",
    "        else:\n",
    "            X = activations.astype(np.float32)\n",
    "        \n",
    "        labels_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \n",
    "                                   \"generations\", \"hallucination_labels.json\")\n",
    "        with open(labels_path, 'r') as f:\n",
    "            labels_data = json.load(f)\n",
    "        \n",
    "        y = np.array([item['is_hallucination'] for item in labels_data], dtype=int)\n",
    "        instance_ids = np.arange(len(y))\n",
    "        \n",
    "        return X, y, instance_ids\n",
    "\n",
    "\n",
    "def load_concatenated_layers(model_name, dataset_name, layer_indices, type_layer):\n",
    "    \"\"\"Carica multipli layer e li concatena.\"\"\"\n",
    "    print(f\"   Caricamento {model_name} [{type_layer}]: layers {layer_indices}...\")\n",
    "    combined_features = []\n",
    "    y = None\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        try:\n",
    "            X_layer, y_layer, _ = load_activations_and_labels(model_name, dataset_name, layer_idx, type_layer)\n",
    "            combined_features.append(X_layer)\n",
    "            if y is None:\n",
    "                y = y_layer\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Warning: Layer {layer_idx} non trovato: {e}. Salto.\")\n",
    "            continue\n",
    "\n",
    "    if not combined_features:\n",
    "        raise ValueError(f\"Nessun layer caricato per {model_name}\")\n",
    "\n",
    "    X_final = np.concatenate(combined_features, axis=1)\n",
    "    return X_final, y\n",
    "\n",
    "\n",
    "def get_generator(seed=SEED):\n",
    "    \"\"\"Create a reproducible generator for DataLoader\"\"\"\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return g\n",
    "\n",
    "\n",
    "def train_autoencoder(X_train, X_val, input_dim, device, model_name, autoencoder_config=AUTOENCODER_CONFIG):\n",
    "    \"\"\"Train autoencoder for dimensionality reduction with early stopping.\"\"\"\n",
    "    \n",
    "    latent_dim = autoencoder_config['latent_dim']\n",
    "    hidden_dim = autoencoder_config['hidden_dim']\n",
    "    \n",
    "    print(f\"   Training Autoencoder for {model_name} ({input_dim} -> {latent_dim})...\")\n",
    "    \n",
    "    set_seed(SEED)\n",
    "    autoencoder = Autoencoder(\n",
    "        input_dim=input_dim,\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=autoencoder_config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        autoencoder.parameters(), \n",
    "        lr=autoencoder_config['learning_rate'], \n",
    "        weight_decay=autoencoder_config['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=autoencoder_config['max_epochs'])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = AutoencoderDataset(X_train)\n",
    "    val_dataset = AutoencoderDataset(X_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=autoencoder_config['batch_size'], \n",
    "                             shuffle=True, num_workers=0, generator=get_generator(SEED))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=autoencoder_config['batch_size'], \n",
    "                           shuffle=False, num_workers=0)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    epochs_trained = 0\n",
    "    \n",
    "    for epoch in range(autoencoder_config['max_epochs']):\n",
    "        # Training\n",
    "        autoencoder.train()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            X_recon, _ = autoencoder(X_batch)\n",
    "            loss = criterion(X_recon, X_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                autoencoder.parameters(), \n",
    "                max_norm=autoencoder_config['gradient_clip_max_norm']\n",
    "            )\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        autoencoder.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch in val_loader:\n",
    "                X_recon, _ = autoencoder(X_batch)\n",
    "                loss = criterion(X_recon, X_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 30 == 0:\n",
    "            print(f\"     Epoch {epoch+1:3d}/{autoencoder_config['max_epochs']} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss - autoencoder_config['early_stopping_min_delta']:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = autoencoder.state_dict().copy()\n",
    "            epochs_trained = epoch + 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= autoencoder_config['early_stopping_patience']:\n",
    "            print(f\"     Early stopping at epoch {epoch+1}. Best Val Loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "    \n",
    "    if epochs_trained == 0:\n",
    "        epochs_trained = autoencoder_config['max_epochs']\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        autoencoder.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"   ✓ Autoencoder trained. Final Val Loss: {best_val_loss:.6f}\")\n",
    "    return autoencoder, best_val_loss, epochs_trained\n",
    "\n",
    "\n",
    "def train_mlp_prober(X_train, y_train, X_val, y_val, input_dim, device, prober_config=PROBER_CONFIG):\n",
    "    \"\"\"Train MLP prober with early stopping based on validation accuracy.\"\"\"\n",
    "    \n",
    "    set_seed(SEED)\n",
    "    prober = MLPProber(\n",
    "        input_dim=input_dim, \n",
    "        hidden_dim=prober_config['hidden_dim'], \n",
    "        dropout=prober_config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Compute class weights for imbalanced data\n",
    "    if prober_config['use_class_weights']:\n",
    "        n_pos = y_train.sum().item() if isinstance(y_train, torch.Tensor) else y_train.sum()\n",
    "        n_neg = len(y_train) - n_pos\n",
    "        if n_pos > 0:\n",
    "            pos_weight = torch.tensor([n_neg / n_pos]).to(device)\n",
    "        else:\n",
    "            pos_weight = torch.tensor([1.0]).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        prober.parameters(), \n",
    "        lr=prober_config['learning_rate'], \n",
    "        weight_decay=prober_config['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=prober_config['max_epochs'])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = ClassificationDataset(X_train, y_train)\n",
    "    val_dataset = ClassificationDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=prober_config['batch_size'], \n",
    "                             shuffle=True, num_workers=0, generator=get_generator(SEED))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=prober_config['batch_size'], \n",
    "                           shuffle=False, num_workers=0)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    epochs_trained = 0\n",
    "    \n",
    "    for epoch in range(prober_config['max_epochs']):\n",
    "        # Training\n",
    "        prober.train()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = prober(X_batch)\n",
    "            loss = criterion(logits, y_batch.float())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                prober.parameters(), \n",
    "                max_norm=prober_config['gradient_clip_max_norm']\n",
    "            )\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        prober.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = prober.predict(X_batch)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        val_f1 = f1_score(all_labels, all_preds)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"     Epoch {epoch+1:3d}/{prober_config['max_epochs']} | Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early Stopping based on accuracy\n",
    "        if val_acc > best_val_acc + prober_config['early_stopping_min_delta']:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = prober.state_dict().copy()\n",
    "            epochs_trained = epoch + 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= prober_config['early_stopping_patience']:\n",
    "            print(f\"     Early stopping at epoch {epoch+1}. Best Val Acc: {best_val_acc:.4f}\")\n",
    "            break\n",
    "    \n",
    "    if epochs_trained == 0:\n",
    "        epochs_trained = prober_config['max_epochs']\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        prober.load_state_dict(best_model_state)\n",
    "    \n",
    "    return prober, best_val_acc, epochs_trained\n",
    "\n",
    "\n",
    "def train_alignment_network(X_source_train, X_target_train, X_source_val, X_target_val, \n",
    "                            latent_dim, device, alignment_config=ALIGNMENT_CONFIG):\n",
    "    \"\"\"Train alignment network to map student latent space to teacher latent space.\"\"\"\n",
    "    \n",
    "    print(\"   Training Alignment Network...\")\n",
    "    \n",
    "    set_seed(SEED)\n",
    "    aligner = AlignmentNetwork(\n",
    "        input_dim=latent_dim,\n",
    "        output_dim=latent_dim,\n",
    "        hidden_dim=alignment_config['hidden_dim'],\n",
    "        dropout=alignment_config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = MixedLoss(alpha=alignment_config['loss_alpha'], beta=alignment_config['loss_beta'])\n",
    "    optimizer = optim.AdamW(\n",
    "        aligner.parameters(), \n",
    "        lr=alignment_config['learning_rate'], \n",
    "        weight_decay=alignment_config['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=alignment_config['max_epochs'])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataset = AlignmentDataset(X_source_train, X_target_train)\n",
    "    val_dataset = AlignmentDataset(X_source_val, X_target_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=alignment_config['batch_size'], \n",
    "                             shuffle=True, num_workers=0, generator=get_generator(SEED))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=alignment_config['batch_size'], \n",
    "                           shuffle=False, num_workers=0)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    epochs_trained = 0\n",
    "    \n",
    "    for epoch in range(alignment_config['max_epochs']):\n",
    "        # Training\n",
    "        aligner.train()\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            projected = aligner(data)\n",
    "            loss = criterion(projected, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                aligner.parameters(), \n",
    "                max_norm=alignment_config['gradient_clip_max_norm']\n",
    "            )\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        aligner.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                projected = aligner(data)\n",
    "                loss = criterion(projected, target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"     Epoch {epoch+1:3d}/{alignment_config['max_epochs']} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss - alignment_config['early_stopping_min_delta']:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = aligner.state_dict().copy()\n",
    "            epochs_trained = epoch + 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= alignment_config['early_stopping_patience']:\n",
    "            print(f\"     Early stopping at epoch {epoch+1}. Best Val Loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "    \n",
    "    if epochs_trained == 0:\n",
    "        epochs_trained = alignment_config['max_epochs']\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        aligner.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"   ✓ Alignment Network trained. Final Val Loss: {best_val_loss:.6f}\")\n",
    "    return aligner, best_val_loss, epochs_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0113b",
   "metadata": {},
   "source": [
    "### Main Experiment Pipeline (with End-to-End Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad33b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_pipeline_with_autoencoder(X_teacher, y_teacher, teacher_name,\n",
    "                                              X_student, y_student, student_name,\n",
    "                                              alignment_data,\n",
    "                                              layer_type, config_name,\n",
    "                                              autoencoder_config=AUTOENCODER_CONFIG,\n",
    "                                              alignment_config=ALIGNMENT_CONFIG,\n",
    "                                              prober_config=PROBER_CONFIG):\n",
    "    \"\"\"\n",
    "    Pipeline con Autoencoder:\n",
    "    - Autoencoder: addestrato sul dataset PROPRIO di ogni modello (undersampling separato)\n",
    "    - Alignment: addestrato sul dataset CONCORDANTE (comuni a entrambi i modelli)\n",
    "    - Prober: addestrato sul dataset PROPRIO del teacher\n",
    "    \n",
    "    Args:\n",
    "        X_teacher, y_teacher: dati del teacher (undersampling proprio)\n",
    "        X_student, y_student: dati dello student (undersampling proprio)\n",
    "        alignment_data: dict con dati concordanti per alignment\n",
    "            - X_teacher_train, X_teacher_val: attivazioni teacher (concordanti)\n",
    "            - X_student_train, X_student_val: attivazioni student (concordanti)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {layer_type.upper()} → {teacher_name} ← {student_name}\")\n",
    "    print(f\"Using Autoencoder with latent_dim={autoencoder_config['latent_dim']}, hidden_dim={autoencoder_config['hidden_dim']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Dati per Autoencoder e Prober (dataset proprio di ogni modello)\n",
    "    X_A_train_full, X_A_test = X_teacher['X_train'], X_teacher['X_test']\n",
    "    y_A_train_full, y_A_test = y_teacher['y_train'], y_teacher['y_test']\n",
    "    X_B_train_full, X_B_test = X_student['X_train'], X_student['X_test']\n",
    "    y_B_train_full, y_B_test = y_student['y_train'], y_student['y_test']\n",
    "    \n",
    "    # Dati per Alignment (dataset concordante)\n",
    "    X_align_teacher_train = alignment_data['X_teacher_train']\n",
    "    X_align_teacher_val = alignment_data['X_teacher_val']\n",
    "    X_align_student_train = alignment_data['X_student_train']\n",
    "    X_align_student_val = alignment_data['X_student_val']\n",
    "\n",
    "    device = DEVICE\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"   Teacher dataset: {len(X_A_train_full)} train, {len(X_A_test)} test\")\n",
    "    print(f\"   Student dataset: {len(X_B_train_full)} train, {len(X_B_test)} test\")\n",
    "    print(f\"   Alignment dataset: {len(X_align_teacher_train)} train, {len(X_align_teacher_val)} val\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Train Autoencoder for Teacher (sul PROPRIO dataset)\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n1. Training Autoencoder for TEACHER (on teacher's own dataset)...\")\n",
    "    \n",
    "    num_train_A = len(X_A_train_full)\n",
    "    indices_A = np.arange(num_train_A)\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(indices_A)\n",
    "    ae_val_size_A = int(num_train_A * 0.15)\n",
    "    ae_train_idx_A = indices_A[ae_val_size_A:]\n",
    "    ae_val_idx_A = indices_A[:ae_val_size_A]\n",
    "    X_A_ae_train = torch.from_numpy(X_A_train_full[ae_train_idx_A]).float().to(device)\n",
    "    X_A_ae_val = torch.from_numpy(X_A_train_full[ae_val_idx_A]).float().to(device)\n",
    "\n",
    "    ae_teacher, ae_teacher_loss, ae_teacher_epochs = train_autoencoder(\n",
    "        X_A_ae_train, X_A_ae_val,\n",
    "        input_dim=X_A_train_full.shape[1],\n",
    "        device=device,\n",
    "        model_name=teacher_name,\n",
    "        autoencoder_config=autoencoder_config\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2. Train Autoencoder for Student (sul PROPRIO dataset)\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n2. Training Autoencoder for STUDENT (on student's own dataset)...\")\n",
    "\n",
    "    num_train_B = len(X_B_train_full)\n",
    "    indices_B = np.arange(num_train_B)\n",
    "    np.random.seed(SEED)\n",
    "    np.random.shuffle(indices_B)\n",
    "    ae_val_size_B = int(num_train_B * 0.15)\n",
    "    ae_train_idx_B = indices_B[ae_val_size_B:]\n",
    "    ae_val_idx_B = indices_B[:ae_val_size_B]\n",
    "    X_B_ae_train = torch.from_numpy(X_B_train_full[ae_train_idx_B]).float().to(device)\n",
    "    X_B_ae_val = torch.from_numpy(X_B_train_full[ae_val_idx_B]).float().to(device)\n",
    "\n",
    "    ae_student, ae_student_loss, ae_student_epochs = train_autoencoder(\n",
    "        X_B_ae_train, X_B_ae_val,\n",
    "        input_dim=X_B_train_full.shape[1],\n",
    "        device=device,\n",
    "        model_name=student_name,\n",
    "        autoencoder_config=autoencoder_config\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3. Encode all data to latent space\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n3. Encoding data to latent space...\")\n",
    "\n",
    "    ae_teacher.eval()\n",
    "    ae_student.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Teacher encodings (dataset proprio)\n",
    "        X_A_train_full_t = torch.from_numpy(X_A_train_full).float().to(device)\n",
    "        X_A_test_t = torch.from_numpy(X_A_test).float().to(device)\n",
    "        Z_A_train = ae_teacher.encode(X_A_train_full_t)\n",
    "        Z_A_test = ae_teacher.encode(X_A_test_t)\n",
    "        \n",
    "        # Student encodings (dataset proprio)\n",
    "        X_B_train_full_t = torch.from_numpy(X_B_train_full).float().to(device)\n",
    "        X_B_test_t = torch.from_numpy(X_B_test).float().to(device)\n",
    "        Z_B_train = ae_student.encode(X_B_train_full_t)\n",
    "        Z_B_test = ae_student.encode(X_B_test_t)\n",
    "        \n",
    "        # Alignment data encodings (dataset concordante)\n",
    "        X_align_teacher_train_t = torch.from_numpy(X_align_teacher_train).float().to(device)\n",
    "        X_align_teacher_val_t = torch.from_numpy(X_align_teacher_val).float().to(device)\n",
    "        X_align_student_train_t = torch.from_numpy(X_align_student_train).float().to(device)\n",
    "        X_align_student_val_t = torch.from_numpy(X_align_student_val).float().to(device)\n",
    "        \n",
    "        Z_align_teacher_train = ae_teacher.encode(X_align_teacher_train_t)\n",
    "        Z_align_teacher_val = ae_teacher.encode(X_align_teacher_val_t)\n",
    "        Z_align_student_train = ae_student.encode(X_align_student_train_t)\n",
    "        Z_align_student_val = ae_student.encode(X_align_student_val_t)\n",
    "\n",
    "    print(f\"   Teacher latent shape: {Z_A_train.shape}\")\n",
    "    print(f\"   Student latent shape: {Z_B_train.shape}\")\n",
    "    print(f\"   Alignment latent shape: {Z_align_teacher_train.shape}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4. Train MLP Prober on Teacher's Latent Space (dataset PROPRIO)\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n4. Training MLP Prober on Teacher's latent space (teacher's own dataset)...\")\n",
    "\n",
    "    # Usa gli stessi indici creati per l'autoencoder del teacher\n",
    "    Z_A_prober_train = Z_A_train[ae_train_idx_A]\n",
    "    y_A_prober_train = torch.from_numpy(y_A_train_full[ae_train_idx_A].astype(np.int64)).long().to(device)\n",
    "    Z_A_prober_val = Z_A_train[ae_val_idx_A]\n",
    "    y_A_prober_val = torch.from_numpy(y_A_train_full[ae_val_idx_A].astype(np.int64)).long().to(device)\n",
    "\n",
    "    probe_teacher, best_prober_acc, prober_epochs = train_mlp_prober(\n",
    "        Z_A_prober_train, y_A_prober_train,\n",
    "        Z_A_prober_val, y_A_prober_val,\n",
    "        input_dim=autoencoder_config['latent_dim'],\n",
    "        device=device,\n",
    "        prober_config=prober_config\n",
    "    )\n",
    "    print(f\"   Best prober validation Acc: {best_prober_acc:.4f}\")\n",
    "\n",
    "    # --- Teacher Metrics ---\n",
    "    probe_teacher.eval()\n",
    "    y_pred_teacher = probe_teacher.predict(Z_A_test).cpu().numpy()\n",
    "    y_proba_teacher = probe_teacher.predict_proba(Z_A_test).cpu().numpy()\n",
    "\n",
    "    cm_teacher = confusion_matrix(y_A_test, y_pred_teacher)\n",
    "    acc_teacher = accuracy_score(y_A_test, y_pred_teacher)\n",
    "    prec_teacher = precision_score(y_A_test, y_pred_teacher)\n",
    "    rec_teacher = recall_score(y_A_test, y_pred_teacher)\n",
    "    f1_teacher = f1_score(y_A_test, y_pred_teacher)\n",
    "    auroc_teacher = roc_auc_score(y_A_test, y_proba_teacher)\n",
    "    print(f\"   Teacher Test Acc: {acc_teacher:.4f}, F1: {f1_teacher:.4f}, AUROC: {auroc_teacher:.4f}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Train Alignment Network (dataset CONCORDANTE)\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n5. Training Alignment Network (Student → Teacher latent space, concordant dataset)...\")\n",
    "\n",
    "    aligner, align_loss, align_epochs = train_alignment_network(\n",
    "        Z_align_student_train, Z_align_teacher_train,\n",
    "        Z_align_student_val, Z_align_teacher_val,\n",
    "        latent_dim=autoencoder_config['latent_dim'],\n",
    "        device=device,\n",
    "        alignment_config=alignment_config\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 6. Save Models\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n6. Saving models...\")\n",
    "\n",
    "    model_save_dir = os.path.join(\"models\", layer_type)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "    # Save Teacher Autoencoder\n",
    "    ae_teacher_filename = os.path.join(model_save_dir, f\"{config_name}_autoencoder_{teacher_name}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': ae_teacher.state_dict(),\n",
    "        'autoencoder_config': autoencoder_config,\n",
    "        'input_dim': int(X_A_train_full.shape[1]),\n",
    "        'latent_dim': autoencoder_config['latent_dim'],\n",
    "        'best_val_loss': ae_teacher_loss,\n",
    "        'epochs_trained': ae_teacher_epochs,\n",
    "        'model_name': teacher_name,\n",
    "    }, ae_teacher_filename)\n",
    "    print(f\"   ✓ Teacher Autoencoder saved: {ae_teacher_filename}\")\n",
    "\n",
    "    # Save Student Autoencoder\n",
    "    ae_student_filename = os.path.join(model_save_dir, f\"{config_name}_autoencoder_{student_name}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': ae_student.state_dict(),\n",
    "        'autoencoder_config': autoencoder_config,\n",
    "        'input_dim': int(X_B_train_full.shape[1]),\n",
    "        'latent_dim': autoencoder_config['latent_dim'],\n",
    "        'best_val_loss': ae_student_loss,\n",
    "        'epochs_trained': ae_student_epochs,\n",
    "        'model_name': student_name,\n",
    "    }, ae_student_filename)\n",
    "    print(f\"   ✓ Student Autoencoder saved: {ae_student_filename}\")\n",
    "\n",
    "    # Save MLP Prober\n",
    "    prober_filename = os.path.join(model_save_dir, f\"{config_name}_mlp_prober_{teacher_name}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': probe_teacher.state_dict(),\n",
    "        'prober_config': prober_config,\n",
    "        'input_dim': autoencoder_config['latent_dim'],\n",
    "        'best_val_acc': best_prober_acc,\n",
    "        'epochs_trained': prober_epochs,\n",
    "        'teacher_model': teacher_name,\n",
    "    }, prober_filename)\n",
    "    print(f\"   ✓ MLP Prober saved: {prober_filename}\")\n",
    "\n",
    "    # Save Alignment Network\n",
    "    aligner_filename = os.path.join(model_save_dir, f\"{config_name}_aligner_{student_name}_to_{teacher_name}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': aligner.state_dict(),\n",
    "        'alignment_config': alignment_config,\n",
    "        'input_dim': autoencoder_config['latent_dim'],\n",
    "        'output_dim': autoencoder_config['latent_dim'],\n",
    "        'best_val_loss': align_loss,\n",
    "        'epochs_trained': align_epochs,\n",
    "        'student_model': student_name,\n",
    "        'teacher_model': teacher_name,\n",
    "    }, aligner_filename)\n",
    "    print(f\"   ✓ Alignment Network saved: {aligner_filename}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 7. Evaluation\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n7. Projecting student test set & evaluating...\")\n",
    "\n",
    "    aligner.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_B_aligned = aligner(Z_B_test)\n",
    "    \n",
    "    y_pred_cross = probe_teacher.predict(Z_B_aligned).cpu().numpy()\n",
    "    y_proba_cross = probe_teacher.predict_proba(Z_B_aligned).cpu().numpy()\n",
    "\n",
    "    # --- Cross-Model Metrics ---\n",
    "    cm_cross = confusion_matrix(y_B_test, y_pred_cross)\n",
    "    acc_cross = accuracy_score(y_B_test, y_pred_cross)\n",
    "    prec_cross = precision_score(y_B_test, y_pred_cross)\n",
    "    rec_cross = recall_score(y_B_test, y_pred_cross)\n",
    "    f1_cross = f1_score(y_B_test, y_pred_cross)\n",
    "    auroc_cross = roc_auc_score(y_B_test, y_proba_cross)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FINAL RESULT:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"   Teacher Acc          : {acc_teacher:.4f}, F1: {f1_teacher:.4f}, AUROC: {auroc_teacher:.4f}\")\n",
    "    print(f\"   Student → Teacher Acc: {acc_cross:.4f}, F1: {f1_cross:.4f}, AUROC: {auroc_cross:.4f}\")\n",
    "    print(f\"   Transfer gap (Acc)   : {acc_teacher - acc_cross:.4f}\")\n",
    "    print(f\"   Transfer gap (F1)    : {f1_teacher - f1_cross:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"type\": layer_type,\n",
    "        \"teacher_name\": teacher_name,\n",
    "        \"student_name\": student_name,\n",
    "        \"autoencoder_teacher\": {\n",
    "            \"input_dim\": int(X_A_train_full.shape[1]),\n",
    "            \"config\": autoencoder_config,\n",
    "            \"best_val_loss\": float(ae_teacher_loss),\n",
    "            \"epochs_trained\": ae_teacher_epochs,\n",
    "            \"model_path\": ae_teacher_filename\n",
    "        },\n",
    "        \"autoencoder_student\": {\n",
    "            \"input_dim\": int(X_B_train_full.shape[1]),\n",
    "            \"config\": autoencoder_config,\n",
    "            \"best_val_loss\": float(ae_student_loss),\n",
    "            \"epochs_trained\": ae_student_epochs,\n",
    "            \"model_path\": ae_student_filename\n",
    "        },\n",
    "        \"prober_model\": {\n",
    "            \"input_dim\": autoencoder_config['latent_dim'],\n",
    "            \"config\": prober_config,\n",
    "            \"best_val_acc\": float(best_prober_acc),\n",
    "            \"epochs_trained\": prober_epochs,\n",
    "            \"model_path\": prober_filename\n",
    "        },\n",
    "        \"alignment_model\": {\n",
    "            \"input_dim\": autoencoder_config['latent_dim'],\n",
    "            \"output_dim\": autoencoder_config['latent_dim'],\n",
    "            \"config\": alignment_config,\n",
    "            \"best_val_loss\": float(align_loss),\n",
    "            \"epochs_trained\": align_epochs,\n",
    "            \"model_path\": aligner_filename\n",
    "        },\n",
    "        \"teacher\": {\n",
    "            \"accuracy\": acc_teacher,\n",
    "            \"precision\": prec_teacher,\n",
    "            \"recall\": rec_teacher,\n",
    "            \"f1\": f1_teacher,\n",
    "            \"auroc\": auroc_teacher,\n",
    "            \"confusion_matrix\": cm_teacher.tolist()\n",
    "        },\n",
    "        \"student_on_teacher\": {\n",
    "            \"accuracy\": acc_cross,\n",
    "            \"precision\": prec_cross,\n",
    "            \"recall\": rec_cross,\n",
    "            \"f1\": f1_cross,\n",
    "            \"auroc\": auroc_cross,\n",
    "            \"confusion_matrix\": cm_cross.tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, layer_type, model_name=\"\", save_dir=\"confusion_matrices\"):\n",
    "    \"\"\"Plot and save confusion matrix as image.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,\n",
    "                xticklabels=['Non-Hallucinated', 'Hallucinated'],\n",
    "                yticklabels=['Non-Hallucinated', 'Hallucinated'])\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    title = f'Confusion Matrix - {layer_type.upper()} Layers'\n",
    "    if model_name:\n",
    "        title += f' ({model_name})'\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = os.path.join(save_dir, f'confusion_matrix_{layer_type}_{model_name}.png' if model_name else f'confusion_matrix_{layer_type}.png')\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   ✓ Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcd6f6",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300a9306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: PREPARAZIONE DATI\n",
      "================================================================================\n",
      "\n",
      "Step 1: Caricamento statistiche modelli...\n",
      "   Qwen2.5-7B: 27416 totali, 3565 allucinazioni\n",
      "   Falcon3-7B-Base: 27416 totali, 7531 allucinazioni\n",
      "\n",
      "Step 2: Analisi concordanza e undersampling per ALLINEAMENTO...\n",
      "  Campioni concordanti: 22078 / 27416\n",
      "    - Hallucinated (concordanti): 2879\n",
      "    - Non-hallucinated (concordanti): 19199\n",
      "  Dopo undersampling: 5758 campioni bilanciati (2879 per classe)\n",
      "\n",
      "Step 3: Preparazione dataset bilanciati per ogni LLM...\n",
      "   Qwen2.5-7B bilanciato: 7130 campioni (3565 hall, 3565 non-hall)\n",
      "   Falcon3-7B-Base bilanciato: 15062 campioni (7531 hall, 7531 non-hall)\n",
      "\n",
      "Campioni CONCORDANTI per Alignment/Autoencoder: 5758\n",
      "  Train: 4030, Val: 1728\n",
      "\n",
      "Campioni per PROBER (per modello):\n",
      "  Qwen2.5-7B: train=4991, test=2139\n",
      "  Falcon3-7B-Base: train=10543, test=4519\n",
      "\n",
      "Using LATENT_DIM=128\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: CARICAMENTO E PREPARAZIONE DATI PER LAYER TYPE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: ATTN\n",
      "========================================\n",
      "   Caricamento Qwen2.5-7B [attn]: layers [14, 15, 17]...\n",
      "   Caricamento Falcon3-7B-Base [attn]: layers [18, 19, 26]...\n",
      "   [ATTN] Align: train=4030, val=1728\n",
      "   [ATTN] Qwen2.5-7B: train=4991 ([2524 2467]), test=2139\n",
      "   [ATTN] Falcon3-7B-Base: train=10543 ([5223 5320]), test=4519\n",
      "   Normalizing data...\n",
      "\n",
      "   --- Scenario: Qwen2.5-7B (Teacher) <- Falcon3-7B-Base (Student) ---\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: ATTN → Qwen2.5-7B ← Falcon3-7B-Base\n",
      "Using Autoencoder with latent_dim=128, hidden_dim=256\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "   Teacher dataset: 4991 train, 2139 test\n",
      "   Student dataset: 10543 train, 4519 test\n",
      "   Alignment dataset: 4030 train, 1728 val\n",
      "\n",
      "1. Training Autoencoder for TEACHER (on teacher's own dataset)...\n",
      "   Training Autoencoder for Qwen2.5-7B (10752 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.168457 | Val Loss: 0.130626\n",
      "     Epoch  60/300 | Train Loss: 0.153647 | Val Loss: 0.114033\n",
      "     Epoch  90/300 | Train Loss: 0.147577 | Val Loss: 0.108388\n",
      "     Epoch 120/300 | Train Loss: 0.143856 | Val Loss: 0.103793\n",
      "     Epoch 150/300 | Train Loss: 0.140541 | Val Loss: 0.101244\n",
      "     Epoch 180/300 | Train Loss: 0.138573 | Val Loss: 0.099541\n",
      "     Epoch 210/300 | Train Loss: 0.138341 | Val Loss: 0.097536\n",
      "     Epoch 240/300 | Train Loss: 0.136673 | Val Loss: 0.097219\n",
      "     Epoch 270/300 | Train Loss: 0.135240 | Val Loss: 0.096675\n",
      "     Epoch 300/300 | Train Loss: 0.136423 | Val Loss: 0.096548\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.096520\n",
      "\n",
      "2. Training Autoencoder for STUDENT (on student's own dataset)...\n",
      "   Training Autoencoder for Falcon3-7B-Base (9216 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.116434 | Val Loss: 0.110730\n",
      "     Epoch  60/300 | Train Loss: 0.107460 | Val Loss: 0.098117\n",
      "     Epoch  90/300 | Train Loss: 0.100262 | Val Loss: 0.091788\n",
      "     Epoch 120/300 | Train Loss: 0.099482 | Val Loss: 0.088299\n",
      "     Epoch 150/300 | Train Loss: 0.093499 | Val Loss: 0.086331\n",
      "     Epoch 180/300 | Train Loss: 0.092679 | Val Loss: 0.085258\n",
      "     Epoch 210/300 | Train Loss: 0.090540 | Val Loss: 0.084510\n",
      "     Epoch 240/300 | Train Loss: 0.089445 | Val Loss: 0.083755\n",
      "     Epoch 270/300 | Train Loss: 0.090977 | Val Loss: 0.083142\n",
      "     Early stopping at epoch 289. Best Val Loss: 0.083049\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.083049\n",
      "\n",
      "3. Encoding data to latent space...\n",
      "   Teacher latent shape: torch.Size([4991, 128])\n",
      "   Student latent shape: torch.Size([10543, 128])\n",
      "   Alignment latent shape: torch.Size([4030, 128])\n",
      "\n",
      "4. Training MLP Prober on Teacher's latent space (teacher's own dataset)...\n",
      "     Epoch  20/200 | Train Loss: 0.0383 | Val F1: 0.9911 | Val Acc: 0.9906\n",
      "     Epoch  40/200 | Train Loss: 0.0208 | Val F1: 0.9936 | Val Acc: 0.9933\n",
      "     Early stopping at epoch 58. Best Val Acc: 0.9933\n",
      "   Best prober validation Acc: 0.9933\n",
      "   Teacher Test Acc: 0.9916, F1: 0.9918, AUROC: 0.9994\n",
      "\n",
      "5. Training Alignment Network (Student → Teacher latent space, concordant dataset)...\n",
      "   Training Alignment Network...\n",
      "     Epoch  50/500 | Train Loss: 0.573675 | Val Loss: 0.308845\n",
      "     Epoch 100/500 | Train Loss: 0.567288 | Val Loss: 0.290570\n",
      "     Epoch 150/500 | Train Loss: 0.560408 | Val Loss: 0.289905\n",
      "     Epoch 200/500 | Train Loss: 0.555131 | Val Loss: 0.289651\n",
      "     Epoch 250/500 | Train Loss: 0.555443 | Val Loss: 0.282259\n",
      "     Epoch 300/500 | Train Loss: 0.550891 | Val Loss: 0.277938\n",
      "     Epoch 350/500 | Train Loss: 0.546543 | Val Loss: 0.277391\n",
      "     Epoch 400/500 | Train Loss: 0.547056 | Val Loss: 0.278647\n",
      "     Epoch 450/500 | Train Loss: 0.544580 | Val Loss: 0.276682\n",
      "     Early stopping at epoch 463. Best Val Loss: 0.275919\n",
      "   ✓ Alignment Network trained. Final Val Loss: 0.275919\n",
      "\n",
      "6. Saving models...\n",
      "   ✓ Teacher Autoencoder saved: models\\attn\\CONFIG1_autoencoder_Qwen2.5-7B.pt\n",
      "   ✓ Student Autoencoder saved: models\\attn\\CONFIG1_autoencoder_Falcon3-7B-Base.pt\n",
      "   ✓ MLP Prober saved: models\\attn\\CONFIG1_mlp_prober_Qwen2.5-7B.pt\n",
      "   ✓ Alignment Network saved: models\\attn\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt\n",
      "\n",
      "7. Projecting student test set & evaluating...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT:\n",
      "==================================================\n",
      "   Teacher Acc          : 0.9916, F1: 0.9918, AUROC: 0.9994\n",
      "   Student → Teacher Acc: 0.9706, F1: 0.9697, AUROC: 0.9953\n",
      "   Transfer gap (Acc)   : 0.0210\n",
      "   Transfer gap (F1)    : 0.0221\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_attn_Teacher_Qwen2_5_7B.png\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_attn_Falcon3_7B_Base_on_Qwen2_5_7B.png\n",
      "\n",
      "   --- Scenario: Falcon3-7B-Base (Teacher) <- Qwen2.5-7B (Student) ---\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: ATTN → Falcon3-7B-Base ← Qwen2.5-7B\n",
      "Using Autoencoder with latent_dim=128, hidden_dim=256\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "   Teacher dataset: 10543 train, 4519 test\n",
      "   Student dataset: 4991 train, 2139 test\n",
      "   Alignment dataset: 4030 train, 1728 val\n",
      "\n",
      "1. Training Autoencoder for TEACHER (on teacher's own dataset)...\n",
      "   Training Autoencoder for Falcon3-7B-Base (9216 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.116434 | Val Loss: 0.110730\n",
      "     Epoch  60/300 | Train Loss: 0.107460 | Val Loss: 0.098117\n",
      "     Epoch  90/300 | Train Loss: 0.100262 | Val Loss: 0.091788\n",
      "     Epoch 120/300 | Train Loss: 0.099482 | Val Loss: 0.088299\n",
      "     Epoch 150/300 | Train Loss: 0.093499 | Val Loss: 0.086331\n",
      "     Epoch 180/300 | Train Loss: 0.092679 | Val Loss: 0.085258\n",
      "     Epoch 210/300 | Train Loss: 0.090540 | Val Loss: 0.084510\n",
      "     Epoch 240/300 | Train Loss: 0.089445 | Val Loss: 0.083755\n",
      "     Epoch 270/300 | Train Loss: 0.090977 | Val Loss: 0.083142\n",
      "     Early stopping at epoch 289. Best Val Loss: 0.083049\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.083049\n",
      "\n",
      "2. Training Autoencoder for STUDENT (on student's own dataset)...\n",
      "   Training Autoencoder for Qwen2.5-7B (10752 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.168457 | Val Loss: 0.130626\n",
      "     Epoch  60/300 | Train Loss: 0.153647 | Val Loss: 0.114033\n",
      "     Epoch  90/300 | Train Loss: 0.147577 | Val Loss: 0.108388\n",
      "     Epoch 120/300 | Train Loss: 0.143856 | Val Loss: 0.103793\n",
      "     Epoch 150/300 | Train Loss: 0.140541 | Val Loss: 0.101244\n",
      "     Epoch 180/300 | Train Loss: 0.138573 | Val Loss: 0.099541\n",
      "     Epoch 210/300 | Train Loss: 0.138341 | Val Loss: 0.097536\n",
      "     Epoch 240/300 | Train Loss: 0.136673 | Val Loss: 0.097219\n",
      "     Epoch 270/300 | Train Loss: 0.135240 | Val Loss: 0.096675\n",
      "     Epoch 300/300 | Train Loss: 0.136423 | Val Loss: 0.096548\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.096520\n",
      "\n",
      "3. Encoding data to latent space...\n",
      "   Teacher latent shape: torch.Size([10543, 128])\n",
      "   Student latent shape: torch.Size([4991, 128])\n",
      "   Alignment latent shape: torch.Size([4030, 128])\n",
      "\n",
      "4. Training MLP Prober on Teacher's latent space (teacher's own dataset)...\n",
      "     Epoch  20/200 | Train Loss: 0.0621 | Val F1: 0.9861 | Val Acc: 0.9861\n",
      "     Epoch  40/200 | Train Loss: 0.0456 | Val F1: 0.9899 | Val Acc: 0.9899\n",
      "     Epoch  60/200 | Train Loss: 0.0346 | Val F1: 0.9900 | Val Acc: 0.9899\n",
      "     Early stopping at epoch 66. Best Val Acc: 0.9924\n",
      "   Best prober validation Acc: 0.9924\n",
      "   Teacher Test Acc: 0.9841, F1: 0.9836, AUROC: 0.9983\n",
      "\n",
      "5. Training Alignment Network (Student → Teacher latent space, concordant dataset)...\n",
      "   Training Alignment Network...\n",
      "     Epoch  50/500 | Train Loss: 0.554115 | Val Loss: 0.293529\n",
      "     Epoch 100/500 | Train Loss: 0.549324 | Val Loss: 0.286681\n",
      "     Epoch 150/500 | Train Loss: 0.542740 | Val Loss: 0.284190\n",
      "     Epoch 200/500 | Train Loss: 0.536054 | Val Loss: 0.283986\n",
      "     Early stopping at epoch 227. Best Val Loss: 0.282234\n",
      "   ✓ Alignment Network trained. Final Val Loss: 0.282234\n",
      "\n",
      "6. Saving models...\n",
      "   ✓ Teacher Autoencoder saved: models\\attn\\CONFIG1_autoencoder_Falcon3-7B-Base.pt\n",
      "   ✓ Student Autoencoder saved: models\\attn\\CONFIG1_autoencoder_Qwen2.5-7B.pt\n",
      "   ✓ MLP Prober saved: models\\attn\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt\n",
      "   ✓ Alignment Network saved: models\\attn\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt\n",
      "\n",
      "7. Projecting student test set & evaluating...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT:\n",
      "==================================================\n",
      "   Teacher Acc          : 0.9841, F1: 0.9836, AUROC: 0.9983\n",
      "   Student → Teacher Acc: 0.8537, F1: 0.8519, AUROC: 0.9367\n",
      "   Transfer gap (Acc)   : 0.1304\n",
      "   Transfer gap (F1)    : 0.1318\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_attn_Teacher_Falcon3_7B_Base.png\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_attn_Qwen2_5_7B_on_Falcon3_7B_Base.png\n",
      "   Memory freed for attn.\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: MLP\n",
      "========================================\n",
      "   Caricamento Qwen2.5-7B [mlp]: layers [14, 23, 25]...\n",
      "   Caricamento Falcon3-7B-Base [mlp]: layers [18, 19, 20]...\n",
      "   [MLP] Align: train=4030, val=1728\n",
      "   [MLP] Qwen2.5-7B: train=4991 ([2524 2467]), test=2139\n",
      "   [MLP] Falcon3-7B-Base: train=10543 ([5223 5320]), test=4519\n",
      "   Normalizing data...\n",
      "\n",
      "   --- Scenario: Qwen2.5-7B (Teacher) <- Falcon3-7B-Base (Student) ---\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: MLP → Qwen2.5-7B ← Falcon3-7B-Base\n",
      "Using Autoencoder with latent_dim=128, hidden_dim=256\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "   Teacher dataset: 4991 train, 2139 test\n",
      "   Student dataset: 10543 train, 4519 test\n",
      "   Alignment dataset: 4030 train, 1728 val\n",
      "\n",
      "1. Training Autoencoder for TEACHER (on teacher's own dataset)...\n",
      "   Training Autoencoder for Qwen2.5-7B (10752 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.150964 | Val Loss: 0.118581\n",
      "     Epoch  60/300 | Train Loss: 0.136044 | Val Loss: 0.102957\n",
      "     Epoch  90/300 | Train Loss: 0.129913 | Val Loss: 0.096884\n",
      "     Epoch 120/300 | Train Loss: 0.126577 | Val Loss: 0.093555\n",
      "     Epoch 150/300 | Train Loss: 0.123312 | Val Loss: 0.089486\n",
      "     Epoch 180/300 | Train Loss: 0.121010 | Val Loss: 0.088502\n",
      "     Epoch 210/300 | Train Loss: 0.120500 | Val Loss: 0.086581\n",
      "     Epoch 240/300 | Train Loss: 0.119321 | Val Loss: 0.085471\n",
      "     Epoch 270/300 | Train Loss: 0.119034 | Val Loss: 0.085139\n",
      "     Epoch 300/300 | Train Loss: 0.118540 | Val Loss: 0.084992\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.084977\n",
      "\n",
      "2. Training Autoencoder for STUDENT (on student's own dataset)...\n",
      "   Training Autoencoder for Falcon3-7B-Base (9216 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.078001 | Val Loss: 0.071088\n",
      "     Epoch  60/300 | Train Loss: 0.073688 | Val Loss: 0.067158\n",
      "     Epoch  90/300 | Train Loss: 0.070869 | Val Loss: 0.063696\n",
      "     Epoch 120/300 | Train Loss: 0.069573 | Val Loss: 0.061006\n",
      "     Epoch 150/300 | Train Loss: 0.065545 | Val Loss: 0.057306\n",
      "     Epoch 180/300 | Train Loss: 0.064158 | Val Loss: 0.055950\n",
      "     Epoch 210/300 | Train Loss: 0.063666 | Val Loss: 0.054924\n",
      "     Epoch 240/300 | Train Loss: 0.062560 | Val Loss: 0.054345\n",
      "     Epoch 270/300 | Train Loss: 0.063690 | Val Loss: 0.054124\n",
      "     Early stopping at epoch 272. Best Val Loss: 0.054064\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.054064\n",
      "\n",
      "3. Encoding data to latent space...\n",
      "   Teacher latent shape: torch.Size([4991, 128])\n",
      "   Student latent shape: torch.Size([10543, 128])\n",
      "   Alignment latent shape: torch.Size([4030, 128])\n",
      "\n",
      "4. Training MLP Prober on Teacher's latent space (teacher's own dataset)...\n",
      "     Epoch  20/200 | Train Loss: 0.0406 | Val F1: 0.9898 | Val Acc: 0.9893\n",
      "     Epoch  40/200 | Train Loss: 0.0218 | Val F1: 0.9911 | Val Acc: 0.9906\n",
      "     Early stopping at epoch 52. Best Val Acc: 0.9933\n",
      "   Best prober validation Acc: 0.9933\n",
      "   Teacher Test Acc: 0.9897, F1: 0.9900, AUROC: 0.9997\n",
      "\n",
      "5. Training Alignment Network (Student → Teacher latent space, concordant dataset)...\n",
      "   Training Alignment Network...\n",
      "     Epoch  50/500 | Train Loss: 0.569378 | Val Loss: 0.308943\n",
      "     Epoch 100/500 | Train Loss: 0.562659 | Val Loss: 0.297517\n",
      "     Epoch 150/500 | Train Loss: 0.558352 | Val Loss: 0.292149\n",
      "     Epoch 200/500 | Train Loss: 0.553397 | Val Loss: 0.289742\n",
      "     Epoch 250/500 | Train Loss: 0.551702 | Val Loss: 0.289040\n",
      "     Epoch 300/500 | Train Loss: 0.548036 | Val Loss: 0.285815\n",
      "     Early stopping at epoch 321. Best Val Loss: 0.281967\n",
      "   ✓ Alignment Network trained. Final Val Loss: 0.281967\n",
      "\n",
      "6. Saving models...\n",
      "   ✓ Teacher Autoencoder saved: models\\mlp\\CONFIG1_autoencoder_Qwen2.5-7B.pt\n",
      "   ✓ Student Autoencoder saved: models\\mlp\\CONFIG1_autoencoder_Falcon3-7B-Base.pt\n",
      "   ✓ MLP Prober saved: models\\mlp\\CONFIG1_mlp_prober_Qwen2.5-7B.pt\n",
      "   ✓ Alignment Network saved: models\\mlp\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt\n",
      "\n",
      "7. Projecting student test set & evaluating...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT:\n",
      "==================================================\n",
      "   Teacher Acc          : 0.9897, F1: 0.9900, AUROC: 0.9997\n",
      "   Student → Teacher Acc: 0.9608, F1: 0.9593, AUROC: 0.9915\n",
      "   Transfer gap (Acc)   : 0.0289\n",
      "   Transfer gap (F1)    : 0.0307\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_mlp_Teacher_Qwen2_5_7B.png\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_mlp_Falcon3_7B_Base_on_Qwen2_5_7B.png\n",
      "\n",
      "   --- Scenario: Falcon3-7B-Base (Teacher) <- Qwen2.5-7B (Student) ---\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: MLP → Falcon3-7B-Base ← Qwen2.5-7B\n",
      "Using Autoencoder with latent_dim=128, hidden_dim=256\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "   Teacher dataset: 10543 train, 4519 test\n",
      "   Student dataset: 4991 train, 2139 test\n",
      "   Alignment dataset: 4030 train, 1728 val\n",
      "\n",
      "1. Training Autoencoder for TEACHER (on teacher's own dataset)...\n",
      "   Training Autoencoder for Falcon3-7B-Base (9216 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.078001 | Val Loss: 0.071088\n",
      "     Epoch  60/300 | Train Loss: 0.073688 | Val Loss: 0.067158\n",
      "     Epoch  90/300 | Train Loss: 0.070869 | Val Loss: 0.063696\n",
      "     Epoch 120/300 | Train Loss: 0.069573 | Val Loss: 0.061006\n",
      "     Epoch 150/300 | Train Loss: 0.065545 | Val Loss: 0.057306\n",
      "     Epoch 180/300 | Train Loss: 0.064158 | Val Loss: 0.055950\n",
      "     Epoch 210/300 | Train Loss: 0.063666 | Val Loss: 0.054924\n",
      "     Epoch 240/300 | Train Loss: 0.062560 | Val Loss: 0.054345\n",
      "     Epoch 270/300 | Train Loss: 0.063690 | Val Loss: 0.054124\n",
      "     Early stopping at epoch 272. Best Val Loss: 0.054064\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.054064\n",
      "\n",
      "2. Training Autoencoder for STUDENT (on student's own dataset)...\n",
      "   Training Autoencoder for Qwen2.5-7B (10752 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.150964 | Val Loss: 0.118581\n",
      "     Epoch  60/300 | Train Loss: 0.136044 | Val Loss: 0.102957\n",
      "     Epoch  90/300 | Train Loss: 0.129913 | Val Loss: 0.096884\n",
      "     Epoch 120/300 | Train Loss: 0.126577 | Val Loss: 0.093555\n",
      "     Epoch 150/300 | Train Loss: 0.123312 | Val Loss: 0.089486\n",
      "     Epoch 180/300 | Train Loss: 0.121010 | Val Loss: 0.088502\n",
      "     Epoch 210/300 | Train Loss: 0.120500 | Val Loss: 0.086581\n",
      "     Epoch 240/300 | Train Loss: 0.119321 | Val Loss: 0.085471\n",
      "     Epoch 270/300 | Train Loss: 0.119034 | Val Loss: 0.085139\n",
      "     Epoch 300/300 | Train Loss: 0.118540 | Val Loss: 0.084992\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.084977\n",
      "\n",
      "3. Encoding data to latent space...\n",
      "   Teacher latent shape: torch.Size([10543, 128])\n",
      "   Student latent shape: torch.Size([4991, 128])\n",
      "   Alignment latent shape: torch.Size([4030, 128])\n",
      "\n",
      "4. Training MLP Prober on Teacher's latent space (teacher's own dataset)...\n",
      "     Epoch  20/200 | Train Loss: 0.0871 | Val F1: 0.9723 | Val Acc: 0.9722\n",
      "     Epoch  40/200 | Train Loss: 0.0578 | Val F1: 0.9753 | Val Acc: 0.9753\n",
      "     Epoch  60/200 | Train Loss: 0.0511 | Val F1: 0.9798 | Val Acc: 0.9798\n",
      "     Epoch  80/200 | Train Loss: 0.0439 | Val F1: 0.9805 | Val Acc: 0.9804\n",
      "     Epoch 100/200 | Train Loss: 0.0414 | Val F1: 0.9830 | Val Acc: 0.9829\n",
      "     Epoch 120/200 | Train Loss: 0.0368 | Val F1: 0.9849 | Val Acc: 0.9848\n",
      "     Epoch 140/200 | Train Loss: 0.0344 | Val F1: 0.9850 | Val Acc: 0.9848\n",
      "     Early stopping at epoch 153. Best Val Acc: 0.9873\n",
      "   Best prober validation Acc: 0.9873\n",
      "   Teacher Test Acc: 0.9807, F1: 0.9803, AUROC: 0.9971\n",
      "\n",
      "5. Training Alignment Network (Student → Teacher latent space, concordant dataset)...\n",
      "   Training Alignment Network...\n",
      "     Epoch  50/500 | Train Loss: 0.569436 | Val Loss: 0.318855\n",
      "     Epoch 100/500 | Train Loss: 0.559939 | Val Loss: 0.306679\n",
      "     Epoch 150/500 | Train Loss: 0.553738 | Val Loss: 0.308056\n",
      "     Epoch 200/500 | Train Loss: 0.551425 | Val Loss: 0.307089\n",
      "     Early stopping at epoch 223. Best Val Loss: 0.303948\n",
      "   ✓ Alignment Network trained. Final Val Loss: 0.303948\n",
      "\n",
      "6. Saving models...\n",
      "   ✓ Teacher Autoencoder saved: models\\mlp\\CONFIG1_autoencoder_Falcon3-7B-Base.pt\n",
      "   ✓ Student Autoencoder saved: models\\mlp\\CONFIG1_autoencoder_Qwen2.5-7B.pt\n",
      "   ✓ MLP Prober saved: models\\mlp\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt\n",
      "   ✓ Alignment Network saved: models\\mlp\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt\n",
      "\n",
      "7. Projecting student test set & evaluating...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT:\n",
      "==================================================\n",
      "   Teacher Acc          : 0.9807, F1: 0.9803, AUROC: 0.9971\n",
      "   Student → Teacher Acc: 0.9411, F1: 0.9406, AUROC: 0.9832\n",
      "   Transfer gap (Acc)   : 0.0397\n",
      "   Transfer gap (F1)    : 0.0396\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_mlp_Teacher_Falcon3_7B_Base.png\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_mlp_Qwen2_5_7B_on_Falcon3_7B_Base.png\n",
      "   Memory freed for mlp.\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: HIDDEN\n",
      "========================================\n",
      "   Caricamento Qwen2.5-7B [hidden]: layers [15, 16, 17]...\n",
      "   Caricamento Falcon3-7B-Base [hidden]: layers [17, 18, 21]...\n",
      "   [HIDDEN] Align: train=4030, val=1728\n",
      "   [HIDDEN] Qwen2.5-7B: train=4991 ([2524 2467]), test=2139\n",
      "   [HIDDEN] Falcon3-7B-Base: train=10543 ([5223 5320]), test=4519\n",
      "   Normalizing data...\n",
      "\n",
      "   --- Scenario: Qwen2.5-7B (Teacher) <- Falcon3-7B-Base (Student) ---\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: HIDDEN → Qwen2.5-7B ← Falcon3-7B-Base\n",
      "Using Autoencoder with latent_dim=128, hidden_dim=256\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "   Teacher dataset: 4991 train, 2139 test\n",
      "   Student dataset: 10543 train, 4519 test\n",
      "   Alignment dataset: 4030 train, 1728 val\n",
      "\n",
      "1. Training Autoencoder for TEACHER (on teacher's own dataset)...\n",
      "   Training Autoencoder for Qwen2.5-7B (10752 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.122119 | Val Loss: 0.095010\n",
      "     Epoch  60/300 | Train Loss: 0.110695 | Val Loss: 0.081918\n",
      "     Epoch  90/300 | Train Loss: 0.104824 | Val Loss: 0.074610\n",
      "     Epoch 120/300 | Train Loss: 0.102144 | Val Loss: 0.071168\n",
      "     Epoch 150/300 | Train Loss: 0.098877 | Val Loss: 0.069055\n",
      "     Epoch 180/300 | Train Loss: 0.098559 | Val Loss: 0.067188\n",
      "     Epoch 210/300 | Train Loss: 0.096579 | Val Loss: 0.066108\n",
      "     Epoch 240/300 | Train Loss: 0.095732 | Val Loss: 0.065594\n",
      "     Epoch 270/300 | Train Loss: 0.096025 | Val Loss: 0.065351\n",
      "     Early stopping at epoch 289. Best Val Loss: 0.065256\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.065256\n",
      "\n",
      "2. Training Autoencoder for STUDENT (on student's own dataset)...\n",
      "   Training Autoencoder for Falcon3-7B-Base (9216 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.074926 | Val Loss: 0.074458\n",
      "     Epoch  60/300 | Train Loss: 0.069876 | Val Loss: 0.067891\n",
      "     Epoch  90/300 | Train Loss: 0.065875 | Val Loss: 0.063839\n",
      "     Epoch 120/300 | Train Loss: 0.065346 | Val Loss: 0.062678\n",
      "     Epoch 150/300 | Train Loss: 0.061977 | Val Loss: 0.060987\n",
      "     Epoch 180/300 | Train Loss: 0.060857 | Val Loss: 0.059857\n",
      "     Epoch 210/300 | Train Loss: 0.058880 | Val Loss: 0.059344\n",
      "     Early stopping at epoch 216. Best Val Loss: 0.059189\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.059189\n",
      "\n",
      "3. Encoding data to latent space...\n",
      "   Teacher latent shape: torch.Size([4991, 128])\n",
      "   Student latent shape: torch.Size([10543, 128])\n",
      "   Alignment latent shape: torch.Size([4030, 128])\n",
      "\n",
      "4. Training MLP Prober on Teacher's latent space (teacher's own dataset)...\n",
      "     Epoch  20/200 | Train Loss: 0.0406 | Val F1: 0.9898 | Val Acc: 0.9893\n",
      "     Epoch  40/200 | Train Loss: 0.0248 | Val F1: 0.9923 | Val Acc: 0.9920\n",
      "     Early stopping at epoch 54. Best Val Acc: 0.9933\n",
      "   Best prober validation Acc: 0.9933\n",
      "   Teacher Test Acc: 0.9935, F1: 0.9936, AUROC: 0.9997\n",
      "\n",
      "5. Training Alignment Network (Student → Teacher latent space, concordant dataset)...\n",
      "   Training Alignment Network...\n",
      "     Epoch  50/500 | Train Loss: 0.572121 | Val Loss: 0.314401\n",
      "     Epoch 100/500 | Train Loss: 0.560498 | Val Loss: 0.301401\n",
      "     Epoch 150/500 | Train Loss: 0.555100 | Val Loss: 0.296076\n",
      "     Epoch 200/500 | Train Loss: 0.551358 | Val Loss: 0.295856\n",
      "     Epoch 250/500 | Train Loss: 0.548665 | Val Loss: 0.290713\n",
      "     Epoch 300/500 | Train Loss: 0.548670 | Val Loss: 0.289298\n",
      "     Epoch 350/500 | Train Loss: 0.543550 | Val Loss: 0.286104\n",
      "     Epoch 400/500 | Train Loss: 0.543051 | Val Loss: 0.285435\n",
      "     Early stopping at epoch 446. Best Val Loss: 0.284021\n",
      "   ✓ Alignment Network trained. Final Val Loss: 0.284021\n",
      "\n",
      "6. Saving models...\n",
      "   ✓ Teacher Autoencoder saved: models\\hidden\\CONFIG1_autoencoder_Qwen2.5-7B.pt\n",
      "   ✓ Student Autoencoder saved: models\\hidden\\CONFIG1_autoencoder_Falcon3-7B-Base.pt\n",
      "   ✓ MLP Prober saved: models\\hidden\\CONFIG1_mlp_prober_Qwen2.5-7B.pt\n",
      "   ✓ Alignment Network saved: models\\hidden\\CONFIG1_aligner_Falcon3-7B-Base_to_Qwen2.5-7B.pt\n",
      "\n",
      "7. Projecting student test set & evaluating...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT:\n",
      "==================================================\n",
      "   Teacher Acc          : 0.9935, F1: 0.9936, AUROC: 0.9997\n",
      "   Student → Teacher Acc: 0.9586, F1: 0.9569, AUROC: 0.9892\n",
      "   Transfer gap (Acc)   : 0.0348\n",
      "   Transfer gap (F1)    : 0.0367\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_hidden_Teacher_Qwen2_5_7B.png\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_hidden_Falcon3_7B_Base_on_Qwen2_5_7B.png\n",
      "\n",
      "   --- Scenario: Falcon3-7B-Base (Teacher) <- Qwen2.5-7B (Student) ---\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: HIDDEN → Falcon3-7B-Base ← Qwen2.5-7B\n",
      "Using Autoencoder with latent_dim=128, hidden_dim=256\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "   Teacher dataset: 10543 train, 4519 test\n",
      "   Student dataset: 4991 train, 2139 test\n",
      "   Alignment dataset: 4030 train, 1728 val\n",
      "\n",
      "1. Training Autoencoder for TEACHER (on teacher's own dataset)...\n",
      "   Training Autoencoder for Falcon3-7B-Base (9216 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.074926 | Val Loss: 0.074458\n",
      "     Epoch  60/300 | Train Loss: 0.069876 | Val Loss: 0.067891\n",
      "     Epoch  90/300 | Train Loss: 0.065875 | Val Loss: 0.063839\n",
      "     Epoch 120/300 | Train Loss: 0.065346 | Val Loss: 0.062678\n",
      "     Epoch 150/300 | Train Loss: 0.061977 | Val Loss: 0.060987\n",
      "     Epoch 180/300 | Train Loss: 0.060857 | Val Loss: 0.059857\n",
      "     Epoch 210/300 | Train Loss: 0.058880 | Val Loss: 0.059344\n",
      "     Early stopping at epoch 216. Best Val Loss: 0.059189\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.059189\n",
      "\n",
      "2. Training Autoencoder for STUDENT (on student's own dataset)...\n",
      "   Training Autoencoder for Qwen2.5-7B (10752 -> 128)...\n",
      "     Epoch  30/300 | Train Loss: 0.122119 | Val Loss: 0.095010\n",
      "     Epoch  60/300 | Train Loss: 0.110695 | Val Loss: 0.081918\n",
      "     Epoch  90/300 | Train Loss: 0.104824 | Val Loss: 0.074610\n",
      "     Epoch 120/300 | Train Loss: 0.102144 | Val Loss: 0.071168\n",
      "     Epoch 150/300 | Train Loss: 0.098877 | Val Loss: 0.069055\n",
      "     Epoch 180/300 | Train Loss: 0.098559 | Val Loss: 0.067188\n",
      "     Epoch 210/300 | Train Loss: 0.096579 | Val Loss: 0.066108\n",
      "     Epoch 240/300 | Train Loss: 0.095732 | Val Loss: 0.065594\n",
      "     Epoch 270/300 | Train Loss: 0.096025 | Val Loss: 0.065351\n",
      "     Early stopping at epoch 289. Best Val Loss: 0.065256\n",
      "   ✓ Autoencoder trained. Final Val Loss: 0.065256\n",
      "\n",
      "3. Encoding data to latent space...\n",
      "   Teacher latent shape: torch.Size([10543, 128])\n",
      "   Student latent shape: torch.Size([4991, 128])\n",
      "   Alignment latent shape: torch.Size([4030, 128])\n",
      "\n",
      "4. Training MLP Prober on Teacher's latent space (teacher's own dataset)...\n",
      "     Epoch  20/200 | Train Loss: 0.0992 | Val F1: 0.9684 | Val Acc: 0.9684\n",
      "     Epoch  40/200 | Train Loss: 0.0662 | Val F1: 0.9708 | Val Acc: 0.9709\n",
      "     Epoch  60/200 | Train Loss: 0.0582 | Val F1: 0.9743 | Val Acc: 0.9741\n",
      "     Epoch  80/200 | Train Loss: 0.0524 | Val F1: 0.9762 | Val Acc: 0.9760\n",
      "     Early stopping at epoch 86. Best Val Acc: 0.9779\n",
      "   Best prober validation Acc: 0.9779\n",
      "   Teacher Test Acc: 0.9770, F1: 0.9763, AUROC: 0.9952\n",
      "\n",
      "5. Training Alignment Network (Student → Teacher latent space, concordant dataset)...\n",
      "   Training Alignment Network...\n",
      "     Epoch  50/500 | Train Loss: 0.557992 | Val Loss: 0.309792\n",
      "     Epoch 100/500 | Train Loss: 0.546349 | Val Loss: 0.301210\n",
      "     Epoch 150/500 | Train Loss: 0.542171 | Val Loss: 0.300678\n",
      "     Epoch 200/500 | Train Loss: 0.536741 | Val Loss: 0.300701\n",
      "     Epoch 250/500 | Train Loss: 0.535212 | Val Loss: 0.300031\n",
      "     Early stopping at epoch 258. Best Val Loss: 0.297232\n",
      "   ✓ Alignment Network trained. Final Val Loss: 0.297232\n",
      "\n",
      "6. Saving models...\n",
      "   ✓ Teacher Autoencoder saved: models\\hidden\\CONFIG1_autoencoder_Falcon3-7B-Base.pt\n",
      "   ✓ Student Autoencoder saved: models\\hidden\\CONFIG1_autoencoder_Qwen2.5-7B.pt\n",
      "   ✓ MLP Prober saved: models\\hidden\\CONFIG1_mlp_prober_Falcon3-7B-Base.pt\n",
      "   ✓ Alignment Network saved: models\\hidden\\CONFIG1_aligner_Qwen2.5-7B_to_Falcon3-7B-Base.pt\n",
      "\n",
      "7. Projecting student test set & evaluating...\n",
      "\n",
      "==================================================\n",
      "FINAL RESULT:\n",
      "==================================================\n",
      "   Teacher Acc          : 0.9770, F1: 0.9763, AUROC: 0.9952\n",
      "   Student → Teacher Acc: 0.9004, F1: 0.8987, AUROC: 0.9503\n",
      "   Transfer gap (Acc)   : 0.0766\n",
      "   Transfer gap (F1)    : 0.0776\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_hidden_Teacher_Falcon3_7B_Base.png\n",
      "   ✓ Saved: confusion_matrices\\confusion_matrix_hidden_Qwen2_5_7B_on_Falcon3_7B_Base.png\n",
      "   Memory freed for hidden.\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: SALVATAGGIO RISULTATI\n",
      "================================================================================\n",
      "\n",
      "✓ Results saved to: results_metrics/approach2_autoencoder_results.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: PREPARAZIONE DATI\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: Ottieni statistiche per entrambi i modelli\n",
    "# ============================================\n",
    "print(\"Step 1: Caricamento statistiche modelli...\")\n",
    "model_a_stats = get_stats(MODEL_A, DATASET_NAME)\n",
    "model_b_stats = get_stats(MODEL_B, DATASET_NAME)\n",
    "print(f\"   {MODEL_A}: {model_a_stats['total']} totali, {model_a_stats['hallucinations']} allucinazioni\")\n",
    "print(f\"   {MODEL_B}: {model_b_stats['total']} totali, {model_b_stats['hallucinations']} allucinazioni\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: Trova campioni concordanti con undersampling (per ALLINEAMENTO)\n",
    "# ============================================\n",
    "print(\"\\nStep 2: Analisi concordanza e undersampling per ALLINEAMENTO...\")\n",
    "alignment_indices, alignment_labels = get_concordant_indices_and_undersample(model_a_stats, model_b_stats, seed=SEED)\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: Prepara dataset bilanciati per ogni LLM (con undersampling separato)\n",
    "# ============================================\n",
    "print(\"\\nStep 3: Preparazione dataset bilanciati per ogni LLM...\")\n",
    "model_a_balanced_idx, model_a_balanced_labels = get_undersampled_indices_per_model(model_a_stats, SEED)\n",
    "model_b_balanced_idx, model_b_balanced_labels = get_undersampled_indices_per_model(model_b_stats, SEED)\n",
    "\n",
    "print(f\"   {MODEL_A} bilanciato: {len(model_a_balanced_idx)} campioni ({np.sum(model_a_balanced_labels==1)} hall, {np.sum(model_a_balanced_labels==0)} non-hall)\")\n",
    "print(f\"   {MODEL_B} bilanciato: {len(model_b_balanced_idx)} campioni ({np.sum(model_b_balanced_labels==1)} hall, {np.sum(model_b_balanced_labels==0)} non-hall)\")\n",
    "\n",
    "# ============================================\n",
    "# SPLIT per ALIGNMENT (campioni concordanti)\n",
    "# ============================================\n",
    "n_alignment = len(alignment_indices)\n",
    "rng = np.random.RandomState(SEED)\n",
    "shuffled_alignment_idx = rng.permutation(n_alignment)\n",
    "split_idx_align = int(0.7 * n_alignment)\n",
    "alignment_train_local_idx = shuffled_alignment_idx[:split_idx_align]\n",
    "alignment_val_local_idx = shuffled_alignment_idx[split_idx_align:]\n",
    "\n",
    "print(f\"\\nCampioni CONCORDANTI per Alignment/Autoencoder: {n_alignment}\")\n",
    "print(f\"  Train: {len(alignment_train_local_idx)}, Val: {len(alignment_val_local_idx)}\")\n",
    "\n",
    "# ============================================\n",
    "# SPLIT per PROBER (undersampling separato per modello)\n",
    "# ============================================\n",
    "rng_a = np.random.RandomState(SEED)\n",
    "rng_b = np.random.RandomState(SEED + 1)\n",
    "\n",
    "shuffled_a = rng_a.permutation(len(model_a_balanced_idx))\n",
    "shuffled_b = rng_b.permutation(len(model_b_balanced_idx))\n",
    "\n",
    "split_a = int(0.7 * len(model_a_balanced_idx))\n",
    "split_b = int(0.7 * len(model_b_balanced_idx))\n",
    "\n",
    "model_a_train_local = shuffled_a[:split_a]\n",
    "model_a_test_local = shuffled_a[split_a:]\n",
    "model_b_train_local = shuffled_b[:split_b]\n",
    "model_b_test_local = shuffled_b[split_b:]\n",
    "\n",
    "print(f\"\\nCampioni per PROBER (per modello):\")\n",
    "print(f\"  {MODEL_A}: train={len(model_a_train_local)}, test={len(model_a_test_local)}\")\n",
    "print(f\"  {MODEL_B}: train={len(model_b_train_local)}, test={len(model_b_test_local)}\")\n",
    "print(f\"\\nUsing LATENT_DIM={AUTOENCODER_CONFIG['latent_dim']}\")\n",
    "\n",
    "scenarios = [\n",
    "    {\"teacher_model\": MODEL_A, \"student_model\": MODEL_B},\n",
    "    {\"teacher_model\": MODEL_B, \"student_model\": MODEL_A}\n",
    "]\n",
    "\n",
    "scenario_results_map = {0: [], 1: []}\n",
    "\n",
    "# ============================================\n",
    "# PHASE 2: CARICAMENTO E PREPARAZIONE DATI\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: CARICAMENTO E PREPARAZIONE DATI PER LAYER TYPE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for layer_type in ['attn', 'mlp', 'hidden']:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"PROCESSING LAYER TYPE: {layer_type.upper()}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    try:\n",
    "        # ============================================\n",
    "        # Carica dati COMPLETI per entrambi i modelli\n",
    "        # ============================================\n",
    "        X_model_a_full, _ = load_concatenated_layers(MODEL_A, DATASET_NAME, LAYER_CONFIG[MODEL_A][layer_type], layer_type)\n",
    "        X_model_b_full, _ = load_concatenated_layers(MODEL_B, DATASET_NAME, LAYER_CONFIG[MODEL_B][layer_type], layer_type)\n",
    "        \n",
    "        # ============================================\n",
    "        # DATI PER ALIGNMENT/AUTOENCODER (concordanti)\n",
    "        # ============================================\n",
    "        X_align_a_train = X_model_a_full[alignment_indices][alignment_train_local_idx]\n",
    "        X_align_b_train = X_model_b_full[alignment_indices][alignment_train_local_idx]\n",
    "        X_align_a_val = X_model_a_full[alignment_indices][alignment_val_local_idx]\n",
    "        X_align_b_val = X_model_b_full[alignment_indices][alignment_val_local_idx]\n",
    "        \n",
    "        # ============================================\n",
    "        # DATI PER PROBER MODEL A (undersampling separato)\n",
    "        # ============================================\n",
    "        X_a_balanced = X_model_a_full[model_a_balanced_idx]\n",
    "        X_a_train = X_a_balanced[model_a_train_local]\n",
    "        X_a_test = X_a_balanced[model_a_test_local]\n",
    "        y_a_train = model_a_balanced_labels[model_a_train_local]\n",
    "        y_a_test = model_a_balanced_labels[model_a_test_local]\n",
    "        \n",
    "        # ============================================\n",
    "        # DATI PER PROBER MODEL B (undersampling separato)\n",
    "        # ============================================\n",
    "        X_b_balanced = X_model_b_full[model_b_balanced_idx]\n",
    "        X_b_train = X_b_balanced[model_b_train_local]\n",
    "        X_b_test = X_b_balanced[model_b_test_local]\n",
    "        y_b_train = model_b_balanced_labels[model_b_train_local]\n",
    "        y_b_test = model_b_balanced_labels[model_b_test_local]\n",
    "        \n",
    "        del X_model_a_full, X_model_b_full, X_a_balanced, X_b_balanced\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"   [{layer_type.upper()}] Align: train={X_align_a_train.shape[0]}, val={X_align_a_val.shape[0]}\")\n",
    "        print(f\"   [{layer_type.upper()}] {MODEL_A}: train={len(y_a_train)} ({np.bincount(y_a_train)}), test={len(y_a_test)}\")\n",
    "        print(f\"   [{layer_type.upper()}] {MODEL_B}: train={len(y_b_train)} ({np.bincount(y_b_train)}), test={len(y_b_test)}\")\n",
    "        \n",
    "        # ============================================\n",
    "        # Normalizzazione\n",
    "        # ============================================\n",
    "        print(\"   Normalizing data...\")\n",
    "        scaler_align_a, scaler_align_b = StandardScaler(), StandardScaler()\n",
    "        scaler_a, scaler_b = StandardScaler(), StandardScaler()\n",
    "        \n",
    "        X_align_a_train_norm = scaler_align_a.fit_transform(X_align_a_train).astype(np.float32)\n",
    "        X_align_b_train_norm = scaler_align_b.fit_transform(X_align_b_train).astype(np.float32)\n",
    "        X_align_a_val_norm = scaler_align_a.transform(X_align_a_val).astype(np.float32)\n",
    "        X_align_b_val_norm = scaler_align_b.transform(X_align_b_val).astype(np.float32)\n",
    "        \n",
    "        X_a_train_norm = scaler_a.fit_transform(X_a_train).astype(np.float32)\n",
    "        X_a_test_norm = scaler_a.transform(X_a_test).astype(np.float32)\n",
    "        \n",
    "        X_b_train_norm = scaler_b.fit_transform(X_b_train).astype(np.float32)\n",
    "        X_b_test_norm = scaler_b.transform(X_b_test).astype(np.float32)\n",
    "        \n",
    "        # ============================================\n",
    "        # Prepara struttura dati\n",
    "        # ============================================\n",
    "        data_splits = {\n",
    "            \"alignment\": {\n",
    "                \"X_a_train\": X_align_a_train_norm,\n",
    "                \"X_b_train\": X_align_b_train_norm,\n",
    "                \"X_a_val\": X_align_a_val_norm,\n",
    "                \"X_b_val\": X_align_b_val_norm,\n",
    "                \"scaler_a\": scaler_align_a,\n",
    "                \"scaler_b\": scaler_align_b\n",
    "            },\n",
    "            \"model_a\": {\n",
    "                \"X_train\": X_a_train_norm, \"X_test\": X_a_test_norm,\n",
    "                \"y_train\": y_a_train, \"y_test\": y_a_test,\n",
    "                \"X_test_raw\": X_a_test\n",
    "            },\n",
    "            \"model_b\": {\n",
    "                \"X_train\": X_b_train_norm, \"X_test\": X_b_test_norm,\n",
    "                \"y_train\": y_b_train, \"y_test\": y_b_test,\n",
    "                \"X_test_raw\": X_b_test\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # ============================================\n",
    "        # Esegui esperimenti per entrambi gli scenari\n",
    "        # ============================================\n",
    "        for i, scenario in enumerate(scenarios):\n",
    "            print(f\"\\n   --- Scenario: {scenario['teacher_model']} (Teacher) <- {scenario['student_model']} (Student) ---\")\n",
    "            \n",
    "            set_seed(SEED)\n",
    "            \n",
    "            if scenario['teacher_model'] == MODEL_A:\n",
    "                teacher_data = data_splits['model_a']\n",
    "                student_data = data_splits['model_b']\n",
    "                align_teacher_train = data_splits['alignment']['X_a_train']\n",
    "                align_student_train = data_splits['alignment']['X_b_train']\n",
    "                align_teacher_val = data_splits['alignment']['X_a_val']\n",
    "                align_student_val = data_splits['alignment']['X_b_val']\n",
    "                student_scaler = data_splits['alignment']['scaler_b']\n",
    "            else:\n",
    "                teacher_data = data_splits['model_b']\n",
    "                student_data = data_splits['model_a']\n",
    "                align_teacher_train = data_splits['alignment']['X_b_train']\n",
    "                align_student_train = data_splits['alignment']['X_a_train']\n",
    "                align_teacher_val = data_splits['alignment']['X_b_val']\n",
    "                align_student_val = data_splits['alignment']['X_a_val']\n",
    "                student_scaler = data_splits['alignment']['scaler_a']\n",
    "            # Prepara alignment_data per la funzione\n",
    "            alignment_data = {\n",
    "                'X_teacher_train': align_teacher_train,\n",
    "                'X_teacher_val': align_teacher_val,\n",
    "                'X_student_train': align_student_train,\n",
    "                'X_student_val': align_student_val\n",
    "            }\n",
    "            \n",
    "            res = run_experiment_pipeline_with_autoencoder(\n",
    "                {\"X_train\": teacher_data['X_train'], \"X_test\": teacher_data['X_test']},\n",
    "                {\"y_train\": teacher_data['y_train'], \"y_test\": teacher_data['y_test']},\n",
    "                scenario['teacher_model'],\n",
    "                {\"X_train\": student_data['X_train'], \"X_test\": student_data['X_test']},\n",
    "                {\"y_train\": student_data['y_train'], \"y_test\": student_data['y_test']},\n",
    "                scenario['student_model'],\n",
    "                alignment_data,\n",
    "                layer_type, \"CONFIG1\"\n",
    "            )\n",
    "            scenario_results_map[i].append(res)\n",
    "            \n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"Teacher_{scenario['teacher_model'].replace('.', '_').replace('-', '_')}\"\n",
    "            )\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['student_on_teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"{scenario['student_model'].replace('.', '_').replace('-', '_')}_on_{scenario['teacher_model'].replace('.', '_').replace('-', '_')}\"\n",
    "            )\n",
    "\n",
    "        del data_splits\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"   Memory freed for {layer_type}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in layer {layer_type}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# ============================================\n",
    "# PHASE 3: SALVATAGGIO RISULTATI\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: SALVATAGGIO RISULTATI\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Reconstruct all_results\n",
    "all_results = []\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    all_results.append({\n",
    "        \"scenario\": f\"{scenario['teacher_model']} (teacher) → {scenario['student_model']} (student)\",\n",
    "        \"results\": scenario_results_map[i]\n",
    "    })\n",
    "\n",
    "# Save JSON\n",
    "os.makedirs(\"results_metrics\", exist_ok=True)\n",
    "metrics_file = \"results_metrics/approach2_autoencoder_results.json\"\n",
    "\n",
    "all_results_json = []\n",
    "\n",
    "for scenario_data in all_results:\n",
    "    scenario_results = []\n",
    "    \n",
    "    for r in scenario_data['results']:\n",
    "        ae_t_config = r['autoencoder_teacher']['config']\n",
    "        ae_s_config = r['autoencoder_student']['config']\n",
    "        prober_cfg = r['prober_model']['config']\n",
    "        align_config = r['alignment_model']['config']\n",
    "        \n",
    "        result_entry = {\n",
    "            \"layer_type\": r['type'],\n",
    "            \"teacher_model\": r['teacher_name'],\n",
    "            \"student_model\": r['student_name'],\n",
    "            \"data_info\": {\n",
    "                \"alignment_samples_train\": int(len(alignment_train_local_idx)),\n",
    "                \"alignment_samples_val\": int(len(alignment_val_local_idx)),\n",
    "                \"model_a_train\": int(len(model_a_train_local)),\n",
    "                \"model_a_test\": int(len(model_a_test_local)),\n",
    "                \"model_b_train\": int(len(model_b_train_local)),\n",
    "                \"model_b_test\": int(len(model_b_test_local)),\n",
    "                \"concordant_undersampling_for_alignment\": True,\n",
    "                \"separate_undersampling_per_model\": True\n",
    "            },\n",
    "            \n",
    "            # ==================== TEACHER AUTOENCODER ====================\n",
    "            \"teacher_autoencoder\": {\n",
    "                \"architecture\": {\n",
    "                    \"input_dim\": r['autoencoder_teacher']['input_dim'],\n",
    "                    \"latent_dim\": ae_t_config['latent_dim'],\n",
    "                    \"hidden_dim\": ae_t_config['hidden_dim'],\n",
    "                    \"dropout\": ae_t_config['dropout']\n",
    "                },\n",
    "                \"training_hyperparameters\": {\n",
    "                    \"optimizer\": ae_t_config['optimizer'],\n",
    "                    \"learning_rate\": ae_t_config['learning_rate'],\n",
    "                    \"weight_decay\": ae_t_config['weight_decay'],\n",
    "                    \"batch_size\": ae_t_config['batch_size'],\n",
    "                    \"max_epochs\": ae_t_config['max_epochs'],\n",
    "                    \"scheduler\": ae_t_config['scheduler'],\n",
    "                    \"gradient_clip_max_norm\": ae_t_config['gradient_clip_max_norm'],\n",
    "                    \"early_stopping_patience\": ae_t_config['early_stopping_patience'],\n",
    "                    \"early_stopping_min_delta\": ae_t_config['early_stopping_min_delta'],\n",
    "                    \"loss_function\": ae_t_config['loss_function']\n",
    "                },\n",
    "                \"training_results\": {\n",
    "                    \"best_val_loss\": round(r['autoencoder_teacher']['best_val_loss'], 6),\n",
    "                    \"epochs_trained\": r['autoencoder_teacher']['epochs_trained'],\n",
    "                    \"model_saved_path\": r['autoencoder_teacher']['model_path']\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # ==================== STUDENT AUTOENCODER ====================\n",
    "            \"student_autoencoder\": {\n",
    "                \"architecture\": {\n",
    "                    \"input_dim\": r['autoencoder_student']['input_dim'],\n",
    "                    \"latent_dim\": ae_s_config['latent_dim'],\n",
    "                    \"hidden_dim\": ae_s_config['hidden_dim'],\n",
    "                    \"dropout\": ae_s_config['dropout']\n",
    "                },\n",
    "                \"training_hyperparameters\": {\n",
    "                    \"optimizer\": ae_s_config['optimizer'],\n",
    "                    \"learning_rate\": ae_s_config['learning_rate'],\n",
    "                    \"weight_decay\": ae_s_config['weight_decay'],\n",
    "                    \"batch_size\": ae_s_config['batch_size'],\n",
    "                    \"max_epochs\": ae_s_config['max_epochs'],\n",
    "                    \"scheduler\": ae_s_config['scheduler'],\n",
    "                    \"gradient_clip_max_norm\": ae_s_config['gradient_clip_max_norm'],\n",
    "                    \"early_stopping_patience\": ae_s_config['early_stopping_patience'],\n",
    "                    \"early_stopping_min_delta\": ae_s_config['early_stopping_min_delta'],\n",
    "                    \"loss_function\": ae_s_config['loss_function']\n",
    "                },\n",
    "                \"training_results\": {\n",
    "                    \"best_val_loss\": round(r['autoencoder_student']['best_val_loss'], 6),\n",
    "                    \"epochs_trained\": r['autoencoder_student']['epochs_trained'],\n",
    "                    \"model_saved_path\": r['autoencoder_student']['model_path']\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # ==================== PROBER MODEL ====================\n",
    "            \"prober_model\": {\n",
    "                \"architecture\": {\n",
    "                    \"type\": \"MLPProber\",\n",
    "                    \"input_dim\": r['prober_model']['input_dim'],\n",
    "                    \"hidden_dim\": prober_cfg['hidden_dim'],\n",
    "                    \"dropout\": prober_cfg['dropout']\n",
    "                },\n",
    "                \"training_hyperparameters\": {\n",
    "                    \"optimizer\": prober_cfg['optimizer'],\n",
    "                    \"learning_rate\": prober_cfg['learning_rate'],\n",
    "                    \"weight_decay\": prober_cfg['weight_decay'],\n",
    "                    \"batch_size\": prober_cfg['batch_size'],\n",
    "                    \"max_epochs\": prober_cfg['max_epochs'],\n",
    "                    \"scheduler\": prober_cfg['scheduler'],\n",
    "                    \"gradient_clip_max_norm\": prober_cfg['gradient_clip_max_norm'],\n",
    "                    \"early_stopping_patience\": prober_cfg['early_stopping_patience'],\n",
    "                    \"early_stopping_min_delta\": prober_cfg['early_stopping_min_delta'],\n",
    "                    \"loss_function\": prober_cfg['loss_function'],\n",
    "                    \"use_class_weights\": prober_cfg['use_class_weights']\n",
    "                },\n",
    "                \"training_results\": {\n",
    "                    \"best_val_acc\": round(r['prober_model']['best_val_acc'], 4),\n",
    "                    \"epochs_trained\": r['prober_model']['epochs_trained'],\n",
    "                    \"model_saved_path\": r['prober_model']['model_path']\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # ==================== ALIGNMENT MODEL ====================\n",
    "            \"alignment_model\": {\n",
    "                \"architecture\": {\n",
    "                    \"type\": \"AlignmentNetwork\",\n",
    "                    \"input_dim\": r['alignment_model']['input_dim'],\n",
    "                    \"output_dim\": r['alignment_model']['output_dim'],\n",
    "                    \"hidden_dim\": align_config['hidden_dim'],\n",
    "                    \"dropout\": align_config['dropout']\n",
    "                },\n",
    "                \"training_hyperparameters\": {\n",
    "                    \"optimizer\": align_config['optimizer'],\n",
    "                    \"learning_rate\": align_config['learning_rate'],\n",
    "                    \"weight_decay\": align_config['weight_decay'],\n",
    "                    \"batch_size\": align_config['batch_size'],\n",
    "                    \"max_epochs\": align_config['max_epochs'],\n",
    "                    \"scheduler\": align_config['scheduler'],\n",
    "                    \"gradient_clip_max_norm\": align_config['gradient_clip_max_norm'],\n",
    "                    \"early_stopping_patience\": align_config['early_stopping_patience'],\n",
    "                    \"early_stopping_min_delta\": align_config['early_stopping_min_delta']\n",
    "                },\n",
    "                \"loss_function\": {\n",
    "                    \"type\": \"MixedLoss\",\n",
    "                    \"mse_weight\": align_config['loss_alpha'],\n",
    "                    \"cosine_weight\": align_config['loss_beta']\n",
    "                },\n",
    "                \"training_results\": {\n",
    "                    \"best_val_loss\": round(r['alignment_model']['best_val_loss'], 6),\n",
    "                    \"epochs_trained\": r['alignment_model']['epochs_trained'],\n",
    "                    \"model_saved_path\": r['alignment_model']['model_path']\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            # ==================== PERFORMANCE METRICS ====================\n",
    "            \"metrics\": {\n",
    "                \"teacher\": {\n",
    "                    \"accuracy\": round(r['teacher']['accuracy'], 4),\n",
    "                    \"precision\": round(r['teacher']['precision'], 4),\n",
    "                    \"recall\": round(r['teacher']['recall'], 4),\n",
    "                    \"f1_score\": round(r['teacher']['f1'], 4),\n",
    "                    \"auroc\": round(r['teacher']['auroc'], 4),\n",
    "                    \"confusion_matrix\": {\n",
    "                        \"TN\": int(r['teacher']['confusion_matrix'][0][0]),\n",
    "                        \"FP\": int(r['teacher']['confusion_matrix'][0][1]),\n",
    "                        \"FN\": int(r['teacher']['confusion_matrix'][1][0]),\n",
    "                        \"TP\": int(r['teacher']['confusion_matrix'][1][1])\n",
    "                    }\n",
    "                },\n",
    "                \"student_on_teacher\": {\n",
    "                    \"accuracy\": round(r['student_on_teacher']['accuracy'], 4),\n",
    "                    \"precision\": round(r['student_on_teacher']['precision'], 4),\n",
    "                    \"recall\": round(r['student_on_teacher']['recall'], 4),\n",
    "                    \"f1_score\": round(r['student_on_teacher']['f1'], 4),\n",
    "                    \"auroc\": round(r['student_on_teacher']['auroc'], 4),\n",
    "                    \"confusion_matrix\": {\n",
    "                        \"TN\": int(r['student_on_teacher']['confusion_matrix'][0][0]),\n",
    "                        \"FP\": int(r['student_on_teacher']['confusion_matrix'][0][1]),\n",
    "                        \"FN\": int(r['student_on_teacher']['confusion_matrix'][1][0]),\n",
    "                        \"TP\": int(r['student_on_teacher']['confusion_matrix'][1][1])\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        scenario_results.append(result_entry)\n",
    "    \n",
    "    all_results_json.append({\n",
    "        \"scenario\": scenario_data['scenario'],\n",
    "        \"results\": scenario_results\n",
    "    })\n",
    "\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(all_results_json, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to: {metrics_file}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
