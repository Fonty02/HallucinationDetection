{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033f7dd2",
   "metadata": {},
   "source": [
    "# Approach 3 EtE: Autoencoder-based Alignment with End-to-End Fine-Tuning\n",
    "\n",
    "In this notebook we extend Approach 3 with **End-to-End Fine-Tuning** to improve latent space alignment.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Autoencoder for Teacher**: Learn to compress Teacher activations to latent dimension ($X_T \\to Z_T$)\n",
    "2. **Autoencoder for Student**: Learn to compress Student activations to the same latent dimension ($X_S \\to Z_S$)\n",
    "3. **MLP Prober on Teacher**: Train an MLP classifier on the reduced Teacher space\n",
    "4. **Alignment Network**: Learn to align Student's latent space to Teacher's latent space ($Z_S \\to Z_T$)\n",
    "5. **üÜï End-to-End Fine-Tuning**: Jointly fine-tune Student Encoder + Aligner while keeping Teacher frozen\n",
    "6. **Evaluation**: Test the aligned Student representations on the Teacher's MLP prober\n",
    "\n",
    "## Key Improvement:\n",
    "The End-to-End fine-tuning phase unfreezes the Student Encoder and Aligner, allowing the entire student pipeline to be optimized jointly. The Teacher remains frozen to provide stable target representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733ba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import traceback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "# ==================================================================\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# ==================================================================\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set seeds at import time\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c403d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "HF_DEFAULT_HOME = os.environ.get(\"HF_HOME\", \"~\\\\.cache\\\\huggingface\\\\hub\")\n",
    "\n",
    "# We test the same layers as in the linear approach\n",
    "LAYER_CONFIG = {\n",
    "    \"Qwen2.5-7B\": \n",
    "    {\n",
    "        \"attn\": [15,16,18],\n",
    "        \"mlp\":[16,18,20],\n",
    "        \"hidden\": [18,19,20]\n",
    "    },    \n",
    "    \"Falcon3-7B-Base\": \n",
    "    {\n",
    "        \"attn\": [2,7,12],\n",
    "        \"mlp\":[10,11,12],\n",
    "        \"hidden\": [2,3,19]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================================================================\n",
    "# INCREASED CAPACITY: latent_dim = 2048, hidden_dim = 4096\n",
    "# ==================================================================\n",
    "LATENT_DIM = 128\n",
    "HIDDEN_DIM_AE = 256  # Hidden dimension for Autoencoders\n",
    "HIDDEN_DIM_ALIGN = 256  # Hidden dimension for AlignmentNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef8f91",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6127abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_per_json(model_name, dataset_name):\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\"generations\",\"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    percent_hallucinations = (hallucinations / total) * 100 if total > 0 else 0\n",
    "    allucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'percent_hallucinations': percent_hallucinations,\n",
    "        'hallucinated_items': allucinated_items,\n",
    "        'model_name': model_name,\n",
    "        'dataset_name': dataset_name\n",
    "    }\n",
    "\n",
    "\n",
    "qwen_stats=stats_per_json(\"Qwen2.5-7B\", \"belief_bank\")\n",
    "falcon_stats=stats_per_json(\"Falcon3-7B-Base\", \"belief_bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abef31",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49b09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. Dataset class for Autoencoder Training\n",
    "# ------------------------------------------------------------------\n",
    "class AutoencoderDataset(Dataset):\n",
    "    def __init__(self, X: torch.Tensor):\n",
    "        self.X = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Dataset class for Alignment\n",
    "# ------------------------------------------------------------------\n",
    "class AlignmentDataset(Dataset):\n",
    "    def __init__(self, x_source: torch.Tensor, x_target: torch.Tensor):\n",
    "        self.x_source = x_source\n",
    "        self.x_target = x_target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_source.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_source[idx], self.x_target[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Dataset class for Classification\n",
    "# ------------------------------------------------------------------\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Dataset class for End-to-End Fine-Tuning (raw inputs)\n",
    "# ------------------------------------------------------------------\n",
    "class EndToEndDataset(Dataset):\n",
    "    \"\"\"Dataset for End-to-End fine-tuning: pairs raw student activations with teacher targets\"\"\"\n",
    "    def __init__(self, X_student_raw: torch.Tensor, X_teacher_raw: torch.Tensor):\n",
    "        self.X_student = X_student_raw\n",
    "        self.X_teacher = X_teacher_raw\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X_student.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_student[idx], self.X_teacher[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Autoencoder for Dimensionality Reduction (hidden_dim = 4096)\n",
    "# ------------------------------------------------------------------\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 4096, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Autoencoder for dimensionality reduction.\n",
    "        Architecture: Input -> Hidden -> Latent -> Hidden -> Output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent space\"\"\"\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent representation back to original space\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, z\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. AlignmentNetwork (hidden_dim = 4096)\n",
    "# ------------------------------------------------------------------\n",
    "class AlignmentNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 4096, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        Alignment network to map Student latent space to Teacher latent space.\n",
    "        Since both spaces now have the same dimension, this is a refinement network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Since input_dim == output_dim (both are LATENT_DIM), we use identity + residual\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "        \n",
    "        # Zero-init last layer for residual learning\n",
    "        nn.init.zeros_(self.net[-1].weight)\n",
    "        if self.net[-1].bias is not None:\n",
    "            nn.init.zeros_(self.net[-1].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection: identity + learned refinement\n",
    "        return x + self.net(x)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7. MLP Prober (works on reduced latent space)\n",
    "# ------------------------------------------------------------------\n",
    "class MLPProber(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 512, dropout: float = 0.3):\n",
    "        \"\"\"\n",
    "        MLP classifier for hallucination detection on reduced latent space.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Returns predicted class labels (0 or 1)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return (torch.sigmoid(logits) > 0.5).long()\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"Returns probability of class 1\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return torch.sigmoid(logits)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8. MixedLoss for Alignment\n",
    "# ------------------------------------------------------------------\n",
    "class MixedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight for MSE\n",
    "        self.beta = beta    # Weight for Cosine\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # 1. MSE Loss (Magnitude and exact position)\n",
    "        loss_mse = self.mse(pred, target)\n",
    "        \n",
    "        # 2. Cosine Loss (Direction/Angle)\n",
    "        cosine_sim = F.cosine_similarity(pred, target, dim=1).mean()\n",
    "        loss_cosine = 1 - cosine_sim\n",
    "        \n",
    "        # Combined loss\n",
    "        return self.alpha * loss_mse + self.beta * loss_cosine\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 9. EndToEndLoss for Fine-Tuning (Cosine-focused + small MSE)\n",
    "# ------------------------------------------------------------------\n",
    "class EndToEndLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss for End-to-End fine-tuning.\n",
    "    Prioritizes cosine similarity (direction) with a small MSE weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, cosine_weight=0.8, mse_weight=0.2):\n",
    "        super().__init__()\n",
    "        self.cosine_weight = cosine_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Cosine Loss (Direction/Angle) - PRIMARY\n",
    "        cosine_sim = F.cosine_similarity(pred, target, dim=1).mean()\n",
    "        loss_cosine = 1 - cosine_sim\n",
    "        \n",
    "        # MSE Loss (Magnitude) - SECONDARY\n",
    "        loss_mse = self.mse(pred, target)\n",
    "        \n",
    "        return self.cosine_weight * loss_cosine + self.mse_weight * loss_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff65ef9",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f050692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_layers(model_name, dataset_name, layer_indices, type_layer, stats, train_indices, test_indices):\n",
    "    \"\"\"\n",
    "    Standard loading in RAM (without memmap).\n",
    "    \"\"\"\n",
    "    print(f\" Loading IN-MEMORY {model_name} [{type_layer}]: layers {layer_indices}...\")\n",
    "\n",
    "    total_samples = stats['total']\n",
    "    hallucinated_set = set(stats['hallucinated_items'])\n",
    "\n",
    "    # Label\n",
    "    y_full = np.zeros(total_samples, dtype=np.int8)\n",
    "    y_full[list(hallucinated_set)] = 1\n",
    "    y_train = y_full[train_indices]\n",
    "    y_test  = y_full[test_indices]\n",
    "\n",
    "    # Load and concatenate\n",
    "    all_features = []\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\n",
    "                                 \"activation_\"+type_layer, f\"layer{layer_idx}_activations.pt\")\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\" Warning: Layer {layer_idx} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"  Loading layer {layer_idx}...\", end=\" \")\n",
    "        acts = torch.load(file_path, map_location='cpu')\n",
    "        \n",
    "        if acts.shape[0] > total_samples:\n",
    "            acts = acts[:total_samples]\n",
    "\n",
    "        # Convert to numpy\n",
    "        if isinstance(acts, torch.Tensor):\n",
    "            X_layer = acts.float().numpy() \n",
    "        else:\n",
    "            X_layer = acts.astype(np.float32)\n",
    "\n",
    "        # Flatten\n",
    "        if X_layer.ndim > 2:\n",
    "            X_layer = X_layer.reshape(X_layer.shape[0], -1)\n",
    "            \n",
    "        all_features.append(X_layer)\n",
    "        print(f\"done ({X_layer.shape})\")\n",
    "        \n",
    "        del acts\n",
    "        gc.collect()\n",
    "\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"No valid layers found for {model_name}\")\n",
    "\n",
    "    print(\" Concatenating layers...\")\n",
    "    X_full = np.concatenate(all_features, axis=1)\n",
    "    \n",
    "    X_train = X_full[train_indices]\n",
    "    X_test  = X_full[test_indices]\n",
    "    \n",
    "    print(f\" Completed! Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Helper function for reproducible DataLoader\n",
    "# ==================================================================\n",
    "def get_generator(seed=SEED):\n",
    "    \"\"\"Create a reproducible generator for DataLoader\"\"\"\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return g\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Train Autoencoder\n",
    "# ==================================================================\n",
    "def train_autoencoder(X_train, X_val, input_dim, latent_dim, device, model_name,\n",
    "                      hidden_dim=4096, dropout=0.2, epochs=300, patience=30, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Train autoencoder for dimensionality reduction with early stopping.\n",
    "    \"\"\"\n",
    "    print(f\"   Training Autoencoder for {model_name} ({input_dim} -> {latent_dim})...\")\n",
    "    \n",
    "    autoencoder = Autoencoder(\n",
    "        input_dim=input_dim,\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(autoencoder.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Create dataloaders with reproducible generator\n",
    "    train_dataset = AutoencoderDataset(X_train)\n",
    "    val_dataset = AutoencoderDataset(X_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, generator=get_generator())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        autoencoder.train()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            X_recon, _ = autoencoder(X_batch)\n",
    "            loss = criterion(X_recon, X_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(autoencoder.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        autoencoder.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch in val_loader:\n",
    "                X_recon, _ = autoencoder(X_batch)\n",
    "                loss = criterion(X_recon, X_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 30 == 0:\n",
    "            print(f\"     Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = autoencoder.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"     Early stopping at epoch {epoch+1}. Best Val Loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        autoencoder.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"   ‚úì Autoencoder trained. Final Val Loss: {best_val_loss:.6f}\")\n",
    "    return autoencoder, best_val_loss\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Train MLP Prober\n",
    "# ==================================================================\n",
    "def train_mlp_prober(X_train, y_train, X_val, y_val, input_dim, device, \n",
    "                     hidden_dim=512, dropout=0.3, epochs=200, patience=30, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Train MLP prober with early stopping based on validation accuracy.\n",
    "    \"\"\"\n",
    "    prober = MLPProber(input_dim=input_dim, hidden_dim=hidden_dim, dropout=dropout).to(device)\n",
    "    \n",
    "    # Compute class weights for imbalanced data\n",
    "    n_pos = y_train.sum()\n",
    "    n_neg = len(y_train) - n_pos\n",
    "    pos_weight = torch.tensor([n_neg / n_pos]).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.AdamW(prober.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Create dataloaders with reproducible generator\n",
    "    train_dataset = ClassificationDataset(X_train, y_train)\n",
    "    val_dataset = ClassificationDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, generator=get_generator())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        prober.train()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = prober(X_batch)\n",
    "            loss = criterion(logits, y_batch.float())\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(prober.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        prober.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                preds = prober.predict(X_batch)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        val_f1 = f1_score(all_labels, all_preds)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"     Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early Stopping based on accuracy\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = prober.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"     Early stopping at epoch {epoch+1}. Best Val Acc: {best_val_acc:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        prober.load_state_dict(best_model_state)\n",
    "    \n",
    "    return prober, best_val_acc\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Train Alignment Network\n",
    "# ==================================================================\n",
    "def train_alignment_network(X_source_train, X_target_train, X_source_val, X_target_val, \n",
    "                            latent_dim, device, hidden_dim=4096, epochs=500, patience=50, min_delta=1e-4):\n",
    "    \"\"\"\n",
    "    Train alignment network to map student latent space to teacher latent space.\n",
    "    \"\"\"\n",
    "    print(\"   Training Alignment Network...\")\n",
    "    \n",
    "    aligner = AlignmentNetwork(\n",
    "        input_dim=latent_dim,\n",
    "        output_dim=latent_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = MixedLoss(alpha=0.5, beta=0.5)\n",
    "    optimizer = optim.AdamW(aligner.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Create dataloaders with reproducible generator\n",
    "    train_dataset = AlignmentDataset(X_source_train, X_target_train)\n",
    "    val_dataset = AlignmentDataset(X_source_val, X_target_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, generator=get_generator())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        aligner.train()\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            projected = aligner(data)\n",
    "            loss = criterion(projected, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(aligner.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        aligner.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                projected = aligner(data)\n",
    "                loss = criterion(projected, target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"     Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = aligner.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"     Early stopping at epoch {epoch+1}. Best Val Loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        aligner.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"   ‚úì Alignment Network trained. Final Val Loss: {best_val_loss:.6f}\")\n",
    "    return aligner, best_val_loss\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# üÜï END-TO-END FINE-TUNING FUNCTION\n",
    "# ==================================================================\n",
    "def fine_tune_end_to_end(ae_student, ae_teacher, aligner, \n",
    "                          X_student_train, X_teacher_train,\n",
    "                          X_student_val, X_teacher_val,\n",
    "                          device, epochs=200, patience=40, min_delta=1e-5,\n",
    "                          lr=5e-5, cosine_weight=0.8, mse_weight=0.2):\n",
    "    \"\"\"\n",
    "    End-to-End Fine-Tuning Phase:\n",
    "    \n",
    "    - FREEZE: Teacher Encoder (provides stable target Z_T)\n",
    "    - UNFREEZE: Student Encoder + Aligner (learn to produce Z_S that aligns with Z_T)\n",
    "    \n",
    "    Pipeline: X_S -> EncoderStudent -> Z_S -> Aligner -> Z_S_aligned\n",
    "    Target: Z_T = EncoderTeacher(X_T)\n",
    "    \n",
    "    Loss: Cosine similarity (primary) + MSE (secondary)\n",
    "    \n",
    "    Args:\n",
    "        ae_student: Pre-trained Student Autoencoder\n",
    "        ae_teacher: Pre-trained Teacher Autoencoder\n",
    "        aligner: Pre-trained Alignment Network\n",
    "        X_student_train: Raw student activations for training (numpy)\n",
    "        X_teacher_train: Raw teacher activations for training (numpy)\n",
    "        X_student_val: Raw student activations for validation (numpy)\n",
    "        X_teacher_val: Raw teacher activations for validation (numpy)\n",
    "        device: torch device\n",
    "        epochs: Maximum number of epochs\n",
    "        patience: Early stopping patience\n",
    "        min_delta: Minimum improvement for early stopping\n",
    "        lr: Learning rate (lower than initial training)\n",
    "        cosine_weight: Weight for cosine loss\n",
    "        mse_weight: Weight for MSE loss\n",
    "    \n",
    "    Returns:\n",
    "        ae_student: Fine-tuned Student Autoencoder\n",
    "        aligner: Fine-tuned Alignment Network\n",
    "        best_val_loss: Best validation loss achieved\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîß END-TO-END FINE-TUNING PHASE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"   Freezing: Teacher Encoder\")\n",
    "    print(\"   Unfreezing: Student Encoder + Aligner\")\n",
    "    print(f\"   Loss: {cosine_weight}*Cosine + {mse_weight}*MSE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 1. FREEZE Teacher Encoder completely\n",
    "    # --------------------------------------------------\n",
    "    ae_teacher.eval()\n",
    "    for param in ae_teacher.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"   ‚úì Teacher Encoder FROZEN\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 2. UNFREEZE Student Encoder (only encoder, not decoder)\n",
    "    # --------------------------------------------------\n",
    "    ae_student.train()\n",
    "    for param in ae_student.encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    # Keep decoder frozen (we don't need reconstruction anymore)\n",
    "    for param in ae_student.decoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"   ‚úì Student Encoder UNFROZEN\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 3. UNFREEZE Aligner\n",
    "    # --------------------------------------------------\n",
    "    aligner.train()\n",
    "    for param in aligner.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"   ‚úì Aligner UNFROZEN\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 4. Setup Optimizer (only trainable parameters)\n",
    "    # --------------------------------------------------\n",
    "    trainable_params = list(ae_student.encoder.parameters()) + list(aligner.parameters())\n",
    "    optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Custom loss: Cosine-focused + small MSE\n",
    "    criterion = EndToEndLoss(cosine_weight=cosine_weight, mse_weight=mse_weight)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 5. Create DataLoaders\n",
    "    # --------------------------------------------------\n",
    "    X_S_train_t = torch.from_numpy(X_student_train).float().to(device)\n",
    "    X_T_train_t = torch.from_numpy(X_teacher_train).float().to(device)\n",
    "    X_S_val_t = torch.from_numpy(X_student_val).float().to(device)\n",
    "    X_T_val_t = torch.from_numpy(X_teacher_val).float().to(device)\n",
    "    \n",
    "    train_dataset = EndToEndDataset(X_S_train_t, X_T_train_t)\n",
    "    val_dataset = EndToEndDataset(X_S_val_t, X_T_val_t)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, \n",
    "                               num_workers=0, generator=get_generator())\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 6. Pre-compute Teacher Latent Representations (frozen)\n",
    "    # --------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        Z_T_train = ae_teacher.encode(X_T_train_t)\n",
    "        Z_T_val = ae_teacher.encode(X_T_val_t)\n",
    "    \n",
    "    # Create target datasets with pre-computed Z_T\n",
    "    class EndToEndTargetDataset(Dataset):\n",
    "        def __init__(self, X_student, Z_teacher):\n",
    "            self.X_student = X_student\n",
    "            self.Z_teacher = Z_teacher\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.X_student.shape[0]\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return self.X_student[idx], self.Z_teacher[idx]\n",
    "    \n",
    "    train_dataset_target = EndToEndTargetDataset(X_S_train_t, Z_T_train)\n",
    "    val_dataset_target = EndToEndTargetDataset(X_S_val_t, Z_T_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset_target, batch_size=32, shuffle=True, \n",
    "                               num_workers=0, generator=get_generator())\n",
    "    val_loader = DataLoader(val_dataset_target, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 7. Training Loop\n",
    "    # --------------------------------------------------\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_student_state = None\n",
    "    best_aligner_state = None\n",
    "    \n",
    "    print(\"\\n   Starting End-to-End Fine-Tuning...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        ae_student.encoder.train()\n",
    "        aligner.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_cosine = 0.0\n",
    "        \n",
    "        for X_S_batch, Z_T_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: X_S -> EncoderStudent -> Z_S -> Aligner -> Z_S_aligned\n",
    "            Z_S = ae_student.encode(X_S_batch)\n",
    "            Z_S_aligned = aligner(Z_S)\n",
    "            \n",
    "            # Compute loss against frozen Teacher target\n",
    "            loss = criterion(Z_S_aligned, Z_T_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Track cosine similarity for monitoring\n",
    "            with torch.no_grad():\n",
    "                cos_sim = F.cosine_similarity(Z_S_aligned, Z_T_batch, dim=1).mean()\n",
    "                epoch_cosine += cos_sim.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        avg_train_cosine = epoch_cosine / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        ae_student.encoder.eval()\n",
    "        aligner.eval()\n",
    "        val_loss = 0.0\n",
    "        val_cosine = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_S_batch, Z_T_batch in val_loader:\n",
    "                Z_S = ae_student.encode(X_S_batch)\n",
    "                Z_S_aligned = aligner(Z_S)\n",
    "                loss = criterion(Z_S_aligned, Z_T_batch)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                cos_sim = F.cosine_similarity(Z_S_aligned, Z_T_batch, dim=1).mean()\n",
    "                val_cosine += cos_sim.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_cosine = val_cosine / len(val_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"     Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | \"\n",
    "                  f\"Train Cos: {avg_train_cosine:.4f} | Val Cos: {avg_val_cosine:.4f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_student_state = ae_student.state_dict().copy()\n",
    "            best_aligner_state = aligner.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"     Early stopping at epoch {epoch+1}. Best Val Loss: {best_val_loss:.6f}\")\n",
    "            break\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 8. Load Best Model States\n",
    "    # --------------------------------------------------\n",
    "    if best_student_state is not None:\n",
    "        ae_student.load_state_dict(best_student_state)\n",
    "    if best_aligner_state is not None:\n",
    "        aligner.load_state_dict(best_aligner_state)\n",
    "    \n",
    "    print(f\"\\n   ‚úì End-to-End Fine-Tuning completed!\")\n",
    "    print(f\"   ‚úì Best Val Loss: {best_val_loss:.6f}\")\n",
    "    print(f\"   ‚úì Final Val Cosine Similarity: {avg_val_cosine:.4f}\")\n",
    "    \n",
    "    return ae_student, aligner, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0113b",
   "metadata": {},
   "source": [
    "### Main Experiment Pipeline (with End-to-End Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad33b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Main Experiment Pipeline with End-to-End Fine-Tuning\n",
    "# ==================================================================\n",
    "def run_experiment_pipeline_with_autoencoder(X_teacher, y_teacher, teacher_name,\n",
    "                                              X_student, y_student, student_name, \n",
    "                                              layer_type, config_name, latent_dim=LATENT_DIM,\n",
    "                                              hidden_dim_ae=HIDDEN_DIM_AE, hidden_dim_align=HIDDEN_DIM_ALIGN,\n",
    "                                              patience=50, min_delta=1e-4):\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT: {layer_type.upper()} ‚Üí {teacher_name} ‚Üê {student_name}\")\n",
    "    print(f\"Using Autoencoder with latent_dim={latent_dim}, hidden_dim={hidden_dim_ae}\")\n",
    "    print(f\"+ End-to-End Fine-Tuning Phase\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Data already split (numpy for sklearn)\n",
    "    X_A_train_full, X_A_test = X_teacher['X_train'], X_teacher['X_test']\n",
    "    y_A_train_full, y_A_test = y_teacher['y_train'], y_teacher['y_test']\n",
    "    X_B_train_full, X_B_test = X_student['X_train'], X_student['X_test']\n",
    "    y_B_train_full, y_B_test = y_student['y_train'], y_student['y_test']\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1. Train Autoencoder for Teacher\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n1. Training Autoencoder for TEACHER...\")\n",
    "    \n",
    "    # Split for validation\n",
    "    num_train = len(X_A_train_full)\n",
    "    indices = np.arange(num_train)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    ae_val_size = int(num_train * 0.15)\n",
    "    ae_train_idx = indices[ae_val_size:]\n",
    "    ae_val_idx = indices[:ae_val_size]\n",
    "    \n",
    "    X_A_ae_train = torch.from_numpy(X_A_train_full[ae_train_idx]).float().to(device)\n",
    "    X_A_ae_val = torch.from_numpy(X_A_train_full[ae_val_idx]).float().to(device)\n",
    "    \n",
    "    ae_teacher, ae_teacher_loss = train_autoencoder(\n",
    "        X_A_ae_train, X_A_ae_val,\n",
    "        input_dim=X_A_train_full.shape[1],\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_dim=hidden_dim_ae,\n",
    "        device=device,\n",
    "        model_name=teacher_name,\n",
    "        epochs=300,\n",
    "        patience=30\n",
    "    )\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 2. Train Autoencoder for Student\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n2. Training Autoencoder for STUDENT...\")\n",
    "    \n",
    "    # Use same split indices for consistency\n",
    "    X_B_ae_train = torch.from_numpy(X_B_train_full[ae_train_idx]).float().to(device)\n",
    "    X_B_ae_val = torch.from_numpy(X_B_train_full[ae_val_idx]).float().to(device)\n",
    "    \n",
    "    ae_student, ae_student_loss = train_autoencoder(\n",
    "        X_B_ae_train, X_B_ae_val,\n",
    "        input_dim=X_B_train_full.shape[1],\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_dim=hidden_dim_ae,\n",
    "        device=device,\n",
    "        model_name=student_name,\n",
    "        epochs=300,\n",
    "        patience=30\n",
    "    )\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 3. Encode all data to latent space\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n3. Encoding data to latent space...\")\n",
    "    \n",
    "    ae_teacher.eval()\n",
    "    ae_student.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Teacher encodings\n",
    "        X_A_train_full_t = torch.from_numpy(X_A_train_full).float().to(device)\n",
    "        X_A_test_t = torch.from_numpy(X_A_test).float().to(device)\n",
    "        Z_A_train = ae_teacher.encode(X_A_train_full_t)\n",
    "        Z_A_test = ae_teacher.encode(X_A_test_t)\n",
    "        \n",
    "        # Student encodings\n",
    "        X_B_train_full_t = torch.from_numpy(X_B_train_full).float().to(device)\n",
    "        X_B_test_t = torch.from_numpy(X_B_test).float().to(device)\n",
    "        Z_B_train = ae_student.encode(X_B_train_full_t)\n",
    "        Z_B_test = ae_student.encode(X_B_test_t)\n",
    "    \n",
    "    print(f\"   Teacher latent shape: {Z_A_train.shape}\")\n",
    "    print(f\"   Student latent shape: {Z_B_train.shape}\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 4. Train MLP Prober on Teacher's Latent Space\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n4. Training MLP Prober on Teacher's latent space...\")\n",
    "    \n",
    "    # Validation split for prober\n",
    "    prober_val_size = int(num_train * 0.15)\n",
    "    prober_train_idx = indices[prober_val_size:]\n",
    "    prober_val_idx = indices[:prober_val_size]\n",
    "    \n",
    "    Z_A_prober_train = Z_A_train[prober_train_idx]\n",
    "    y_A_prober_train = torch.from_numpy(y_A_train_full[prober_train_idx]).long().to(device)\n",
    "    Z_A_prober_val = Z_A_train[prober_val_idx]\n",
    "    y_A_prober_val = torch.from_numpy(y_A_train_full[prober_val_idx]).long().to(device)\n",
    "    \n",
    "    probe_teacher, best_prober_acc = train_mlp_prober(\n",
    "        Z_A_prober_train, y_A_prober_train,\n",
    "        Z_A_prober_val, y_A_prober_val,\n",
    "        input_dim=latent_dim,\n",
    "        device=device,\n",
    "        epochs=200,\n",
    "        patience=30\n",
    "    )\n",
    "    print(f\"   Best prober validation Acc: {best_prober_acc:.4f}\")\n",
    "    \n",
    "    # --- Teacher Metrics ---\n",
    "    probe_teacher.eval()\n",
    "    y_pred_teacher = probe_teacher.predict(Z_A_test).cpu().numpy()\n",
    "    \n",
    "    cm_teacher = confusion_matrix(y_A_test, y_pred_teacher)\n",
    "    acc_teacher = accuracy_score(y_A_test, y_pred_teacher)\n",
    "    prec_teacher = precision_score(y_A_test, y_pred_teacher)\n",
    "    rec_teacher = recall_score(y_A_test, y_pred_teacher)\n",
    "    f1_teacher = f1_score(y_A_test, y_pred_teacher)\n",
    "    print(f\"   Teacher Test Acc: {acc_teacher:.4f}, F1: {f1_teacher:.4f}\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 5. Train Alignment Network (Student Latent ‚Üí Teacher Latent)\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n5. Training Alignment Network (Student ‚Üí Teacher latent space)...\")\n",
    "    \n",
    "    # Validation split for alignment\n",
    "    align_val_size = int(num_train * 0.1)\n",
    "    align_train_idx = indices[align_val_size:]\n",
    "    align_val_idx = indices[:align_val_size]\n",
    "    \n",
    "    Z_B_align_train = Z_B_train[align_train_idx]\n",
    "    Z_A_align_train = Z_A_train[align_train_idx]\n",
    "    Z_B_align_val = Z_B_train[align_val_idx]\n",
    "    Z_A_align_val = Z_A_train[align_val_idx]\n",
    "    \n",
    "    aligner, align_loss = train_alignment_network(\n",
    "        Z_B_align_train, Z_A_align_train,\n",
    "        Z_B_align_val, Z_A_align_val,\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_dim=hidden_dim_align,\n",
    "        device=device,\n",
    "        epochs=500,\n",
    "        patience=50\n",
    "    )\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # üÜï 6. END-TO-END FINE-TUNING\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n6. END-TO-END FINE-TUNING (Student Encoder + Aligner)...\")\n",
    "    \n",
    "    # Use training data for fine-tuning\n",
    "    ete_val_size = int(num_train * 0.15)\n",
    "    ete_train_idx = indices[ete_val_size:]\n",
    "    ete_val_idx = indices[:ete_val_size]\n",
    "    \n",
    "    X_B_ete_train = X_B_train_full[ete_train_idx]\n",
    "    X_A_ete_train = X_A_train_full[ete_train_idx]\n",
    "    X_B_ete_val = X_B_train_full[ete_val_idx]\n",
    "    X_A_ete_val = X_A_train_full[ete_val_idx]\n",
    "    \n",
    "    ae_student, aligner, ete_loss = fine_tune_end_to_end(\n",
    "        ae_student=ae_student,\n",
    "        ae_teacher=ae_teacher,\n",
    "        aligner=aligner,\n",
    "        X_student_train=X_B_ete_train,\n",
    "        X_teacher_train=X_A_ete_train,\n",
    "        X_student_val=X_B_ete_val,\n",
    "        X_teacher_val=X_A_ete_val,\n",
    "        device=device,\n",
    "        epochs=200,\n",
    "        patience=40,\n",
    "        lr=5e-5,\n",
    "        cosine_weight=0.8,\n",
    "        mse_weight=0.2\n",
    "    )\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 7. Save Models\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n7. Saving models...\")\n",
    "    \n",
    "    model_save_dir = os.path.join(\"models\", layer_type)\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save Teacher Autoencoder\n",
    "    ae_teacher_filename = os.path.join(model_save_dir, f\"{config_name}_autoencoder_{teacher_name}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': ae_teacher.state_dict(),\n",
    "        'input_dim': X_A_train_full.shape[1],\n",
    "        'latent_dim': latent_dim,\n",
    "        'hidden_dim': hidden_dim_ae,\n",
    "        'best_val_loss': ae_teacher_loss,\n",
    "        'model_name': teacher_name,\n",
    "    }, ae_teacher_filename)\n",
    "    print(f\"   ‚úì Teacher Autoencoder saved: {ae_teacher_filename}\")\n",
    "    \n",
    "    # Save Student Autoencoder (Fine-Tuned)\n",
    "    ae_student_filename = os.path.join(model_save_dir, f\"{config_name}_autoencoder_{student_name}_finetuned.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': ae_student.state_dict(),\n",
    "        'input_dim': X_B_train_full.shape[1],\n",
    "        'latent_dim': latent_dim,\n",
    "        'hidden_dim': hidden_dim_ae,\n",
    "        'best_val_loss': ae_student_loss,\n",
    "        'ete_loss': ete_loss,\n",
    "        'model_name': student_name,\n",
    "        'fine_tuned': True,\n",
    "    }, ae_student_filename)\n",
    "    print(f\"   ‚úì Student Autoencoder (Fine-Tuned) saved: {ae_student_filename}\")\n",
    "    \n",
    "    # Save MLP Prober\n",
    "    prober_filename = os.path.join(model_save_dir, f\"{config_name}_mlp_prober_{teacher_name}.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': probe_teacher.state_dict(),\n",
    "        'input_dim': latent_dim,\n",
    "        'hidden_dim': 512,\n",
    "        'dropout': 0.3,\n",
    "        'teacher_model': teacher_name,\n",
    "    }, prober_filename)\n",
    "    print(f\"   ‚úì MLP Prober saved: {prober_filename}\")\n",
    "    \n",
    "    # Save Alignment Network (Fine-Tuned)\n",
    "    aligner_filename = os.path.join(model_save_dir, f\"{config_name}_aligner_{student_name}_to_{teacher_name}_finetuned.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': aligner.state_dict(),\n",
    "        'input_dim': latent_dim,\n",
    "        'output_dim': latent_dim,\n",
    "        'hidden_dim': hidden_dim_align,\n",
    "        'best_val_loss': align_loss,\n",
    "        'ete_loss': ete_loss,\n",
    "        'student_model': student_name,\n",
    "        'teacher_model': teacher_name,\n",
    "        'fine_tuned': True,\n",
    "    }, aligner_filename)\n",
    "    print(f\"   ‚úì Alignment Network (Fine-Tuned) saved: {aligner_filename}\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 8. Evaluation: Student aligned ‚Üí Teacher MLP prober\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\n8. Projecting student test set & evaluating (after End-to-End Fine-Tuning)...\")\n",
    "    \n",
    "    # Re-encode student test data with fine-tuned encoder\n",
    "    ae_student.eval()\n",
    "    aligner.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Z_B_test_finetuned = ae_student.encode(X_B_test_t)\n",
    "        Z_B_aligned = aligner(Z_B_test_finetuned)\n",
    "    \n",
    "    y_pred_cross = probe_teacher.predict(Z_B_aligned).cpu().numpy()\n",
    "    \n",
    "    # --- Cross-Model Metrics ---\n",
    "    cm_cross = confusion_matrix(y_B_test, y_pred_cross)\n",
    "    acc_cross = accuracy_score(y_B_test, y_pred_cross)\n",
    "    prec_cross = precision_score(y_B_test, y_pred_cross)\n",
    "    rec_cross = recall_score(y_B_test, y_pred_cross)\n",
    "    f1_cross = f1_score(y_B_test, y_pred_cross)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FINAL RESULT (with End-to-End Fine-Tuning):\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"   Teacher Acc          : {acc_teacher:.4f}, F1: {f1_teacher:.4f}\")\n",
    "    print(f\"   Student ‚Üí Teacher Acc: {acc_cross:.4f}, F1: {f1_cross:.4f}\")\n",
    "    print(f\"   Transfer gap (Acc)   : {acc_teacher - acc_cross:.4f}\")\n",
    "    print(f\"   Transfer gap (F1)    : {f1_teacher - f1_cross:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"type\": layer_type,\n",
    "        \"teacher_name\": teacher_name,\n",
    "        \"student_name\": student_name,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"hidden_dim_ae\": hidden_dim_ae,\n",
    "        \"hidden_dim_align\": hidden_dim_align,\n",
    "        \"autoencoder_teacher_loss\": ae_teacher_loss,\n",
    "        \"autoencoder_student_loss\": ae_student_loss,\n",
    "        \"alignment_loss\": align_loss,\n",
    "        \"end_to_end_loss\": ete_loss,\n",
    "        \"teacher\": {\n",
    "            \"accuracy\": acc_teacher,\n",
    "            \"precision\": prec_teacher,\n",
    "            \"recall\": rec_teacher,\n",
    "            \"f1\": f1_teacher,\n",
    "            \"confusion_matrix\": cm_teacher.tolist()\n",
    "        },\n",
    "        \"student_on_teacher\": {\n",
    "            \"accuracy\": acc_cross,\n",
    "            \"precision\": prec_cross,\n",
    "            \"recall\": rec_cross,\n",
    "            \"f1\": f1_cross,\n",
    "            \"confusion_matrix\": cm_cross.tolist()\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, layer_type, model_name=\"\", save_dir=\"confusion_matrices\"):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix as image.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,\n",
    "                xticklabels=['Non-Hallucinated', 'Hallucinated'],\n",
    "                yticklabels=['Non-Hallucinated', 'Hallucinated'])\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    title = f'Confusion Matrix - {layer_type.upper()} Layers'\n",
    "    if model_name:\n",
    "        title += f' ({model_name})'\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = os.path.join(save_dir, f'confusion_matrix_{layer_type}_{model_name}.png' if model_name else f'confusion_matrix_{layer_type}.png')\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"   ‚úì Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bcd6f6",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a9306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: PRE-LOADING AND SPLITTING DATA (same shuffled indices for ALL layer types)\n",
      "================================================================================\n",
      "\n",
      "Train/Test split: 19191/8225 samples\n",
      "Using SEED=42 for reproducibility\n",
      "Using LATENT_DIM=128, HIDDEN_DIM_AE=256, HIDDEN_DIM_ALIGN=256\n",
      "\n",
      "========================================\n",
      "PROCESSING LAYER TYPE: ATTN\n",
      "========================================\n",
      " Loading IN-MEMORY Qwen2.5-7B [attn]: layers [15, 16, 18]...\n",
      "  Loading layer 15... done ((27416, 3584))\n",
      "  Loading layer 16... done ((27416, 3584))\n",
      "  Loading layer 18... done ((27416, 3584))\n",
      " Concatenating layers...\n",
      " Completed! Train: (19191, 10752), Test: (8225, 10752)\n",
      " Loading IN-MEMORY Falcon3-7B-Base [attn]: layers [2, 7, 12]...\n",
      "  Loading layer 2... done ((27416, 3072))\n",
      "  Loading layer 7... done ((27416, 3072))\n",
      "  Loading layer 12... done ((27416, 3072))\n",
      " Concatenating layers...\n",
      " Completed! Train: (19191, 9216), Test: (8225, 9216)\n",
      "   Normalizing data...\n",
      "\n",
      "   --- Scenario: Qwen2.5-7B (Teacher) <- Falcon3-7B-Base (Student) ---\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT: ATTN ‚Üí Qwen2.5-7B ‚Üê Falcon3-7B-Base\n",
      "Using Autoencoder with latent_dim=128, hidden_dim=256\n",
      "+ End-to-End Fine-Tuning Phase\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "\n",
      "1. Training Autoencoder for TEACHER...\n",
      "   Training Autoencoder for Qwen2.5-7B (10752 -> 128)...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: PRE-LOADING AND SPLITTING DATA (same shuffled indices for ALL layer types)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Reset seeds for reproducibility\n",
    "set_seed(SEED)\n",
    "\n",
    "n_samples = qwen_stats['total'] \n",
    "rng = np.random.RandomState(SEED)  # Use global SEED constant\n",
    "shuffled_indices = rng.permutation(n_samples)\n",
    "split_idx = int(0.7 * n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:split_idx]\n",
    "test_indices = shuffled_indices[split_idx:]\n",
    "\n",
    "print(f\"Train/Test split: {len(train_indices)}/{len(test_indices)} samples\")\n",
    "print(f\"Using SEED={SEED} for reproducibility\")\n",
    "print(f\"Using LATENT_DIM={LATENT_DIM}, HIDDEN_DIM_AE={HIDDEN_DIM_AE}, HIDDEN_DIM_ALIGN={HIDDEN_DIM_ALIGN}\")\n",
    "\n",
    "# Define experiment scenarios\n",
    "scenarios = [\n",
    "    {\"teacher_model\": \"Qwen2.5-7B\", \"student_model\": \"Falcon3-7B-Base\"},\n",
    "    {\"teacher_model\": \"Falcon3-7B-Base\", \"student_model\": \"Qwen2.5-7B\"}\n",
    "]\n",
    "\n",
    "# Structure to collect results maintaining scenario order\n",
    "scenario_results_map = {0: [], 1: []}\n",
    "\n",
    "# Loop over layer types (Load -> Execute -> Free Memory)\n",
    "for layer_type in ['attn', 'mlp', 'hidden']:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"PROCESSING LAYER TYPE: {layer_type.upper()}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    try:\n",
    "        # 1. STANDARD LOADING AND SPLITTING\n",
    "        X_qwen_train, X_qwen_test, y_qwen_train, y_qwen_test = load_and_split_layers(\n",
    "            \"Qwen2.5-7B\", \"belief_bank\", \n",
    "            LAYER_CONFIG[\"Qwen2.5-7B\"][layer_type], \n",
    "            layer_type, qwen_stats,\n",
    "            train_indices, test_indices\n",
    "        )\n",
    "\n",
    "        X_falcon_train, X_falcon_test, y_falcon_train, y_falcon_test = load_and_split_layers(\n",
    "            \"Falcon3-7B-Base\", \"belief_bank\", \n",
    "            LAYER_CONFIG[\"Falcon3-7B-Base\"][layer_type], \n",
    "            layer_type, falcon_stats,\n",
    "            train_indices, test_indices\n",
    "        )\n",
    "        \n",
    "        # 2. SCALING (explicit cast to float32 to save memory)\n",
    "        print(\"   Normalizing data...\")\n",
    "        scaler_qwen = StandardScaler()\n",
    "        X_qwen_train = scaler_qwen.fit_transform(X_qwen_train).astype(np.float32)\n",
    "        X_qwen_test = scaler_qwen.transform(X_qwen_test).astype(np.float32)\n",
    "        \n",
    "        scaler_falcon = StandardScaler()\n",
    "        X_falcon_train = scaler_falcon.fit_transform(X_falcon_train).astype(np.float32)\n",
    "        X_falcon_test = scaler_falcon.transform(X_falcon_test).astype(np.float32)\n",
    "        \n",
    "        # Organize data for use\n",
    "        current_data = {\n",
    "            \"qwen\": {\"X_train\": X_qwen_train, \"X_test\": X_qwen_test, \"y_train\": y_qwen_train, \"y_test\": y_qwen_test},\n",
    "            \"falcon\": {\"X_train\": X_falcon_train, \"X_test\": X_falcon_test, \"y_train\": y_falcon_train, \"y_test\": y_falcon_test}\n",
    "        }\n",
    "\n",
    "        # 3. EXECUTE EXPERIMENTS FOR BOTH SCENARIOS\n",
    "        for i, scenario in enumerate(scenarios):\n",
    "            print(f\"\\n   --- Scenario: {scenario['teacher_model']} (Teacher) <- {scenario['student_model']} (Student) ---\")\n",
    "            \n",
    "            # Reset seeds before each experiment for reproducibility\n",
    "            set_seed(SEED)\n",
    "            \n",
    "            if scenario['teacher_model'] == \"Qwen2.5-7B\":\n",
    "                X_teacher_data = current_data['qwen']\n",
    "                X_student_data = current_data['falcon']\n",
    "            else:\n",
    "                X_teacher_data = current_data['falcon']\n",
    "                X_student_data = current_data['qwen']\n",
    "            \n",
    "            res = run_experiment_pipeline_with_autoencoder(\n",
    "                X_teacher_data, X_teacher_data, scenario['teacher_model'],\n",
    "                X_student_data, X_student_data, scenario['student_model'],\n",
    "                layer_type, \"CONFIG1_EtE\",\n",
    "                latent_dim=LATENT_DIM,\n",
    "                hidden_dim_ae=HIDDEN_DIM_AE,\n",
    "                hidden_dim_align=HIDDEN_DIM_ALIGN\n",
    "            )\n",
    "            scenario_results_map[i].append(res)\n",
    "            \n",
    "            # Plot confusion matrices\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"Teacher_{scenario['teacher_model'].replace('.', '_').replace('-', '_')}\"\n",
    "            )\n",
    "            plot_confusion_matrix(\n",
    "                np.array(res['student_on_teacher']['confusion_matrix']), \n",
    "                layer_type, \n",
    "                f\"{scenario['student_model'].replace('.', '_').replace('-', '_')}_on_{scenario['teacher_model'].replace('.', '_').replace('-', '_')}_EtE\"\n",
    "            )\n",
    "\n",
    "        # 4. MEMORY CLEANUP\n",
    "        del current_data, X_qwen_train, X_qwen_test, X_falcon_train, X_falcon_test\n",
    "        del scaler_qwen, scaler_falcon\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"   Memory freed for {layer_type}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error in layer {layer_type}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Reconstruct all_results structure for JSON saving\n",
    "all_results = []\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    all_results.append({\n",
    "        \"scenario\": f\"{scenario['teacher_model']} (teacher) ‚Üí {scenario['student_model']} (student)\",\n",
    "        \"results\": scenario_results_map[i]\n",
    "    })\n",
    "\n",
    "# Save all results to JSON\n",
    "os.makedirs(\"results_metrics\", exist_ok=True)\n",
    "metrics_file = \"results_metrics/experiment_results_all_scenarios_autoencoder_EtE.json\"\n",
    "\n",
    "all_results_json = []\n",
    "for scenario_data in all_results:\n",
    "    scenario_results = []\n",
    "    for r in scenario_data['results']:\n",
    "        scenario_results.append({\n",
    "            \"layer_type\": r['type'],\n",
    "            \"teacher_model\": r['teacher_name'],\n",
    "            \"student_model\": r['student_name'],\n",
    "            \"latent_dim\": r['latent_dim'],\n",
    "            \"hidden_dim_ae\": r['hidden_dim_ae'],\n",
    "            \"hidden_dim_align\": r['hidden_dim_align'],\n",
    "            \"autoencoder_teacher_loss\": round(r['autoencoder_teacher_loss'], 6),\n",
    "            \"autoencoder_student_loss\": round(r['autoencoder_student_loss'], 6),\n",
    "            \"alignment_loss\": round(r['alignment_loss'], 6),\n",
    "            \"end_to_end_loss\": round(r['end_to_end_loss'], 6),\n",
    "            \"teacher\": {\n",
    "                \"accuracy\": round(r['teacher']['accuracy'], 4),\n",
    "                \"precision\": round(r['teacher']['precision'], 4),\n",
    "                \"recall\": round(r['teacher']['recall'], 4),\n",
    "                \"f1_score\": round(r['teacher']['f1'], 4),\n",
    "                \"confusion_matrix\": {\n",
    "                    \"TN\": int(r['teacher']['confusion_matrix'][0][0]),\n",
    "                    \"FP\": int(r['teacher']['confusion_matrix'][0][1]),\n",
    "                    \"FN\": int(r['teacher']['confusion_matrix'][1][0]),\n",
    "                    \"TP\": int(r['teacher']['confusion_matrix'][1][1])\n",
    "                }\n",
    "            },\n",
    "            \"student_on_teacher\": {\n",
    "                \"accuracy\": round(r['student_on_teacher']['accuracy'], 4),\n",
    "                \"precision\": round(r['student_on_teacher']['precision'], 4),\n",
    "                \"recall\": round(r['student_on_teacher']['recall'], 4),\n",
    "                \"f1_score\": round(r['student_on_teacher']['f1'], 4),\n",
    "                \"confusion_matrix\": {\n",
    "                    \"TN\": int(r['student_on_teacher']['confusion_matrix'][0][0]),\n",
    "                    \"FP\": int(r['student_on_teacher']['confusion_matrix'][0][1]),\n",
    "                    \"FN\": int(r['student_on_teacher']['confusion_matrix'][1][0]),\n",
    "                    \"TP\": int(r['student_on_teacher']['confusion_matrix'][1][1])\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    all_results_json.append({\n",
    "        \"scenario\": scenario_data['scenario'],\n",
    "        \"results\": scenario_results\n",
    "    })\n",
    "\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(all_results_json, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Results saved to: {metrics_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
