[
  {
    "layer_type": "attn",
    "teacher_model": "gemma-2-9b-it",
    "student_model": "Llama-3.1-8B-Instruct",
    "data_info": {
      "teacher_train_samples": 17697,
      "teacher_test_samples": 7585,
      "student_train_samples": 15458,
      "student_test_samples": 6626,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 10752,
        "epochs_trained": 15,
        "model_saved_path": "models_frozen_head/attn/frozen_head_encoder_gemma-2-9b-it.pt"
      },
      "shared_head": {
        "epochs_trained": 15,
        "model_saved_path": "models_frozen_head/attn/frozen_head_shared_head_gemma-2-9b-it.pt"
      },
      "student_encoder": {
        "input_dim": 12288,
        "epochs_trained": 51,
        "model_saved_path": "models_frozen_head/attn/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9843,
        "precision": 0.9852,
        "recall": 0.9834,
        "f1_score": 0.9843,
        "auroc": 0.9985,
        "confusion_matrix": {
          "TN": 3734,
          "FP": 56,
          "FN": 63,
          "TP": 3732
        }
      },
      "student_adapter": {
        "accuracy": 0.9885,
        "precision": 0.9898,
        "recall": 0.9868,
        "f1_score": 0.9883,
        "auroc": 0.9986,
        "confusion_matrix": {
          "TN": 3336,
          "FP": 33,
          "FN": 43,
          "TP": 3214
        }
      },
      "transfer_gap": {
        "accuracy_gap": -0.0042
      }
    }
  },
  {
    "layer_type": "attn",
    "teacher_model": "Llama-3.1-8B-Instruct",
    "student_model": "gemma-2-9b-it",
    "data_info": {
      "teacher_train_samples": 15458,
      "teacher_test_samples": 6626,
      "student_train_samples": 17697,
      "student_test_samples": 7585,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 12288,
        "epochs_trained": 19,
        "model_saved_path": "models_frozen_head/attn/frozen_head_encoder_Llama-3.1-8B-Instruct.pt"
      },
      "shared_head": {
        "epochs_trained": 19,
        "model_saved_path": "models_frozen_head/attn/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt"
      },
      "student_encoder": {
        "input_dim": 10752,
        "epochs_trained": 38,
        "model_saved_path": "models_frozen_head/attn/frozen_head_encoder_gemma-2-9b-it_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9878,
        "precision": 0.995,
        "recall": 0.98,
        "f1_score": 0.9875,
        "auroc": 0.9989,
        "confusion_matrix": {
          "TN": 3353,
          "FP": 16,
          "FN": 65,
          "TP": 3192
        }
      },
      "student_adapter": {
        "accuracy": 0.9844,
        "precision": 0.9862,
        "recall": 0.9826,
        "f1_score": 0.9844,
        "auroc": 0.9979,
        "confusion_matrix": {
          "TN": 3738,
          "FP": 52,
          "FN": 66,
          "TP": 3729
        }
      },
      "transfer_gap": {
        "accuracy_gap": 0.0033
      }
    }
  },
  {
    "layer_type": "mlp",
    "teacher_model": "gemma-2-9b-it",
    "student_model": "Llama-3.1-8B-Instruct",
    "data_info": {
      "teacher_train_samples": 17697,
      "teacher_test_samples": 7585,
      "student_train_samples": 15458,
      "student_test_samples": 6626,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 10752,
        "epochs_trained": 29,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_encoder_gemma-2-9b-it.pt"
      },
      "shared_head": {
        "epochs_trained": 29,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_shared_head_gemma-2-9b-it.pt"
      },
      "student_encoder": {
        "input_dim": 12288,
        "epochs_trained": 30,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9847,
        "precision": 0.9868,
        "recall": 0.9826,
        "f1_score": 0.9847,
        "auroc": 0.998,
        "confusion_matrix": {
          "TN": 3740,
          "FP": 50,
          "FN": 66,
          "TP": 3729
        }
      },
      "student_adapter": {
        "accuracy": 0.9807,
        "precision": 0.9857,
        "recall": 0.9748,
        "f1_score": 0.9802,
        "auroc": 0.9971,
        "confusion_matrix": {
          "TN": 3323,
          "FP": 46,
          "FN": 82,
          "TP": 3175
        }
      },
      "transfer_gap": {
        "accuracy_gap": 0.004
      }
    }
  },
  {
    "layer_type": "mlp",
    "teacher_model": "Llama-3.1-8B-Instruct",
    "student_model": "gemma-2-9b-it",
    "data_info": {
      "teacher_train_samples": 15458,
      "teacher_test_samples": 6626,
      "student_train_samples": 17697,
      "student_test_samples": 7585,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 12288,
        "epochs_trained": 24,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_encoder_Llama-3.1-8B-Instruct.pt"
      },
      "shared_head": {
        "epochs_trained": 24,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt"
      },
      "student_encoder": {
        "input_dim": 10752,
        "epochs_trained": 53,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_encoder_gemma-2-9b-it_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9793,
        "precision": 0.9851,
        "recall": 0.9727,
        "f1_score": 0.9788,
        "auroc": 0.9978,
        "confusion_matrix": {
          "TN": 3321,
          "FP": 48,
          "FN": 89,
          "TP": 3168
        }
      },
      "student_adapter": {
        "accuracy": 0.9855,
        "precision": 0.9858,
        "recall": 0.9852,
        "f1_score": 0.9855,
        "auroc": 0.9986,
        "confusion_matrix": {
          "TN": 3736,
          "FP": 54,
          "FN": 56,
          "TP": 3739
        }
      },
      "transfer_gap": {
        "accuracy_gap": -0.0062
      }
    }
  },
  {
    "layer_type": "hidden",
    "teacher_model": "gemma-2-9b-it",
    "student_model": "Llama-3.1-8B-Instruct",
    "data_info": {
      "teacher_train_samples": 17697,
      "teacher_test_samples": 7585,
      "student_train_samples": 15458,
      "student_test_samples": 6626,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 10752,
        "epochs_trained": 83,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_encoder_gemma-2-9b-it.pt"
      },
      "shared_head": {
        "epochs_trained": 83,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_shared_head_gemma-2-9b-it.pt"
      },
      "student_encoder": {
        "input_dim": 12288,
        "epochs_trained": 21,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9873,
        "precision": 0.9889,
        "recall": 0.9858,
        "f1_score": 0.9873,
        "auroc": 0.9984,
        "confusion_matrix": {
          "TN": 3748,
          "FP": 42,
          "FN": 54,
          "TP": 3741
        }
      },
      "student_adapter": {
        "accuracy": 0.9828,
        "precision": 0.9882,
        "recall": 0.9767,
        "f1_score": 0.9824,
        "auroc": 0.9957,
        "confusion_matrix": {
          "TN": 3331,
          "FP": 38,
          "FN": 76,
          "TP": 3181
        }
      },
      "transfer_gap": {
        "accuracy_gap": 0.0045
      }
    }
  },
  {
    "layer_type": "hidden",
    "teacher_model": "Llama-3.1-8B-Instruct",
    "student_model": "gemma-2-9b-it",
    "data_info": {
      "teacher_train_samples": 15458,
      "teacher_test_samples": 6626,
      "student_train_samples": 17697,
      "student_test_samples": 7585,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 12288,
        "epochs_trained": 16,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_encoder_Llama-3.1-8B-Instruct.pt"
      },
      "shared_head": {
        "epochs_trained": 16,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt"
      },
      "student_encoder": {
        "input_dim": 10752,
        "epochs_trained": 51,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_encoder_gemma-2-9b-it_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.984,
        "precision": 0.9846,
        "recall": 0.9828,
        "f1_score": 0.9837,
        "auroc": 0.9985,
        "confusion_matrix": {
          "TN": 3319,
          "FP": 50,
          "FN": 56,
          "TP": 3201
        }
      },
      "student_adapter": {
        "accuracy": 0.9838,
        "precision": 0.9855,
        "recall": 0.9821,
        "f1_score": 0.9838,
        "auroc": 0.9983,
        "confusion_matrix": {
          "TN": 3735,
          "FP": 55,
          "FN": 68,
          "TP": 3727
        }
      },
      "transfer_gap": {
        "accuracy_gap": 0.0002
      }
    }
  }
]