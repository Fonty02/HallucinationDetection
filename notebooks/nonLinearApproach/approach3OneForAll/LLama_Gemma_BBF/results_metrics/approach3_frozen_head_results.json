[
  {
    "layer_type": "attn",
    "teacher_model": "gemma-2-9b-it",
    "student_model": "Llama-3.1-8B-Instruct",
    "data_info": {
      "teacher_train_samples": 1122,
      "teacher_test_samples": 482,
      "student_train_samples": 2518,
      "student_test_samples": 1080,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 10752,
        "epochs_trained": 8,
        "model_saved_path": "models_frozen_head/attn/frozen_head_encoder_gemma-2-9b-it.pt"
      },
      "shared_head": {
        "epochs_trained": 8,
        "model_saved_path": "models_frozen_head/attn/frozen_head_shared_head_gemma-2-9b-it.pt"
      },
      "student_encoder": {
        "input_dim": 12288,
        "epochs_trained": 17,
        "model_saved_path": "models_frozen_head/attn/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9627,
        "precision": 0.9793,
        "recall": 0.9478,
        "f1_score": 0.9633,
        "auroc": 0.9923,
        "confusion_matrix": {
          "TN": 228,
          "FP": 5,
          "FN": 13,
          "TP": 236
        }
      },
      "student_adapter": {
        "accuracy": 0.9861,
        "precision": 0.9868,
        "recall": 0.985,
        "f1_score": 0.9859,
        "auroc": 0.9972,
        "confusion_matrix": {
          "TN": 541,
          "FP": 7,
          "FN": 8,
          "TP": 524
        }
      },
      "transfer_gap": {
        "accuracy_gap": -0.0235
      }
    }
  },
  {
    "layer_type": "attn",
    "teacher_model": "Llama-3.1-8B-Instruct",
    "student_model": "gemma-2-9b-it",
    "data_info": {
      "teacher_train_samples": 2518,
      "teacher_test_samples": 1080,
      "student_train_samples": 1122,
      "student_test_samples": 482,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 12288,
        "epochs_trained": 9,
        "model_saved_path": "models_frozen_head/attn/frozen_head_encoder_Llama-3.1-8B-Instruct.pt"
      },
      "shared_head": {
        "epochs_trained": 9,
        "model_saved_path": "models_frozen_head/attn/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt"
      },
      "student_encoder": {
        "input_dim": 10752,
        "epochs_trained": 19,
        "model_saved_path": "models_frozen_head/attn/frozen_head_encoder_gemma-2-9b-it_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9861,
        "precision": 0.9814,
        "recall": 0.9906,
        "f1_score": 0.986,
        "auroc": 0.9982,
        "confusion_matrix": {
          "TN": 538,
          "FP": 10,
          "FN": 5,
          "TP": 527
        }
      },
      "student_adapter": {
        "accuracy": 0.9627,
        "precision": 0.9753,
        "recall": 0.9518,
        "f1_score": 0.9634,
        "auroc": 0.9894,
        "confusion_matrix": {
          "TN": 227,
          "FP": 6,
          "FN": 12,
          "TP": 237
        }
      },
      "transfer_gap": {
        "accuracy_gap": 0.0235
      }
    }
  },
  {
    "layer_type": "mlp",
    "teacher_model": "gemma-2-9b-it",
    "student_model": "Llama-3.1-8B-Instruct",
    "data_info": {
      "teacher_train_samples": 1122,
      "teacher_test_samples": 482,
      "student_train_samples": 2518,
      "student_test_samples": 1080,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 10752,
        "epochs_trained": 17,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_encoder_gemma-2-9b-it.pt"
      },
      "shared_head": {
        "epochs_trained": 17,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_shared_head_gemma-2-9b-it.pt"
      },
      "student_encoder": {
        "input_dim": 12288,
        "epochs_trained": 12,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9564,
        "precision": 0.9597,
        "recall": 0.9558,
        "f1_score": 0.9577,
        "auroc": 0.982,
        "confusion_matrix": {
          "TN": 223,
          "FP": 10,
          "FN": 11,
          "TP": 238
        }
      },
      "student_adapter": {
        "accuracy": 0.9843,
        "precision": 0.9795,
        "recall": 0.9887,
        "f1_score": 0.9841,
        "auroc": 0.9967,
        "confusion_matrix": {
          "TN": 537,
          "FP": 11,
          "FN": 6,
          "TP": 526
        }
      },
      "transfer_gap": {
        "accuracy_gap": -0.0278
      }
    }
  },
  {
    "layer_type": "mlp",
    "teacher_model": "Llama-3.1-8B-Instruct",
    "student_model": "gemma-2-9b-it",
    "data_info": {
      "teacher_train_samples": 2518,
      "teacher_test_samples": 1080,
      "student_train_samples": 1122,
      "student_test_samples": 482,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 12288,
        "epochs_trained": 10,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_encoder_Llama-3.1-8B-Instruct.pt"
      },
      "shared_head": {
        "epochs_trained": 10,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt"
      },
      "student_encoder": {
        "input_dim": 10752,
        "epochs_trained": 17,
        "model_saved_path": "models_frozen_head/mlp/frozen_head_encoder_gemma-2-9b-it_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9852,
        "precision": 0.9796,
        "recall": 0.9906,
        "f1_score": 0.985,
        "auroc": 0.9984,
        "confusion_matrix": {
          "TN": 537,
          "FP": 11,
          "FN": 5,
          "TP": 527
        }
      },
      "student_adapter": {
        "accuracy": 0.9523,
        "precision": 0.952,
        "recall": 0.9558,
        "f1_score": 0.9539,
        "auroc": 0.9874,
        "confusion_matrix": {
          "TN": 221,
          "FP": 12,
          "FN": 11,
          "TP": 238
        }
      },
      "transfer_gap": {
        "accuracy_gap": 0.0329
      }
    }
  },
  {
    "layer_type": "hidden",
    "teacher_model": "gemma-2-9b-it",
    "student_model": "Llama-3.1-8B-Instruct",
    "data_info": {
      "teacher_train_samples": 1122,
      "teacher_test_samples": 482,
      "student_train_samples": 2518,
      "student_test_samples": 1080,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 10752,
        "epochs_trained": 12,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_encoder_gemma-2-9b-it.pt"
      },
      "shared_head": {
        "epochs_trained": 12,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_shared_head_gemma-2-9b-it.pt"
      },
      "student_encoder": {
        "input_dim": 12288,
        "epochs_trained": 13,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9502,
        "precision": 0.9747,
        "recall": 0.9277,
        "f1_score": 0.9506,
        "auroc": 0.9898,
        "confusion_matrix": {
          "TN": 227,
          "FP": 6,
          "FN": 18,
          "TP": 231
        }
      },
      "student_adapter": {
        "accuracy": 0.9843,
        "precision": 0.9813,
        "recall": 0.9868,
        "f1_score": 0.9841,
        "auroc": 0.9976,
        "confusion_matrix": {
          "TN": 538,
          "FP": 10,
          "FN": 7,
          "TP": 525
        }
      },
      "transfer_gap": {
        "accuracy_gap": -0.0341
      }
    }
  },
  {
    "layer_type": "hidden",
    "teacher_model": "Llama-3.1-8B-Instruct",
    "student_model": "gemma-2-9b-it",
    "data_info": {
      "teacher_train_samples": 2518,
      "teacher_test_samples": 1080,
      "student_train_samples": 1122,
      "student_test_samples": 482,
      "independent_undersampling_per_model": true
    },
    "encoder_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 512,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "head_config": {
      "architecture": {
        "latent_dim": 256,
        "hidden_dim": 128,
        "dropout": 0.3
      },
      "training_hyperparameters": {
        "learning_rate": 0.001,
        "weight_decay": 0.01,
        "batch_size": 64,
        "max_epochs": 100,
        "early_stopping_patience": 15,
        "early_stopping_min_delta": 0.0001,
        "gradient_clip_max_norm": 1.0,
        "optimizer": "AdamW",
        "scheduler": "CosineAnnealingLR",
        "loss_function": "BCEWithLogitsLoss",
        "use_class_weights": true
      }
    },
    "training_results": {
      "teacher_encoder": {
        "input_dim": 12288,
        "epochs_trained": 21,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_encoder_Llama-3.1-8B-Instruct.pt"
      },
      "shared_head": {
        "epochs_trained": 21,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt"
      },
      "student_encoder": {
        "input_dim": 10752,
        "epochs_trained": 14,
        "model_saved_path": "models_frozen_head/hidden/frozen_head_encoder_gemma-2-9b-it_adapter.pt"
      }
    },
    "metrics": {
      "teacher": {
        "accuracy": 0.9778,
        "precision": 0.9739,
        "recall": 0.9812,
        "f1_score": 0.9775,
        "auroc": 0.995,
        "confusion_matrix": {
          "TN": 534,
          "FP": 14,
          "FN": 10,
          "TP": 522
        }
      },
      "student_adapter": {
        "accuracy": 0.9606,
        "precision": 0.9675,
        "recall": 0.9558,
        "f1_score": 0.9616,
        "auroc": 0.9891,
        "confusion_matrix": {
          "TN": 225,
          "FP": 8,
          "FN": 11,
          "TP": 238
        }
      },
      "transfer_gap": {
        "accuracy_gap": 0.0172
      }
    }
  }
]