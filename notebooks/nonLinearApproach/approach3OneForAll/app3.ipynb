{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2d609463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "import traceback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ==================================================================\n",
    "# DEVICE CONFIGURATION\n",
    "# ==================================================================\n",
    "DEVICE = torch.device(\"cuda:2\")\n",
    "\n",
    "# ==================================================================\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# ==================================================================\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set seeds at import time\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3aa4d",
   "metadata": {},
   "source": [
    "## Cella 2: Configurazione Percorsi e Layer Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9f378c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "\n",
    "# Nomi dei modelli (usati come costanti in tutto il notebook)\n",
    "MODEL_A = \"gemma-2-9b-it\"\n",
    "MODEL_B = \"Llama-3.1-8B-Instruct\"\n",
    "\n",
    "LAYER_CONFIG = {\n",
    "    MODEL_A: \n",
    "    {\n",
    "        \"attn\": [21,24,27],\n",
    "        \"mlp\":[22,25,27],\n",
    "        \"hidden\": [23,26,34]\n",
    "    },    \n",
    "    MODEL_B: \n",
    "    {\n",
    "        \"attn\": [8,13,14],\n",
    "        \"mlp\":[14,15,21],\n",
    "        \"hidden\": [14,15,16]\n",
    "    }\n",
    "}\n",
    "DATASET_NAME = \"belief_bank_facts\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf11e1d",
   "metadata": {},
   "source": [
    "## Cella 3: Configurazione Encoder-Head (Teacher e Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eaba5682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# ENCODER-HEAD CONFIGURATION (Frozen Head Approach)\n",
    "# ==================================================================\n",
    "ENCODER_CONFIG = {\n",
    "    \"latent_dim\": 256,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"dropout\": 0.3,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"batch_size\": 64,\n",
    "    \"max_epochs\": 100,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"early_stopping_min_delta\": 1e-4,\n",
    "    \"gradient_clip_max_norm\": 1.0,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss_function\": \"BCEWithLogitsLoss\",\n",
    "    \"use_class_weights\": True\n",
    "}\n",
    "\n",
    "HEAD_CONFIG = {\n",
    "    \"latent_dim\": 256,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout\": 0.3,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"batch_size\": 64,\n",
    "    \"max_epochs\": 100,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"early_stopping_min_delta\": 1e-4,\n",
    "    \"gradient_clip_max_norm\": 1.0,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineAnnealingLR\",\n",
    "    \"loss_function\": \"BCEWithLogitsLoss\",\n",
    "    \"use_class_weights\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a227ec4c",
   "metadata": {},
   "source": [
    "## Cella 4: Definizione Classi Dataset e Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aa3a5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. Dataset classe per Training\n",
    "# ------------------------------------------------------------------\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()  # BCE expects float\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. ENCODER: Maps Input Dimension -> Latent Dimension\n",
    "# ------------------------------------------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 1024, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, latent_dim),\n",
    "            nn.LayerNorm(latent_dim)  # Normalize latent space for stability\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. CLASSIFICATION HEAD: Maps Latent Dimension -> Probability\n",
    "# ------------------------------------------------------------------\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, latent_dim: int, hidden_dim: int = 128, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)  # Binary output\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return (torch.sigmoid(logits) > 0.5).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95258c",
   "metadata": {},
   "source": [
    "## Cella 5: Funzioni Utilità per Caricamento e Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c13680a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_per_json(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Versione originale per la vecchia struttura con hallucination_labels.json\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \n",
    "                             \"generations\", \"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    hallucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'hallucinated_items': hallucinated_items,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "def stats_from_new_structure(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Nuova funzione per la struttura con cartelle hallucinated/ e not_hallucinated/\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    not_hallucinated_path = os.path.join(base_path, \"not_hallucinated\")\n",
    "    \n",
    "    hall_ids_path = os.path.join(hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    not_hall_ids_path = os.path.join(not_hallucinated_path, \"layer0_instance_ids.json\")\n",
    "    \n",
    "    with open(hall_ids_path, 'r') as f:\n",
    "        hallucinated_ids = json.load(f)\n",
    "    with open(not_hall_ids_path, 'r') as f:\n",
    "        not_hallucinated_ids = json.load(f)\n",
    "    \n",
    "    total = len(hallucinated_ids) + len(not_hallucinated_ids)\n",
    "    hallucinations = len(hallucinated_ids)\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'not_hallucinations': len(not_hallucinated_ids),\n",
    "        'hallucinated_ids': hallucinated_ids,\n",
    "        'not_hallucinated_ids': not_hallucinated_ids,\n",
    "        'hallucinated_items': hallucinated_ids,  # Alias per compatibilità\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "def detect_structure_type(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Rileva automaticamente se la struttura è vecchia o nuova.\n",
    "    \"\"\"\n",
    "    base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"activation_attn\")\n",
    "    hallucinated_path = os.path.join(base_path, \"hallucinated\")\n",
    "    if os.path.isdir(hallucinated_path):\n",
    "        return 'new'\n",
    "    return 'old'\n",
    "\n",
    "def get_stats(model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Funzione wrapper che rileva automaticamente la struttura e chiama la funzione appropriata.\n",
    "    \"\"\"\n",
    "    structure = detect_structure_type(model_name, dataset_name)\n",
    "    if structure == 'new':\n",
    "        return stats_from_new_structure(model_name, dataset_name)\n",
    "    else:\n",
    "        return stats_per_json(model_name, dataset_name)\n",
    "\n",
    "\n",
    "def get_concordant_indices_and_undersample(stats_model1, stats_model2, seed=SEED):\n",
    "    \"\"\"\n",
    "    Trova gli indici dove ENTRAMBI i modelli concordano sull'etichetta,\n",
    "    poi applica undersampling per bilanciare le classi.\n",
    "    \n",
    "    Returns:\n",
    "        concordant_indices: array di indici concordanti e bilanciati\n",
    "        labels: array di label corrispondenti (0=non-hallucinated, 1=hallucinated)\n",
    "    \"\"\"\n",
    "    total_samples = stats_model1['total']\n",
    "    assert stats_model1['total'] == stats_model2['total'], \"I due modelli devono avere lo stesso numero di campioni\"\n",
    "    \n",
    "    hall_set_1 = set(stats_model1['hallucinated_items'])\n",
    "    hall_set_2 = set(stats_model2['hallucinated_items'])\n",
    "    \n",
    "    y1 = np.array([1 if i in hall_set_1 else 0 for i in range(total_samples)])\n",
    "    y2 = np.array([1 if i in hall_set_2 else 0 for i in range(total_samples)])\n",
    "    \n",
    "    concordant_mask = (y1 == y2)\n",
    "    concordant_indices = np.where(concordant_mask)[0]\n",
    "    concordant_labels = y1[concordant_indices]\n",
    "    \n",
    "    n_hall = np.sum(concordant_labels == 1)\n",
    "    n_non_hall = np.sum(concordant_labels == 0)\n",
    "    \n",
    "    print(f\"  Campioni concordanti: {len(concordant_indices)} / {total_samples}\")\n",
    "    print(f\"    - Hallucinated (concordanti): {n_hall}\")\n",
    "    print(f\"    - Non-hallucinated (concordanti): {n_non_hall}\")\n",
    "    \n",
    "    min_count = min(n_hall, n_non_hall)\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    hall_concordant = concordant_indices[concordant_labels == 1]\n",
    "    non_hall_concordant = concordant_indices[concordant_labels == 0]\n",
    "    \n",
    "    hall_sampled = rng.choice(hall_concordant, size=min_count, replace=False)\n",
    "    non_hall_sampled = rng.choice(non_hall_concordant, size=min_count, replace=False)\n",
    "    \n",
    "    balanced_indices = np.concatenate([hall_sampled, non_hall_sampled])\n",
    "    balanced_labels = np.concatenate([np.ones(min_count, dtype=np.int8), np.zeros(min_count, dtype=np.int8)])\n",
    "    \n",
    "    shuffle_idx = rng.permutation(len(balanced_indices))\n",
    "    balanced_indices = balanced_indices[shuffle_idx]\n",
    "    balanced_labels = balanced_labels[shuffle_idx]\n",
    "    \n",
    "    print(f\"  Dopo undersampling: {len(balanced_indices)} campioni bilanciati ({min_count} per classe)\")\n",
    "    \n",
    "    return balanced_indices, balanced_labels\n",
    "\n",
    "\n",
    "def get_generator(seed=SEED):\n",
    "    \"\"\"Create reproducible generator for DataLoader\"\"\"\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return g\n",
    "\n",
    "\n",
    "def load_and_split_layers(model_name, dataset_name, layer_indices, type_layer,\n",
    "                          balanced_indices, balanced_labels, train_indices, test_indices):\n",
    "    \"\"\"\n",
    "    Load activation layers with balanced indices pre-calculated.\n",
    "    Supporta sia la vecchia struttura (file direttamente in activation_X/)\n",
    "    sia la nuova struttura (file in hallucinated/ e not_hallucinated/).\n",
    "    \n",
    "    Args:\n",
    "        balanced_indices: global indices for concordant and balanced samples\n",
    "        balanced_labels: corresponding labels (shared between models)\n",
    "        train_indices: LOCAL indices (0..len(balanced_indices)-1) for training\n",
    "        test_indices: LOCAL indices (0..len(balanced_indices)-1) for test\n",
    "    \"\"\"\n",
    "    print(f\" Loading {model_name} [{type_layer}]: layers {layer_indices}...\")\n",
    "\n",
    "    # Rileva la struttura\n",
    "    structure_type = detect_structure_type(model_name, dataset_name)\n",
    "    print(f\"  Struttura rilevata: {structure_type}\")\n",
    "\n",
    "    all_features = []\n",
    "    for layer_idx in layer_indices:\n",
    "        base_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\n",
    "                                 \"activation_\" + type_layer)\n",
    "        \n",
    "        if structure_type == 'new':\n",
    "            # Nuova struttura: carica da hallucinated/ e not_hallucinated/ separatamente\n",
    "            hall_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer_idx}_activations.pt\")\n",
    "            not_hall_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer_idx}_activations.pt\")\n",
    "            hall_ids_path = os.path.join(base_path, \"hallucinated\", f\"layer{layer_idx}_instance_ids.json\")\n",
    "            not_hall_ids_path = os.path.join(base_path, \"not_hallucinated\", f\"layer{layer_idx}_instance_ids.json\")\n",
    "            \n",
    "            if not os.path.exists(hall_path) or not os.path.exists(not_hall_path):\n",
    "                print(f\" Warning: Layer {layer_idx} non trovato. Salto.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Loading layer {layer_idx} (new structure)...\", end=\" \")\n",
    "            \n",
    "            # Carica le attivazioni\n",
    "            acts_hall = torch.load(hall_path, map_location='cpu')\n",
    "            acts_not_hall = torch.load(not_hall_path, map_location='cpu')\n",
    "            \n",
    "            # Carica gli instance_ids per sapere l'ordine\n",
    "            with open(hall_ids_path, 'r') as f:\n",
    "                hall_ids = json.load(f)\n",
    "            with open(not_hall_ids_path, 'r') as f:\n",
    "                not_hall_ids = json.load(f)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            if isinstance(acts_hall, torch.Tensor):\n",
    "                X_hall = acts_hall.float().numpy()\n",
    "            else:\n",
    "                X_hall = acts_hall.astype(np.float32)\n",
    "                \n",
    "            if isinstance(acts_not_hall, torch.Tensor):\n",
    "                X_not_hall = acts_not_hall.float().numpy()\n",
    "            else:\n",
    "                X_not_hall = acts_not_hall.astype(np.float32)\n",
    "            \n",
    "            # Flatten if needed\n",
    "            if X_hall.ndim > 2:\n",
    "                X_hall = X_hall.reshape(X_hall.shape[0], -1)\n",
    "            if X_not_hall.ndim > 2:\n",
    "                X_not_hall = X_not_hall.reshape(X_not_hall.shape[0], -1)\n",
    "            \n",
    "            # Ricostruisci l'array completo nell'ordine originale degli indici\n",
    "            total_samples = len(hall_ids) + len(not_hall_ids)\n",
    "            feature_dim = X_hall.shape[1]\n",
    "            X_layer = np.zeros((total_samples, feature_dim), dtype=np.float32)\n",
    "            \n",
    "            # Mappa: instance_id -> posizione nel file\n",
    "            for i, inst_id in enumerate(hall_ids):\n",
    "                X_layer[inst_id] = X_hall[i]\n",
    "            for i, inst_id in enumerate(not_hall_ids):\n",
    "                X_layer[inst_id] = X_not_hall[i]\n",
    "            \n",
    "            del acts_hall, acts_not_hall, X_hall, X_not_hall\n",
    "            \n",
    "        else:\n",
    "            # Vecchia struttura: file direttamente nella cartella\n",
    "            file_path = os.path.join(base_path, f\"layer{layer_idx}_activations.pt\")\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\" Warning: Layer {layer_idx} non trovato. Salto.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  Loading layer {layer_idx} (old structure)...\", end=\" \")\n",
    "            acts = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "            X_layer = acts.float().numpy() if isinstance(acts, torch.Tensor) else acts.astype(np.float32)\n",
    "            if X_layer.ndim > 2:\n",
    "                X_layer = X_layer.reshape(X_layer.shape[0], -1)\n",
    "            \n",
    "            del acts\n",
    "            \n",
    "        # Select ONLY balanced samples\n",
    "        X_layer = X_layer[balanced_indices]\n",
    "        all_features.append(X_layer)\n",
    "        print(f\"done ({X_layer.shape})\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "    if not all_features:\n",
    "        raise ValueError(f\"No layers found for {model_name}\")\n",
    "\n",
    "    X_balanced = np.concatenate(all_features, axis=1)\n",
    "    \n",
    "    X_train = X_balanced[train_indices]\n",
    "    X_test = X_balanced[test_indices]\n",
    "    y_train = balanced_labels[train_indices]\n",
    "    y_test = balanced_labels[test_indices]\n",
    "    \n",
    "    print(f\" Completed! Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ab584",
   "metadata": {},
   "source": [
    "## Cella 6: Funzione Training Teacher (Encoder + Head Jointly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3256b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher_pipeline(X_train, y_train, X_val, y_val, input_dim, device, \n",
    "                          model_name, encoder_config=ENCODER_CONFIG, head_config=HEAD_CONFIG):\n",
    "    \"\"\"Train encoder + head jointly for teacher model\"\"\"\n",
    "    print(f\"   [Teacher] Training full pipeline for {model_name}...\")\n",
    "    \n",
    "    set_seed(SEED)\n",
    "    \n",
    "    # Initialize modules\n",
    "    encoder = Encoder(input_dim, encoder_config['latent_dim'], \n",
    "                     encoder_config['hidden_dim'], encoder_config['dropout']).to(device)\n",
    "    head = ClassificationHead(encoder_config['latent_dim'], \n",
    "                             head_config['hidden_dim'], head_config['dropout']).to(device)\n",
    "    \n",
    "    # Combine parameters for optimizer\n",
    "    params = list(encoder.parameters()) + list(head.parameters())\n",
    "    optimizer = optim.AdamW(params, lr=encoder_config['learning_rate'], \n",
    "                           weight_decay=encoder_config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=encoder_config['max_epochs'])\n",
    "    \n",
    "    # Class weights for imbalance\n",
    "    n_pos = y_train.sum()\n",
    "    n_neg = len(y_train) - n_pos\n",
    "    if n_pos > 0:\n",
    "        pos_weight = torch.tensor([n_neg / n_pos]).to(device)\n",
    "    else:\n",
    "        pos_weight = torch.tensor([1.0]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    train_loader = DataLoader(SimpleDataset(X_train, y_train), \n",
    "                             batch_size=encoder_config['batch_size'], \n",
    "                             shuffle=True, generator=get_generator(SEED))\n",
    "    val_loader = DataLoader(SimpleDataset(X_val, y_val), \n",
    "                           batch_size=encoder_config['batch_size'], shuffle=False)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_states = None\n",
    "    epochs_trained = 0\n",
    "    \n",
    "    for epoch in range(encoder_config['max_epochs']):\n",
    "        encoder.train()\n",
    "        head.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            latents = encoder(X_batch)\n",
    "            logits = head(latents)\n",
    "            \n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(head.parameters()),\n",
    "                                          max_norm=encoder_config['gradient_clip_max_norm'])\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        encoder.eval()\n",
    "        head.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                latents = encoder(X_batch)\n",
    "                preds = head.predict(latents)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            patience_counter = 0\n",
    "            best_states = {\n",
    "                'encoder': encoder.state_dict().copy(),\n",
    "                'head': head.state_dict().copy()\n",
    "            }\n",
    "            epochs_trained = epoch + 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= encoder_config['early_stopping_patience']:\n",
    "                print(f\"     Early stopping at epoch {epoch+1}. Best Acc: {best_acc:.4f}\")\n",
    "                break\n",
    "    \n",
    "    if epochs_trained == 0:\n",
    "        epochs_trained = encoder_config['max_epochs']\n",
    "    \n",
    "    encoder.load_state_dict(best_states['encoder'])\n",
    "    head.load_state_dict(best_states['head'])\n",
    "    \n",
    "    return encoder, head, best_acc, epochs_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3fe5f",
   "metadata": {},
   "source": [
    "## Cella 7: Funzione Training Student (Adapter con Head Frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4823b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_adapter(X_train, y_train, X_val, y_val, input_dim, frozen_head, device, \n",
    "                         student_name, encoder_config=ENCODER_CONFIG):\n",
    "    \"\"\"Train new encoder with frozen head from teacher\"\"\"\n",
    "    print(f\"   [Student] Training Adapter Encoder for {student_name} (Head Frozen)...\")\n",
    "    \n",
    "    # Freeze the Head\n",
    "    frozen_head.eval()\n",
    "    for param in frozen_head.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    set_seed(SEED)\n",
    "    \n",
    "    # New Encoder for Student\n",
    "    encoder = Encoder(input_dim, encoder_config['latent_dim'], \n",
    "                     encoder_config['hidden_dim'], encoder_config['dropout']).to(device)\n",
    "    \n",
    "    # Optimize ONLY the encoder\n",
    "    optimizer = optim.AdamW(encoder.parameters(), lr=encoder_config['learning_rate'], \n",
    "                           weight_decay=encoder_config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=encoder_config['max_epochs'])\n",
    "    \n",
    "    n_pos = y_train.sum()\n",
    "    n_neg = len(y_train) - n_pos\n",
    "    if n_pos > 0:\n",
    "        pos_weight = torch.tensor([n_neg / n_pos]).to(device)\n",
    "    else:\n",
    "        pos_weight = torch.tensor([1.0]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    train_loader = DataLoader(SimpleDataset(X_train, y_train), \n",
    "                             batch_size=encoder_config['batch_size'], \n",
    "                             shuffle=True, generator=get_generator(SEED))\n",
    "    val_loader = DataLoader(SimpleDataset(X_val, y_val), \n",
    "                           batch_size=encoder_config['batch_size'], shuffle=False)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    epochs_trained = 0\n",
    "    \n",
    "    for epoch in range(encoder_config['max_epochs']):\n",
    "        encoder.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Student Input -> Student Encoder -> Frozen Head -> Loss\n",
    "            latents = encoder(X_batch)\n",
    "            logits = frozen_head(latents)  # Head is fixed\n",
    "            \n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(),\n",
    "                                          max_norm=encoder_config['gradient_clip_max_norm'])\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        encoder.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                latents = encoder(X_batch)\n",
    "                preds = frozen_head.predict(latents)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            patience_counter = 0\n",
    "            best_state = encoder.state_dict().copy()\n",
    "            epochs_trained = epoch + 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= encoder_config['early_stopping_patience']:\n",
    "                print(f\"     Early stopping at epoch {epoch+1}. Best Acc: {best_acc:.4f}\")\n",
    "                break\n",
    "    \n",
    "    if epochs_trained == 0:\n",
    "        epochs_trained = encoder_config['max_epochs']\n",
    "    \n",
    "    # Load best state\n",
    "    if best_state is not None:\n",
    "        encoder.load_state_dict(best_state)\n",
    "    \n",
    "    return encoder, best_acc, epochs_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b120332",
   "metadata": {},
   "source": [
    "## Cella 8: Funzione Salvataggio Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "58f84c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title, filename):\n",
    "    \"\"\"Plot and save confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(\"confusion_matrices_frozen_head\", exist_ok=True)\n",
    "    plt.savefig(os.path.join(\"confusion_matrices_frozen_head\", filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f41913",
   "metadata": {},
   "source": [
    "## Cella 9: Main Execution - Setup Iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a4823306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALISI CONCORDANZA E UNDERSAMPLING\n",
      "============================================================\n",
      "gemma-2-9b-it totali: 27416, hallucinated: 802\n",
      "Llama-3.1-8B-Instruct totali: 27416, hallucinated: 1799\n",
      "\n",
      "  Campioni concordanti: 25749 / 27416\n",
      "    - Hallucinated (concordanti): 467\n",
      "    - Non-hallucinated (concordanti): 25282\n",
      "  Dopo undersampling: 934 campioni bilanciati (467 per classe)\n",
      "\n",
      "Campioni bilanciati totali: 934\n",
      "Train: 653, Test: 281\n",
      "Label train - Hall: 314, Non-Hall: 339\n",
      "Label test  - Hall: 153, Non-Hall: 128\n",
      "\n",
      "Using device: cuda:2\n",
      "\n",
      "Configuration:\n",
      "  LATENT_DIM: 256\n",
      "  HIDDEN_DIM: 512\n",
      "  MAX_EPOCHS: 100\n",
      "  BATCH_SIZE: 64\n"
     ]
    }
   ],
   "source": [
    "# Load statistics\n",
    "model_a_stats = get_stats(MODEL_A, DATASET_NAME)\n",
    "model_b_stats = get_stats(MODEL_B, DATASET_NAME)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ANALISI CONCORDANZA E UNDERSAMPLING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{MODEL_A} totali: {model_a_stats['total']}, hallucinated: {model_a_stats['hallucinations']}\")\n",
    "print(f\"{MODEL_B} totali: {model_b_stats['total']}, hallucinated: {model_b_stats['hallucinations']}\")\n",
    "print()\n",
    "\n",
    "balanced_indices, balanced_labels = get_concordant_indices_and_undersample(model_a_stats, model_b_stats, seed=SEED)\n",
    "\n",
    "# Prepare indices - LOCAL (0..len(balanced_indices)-1)\n",
    "n_balanced = len(balanced_indices)\n",
    "rng = np.random.RandomState(SEED)\n",
    "shuffled_local_indices = rng.permutation(n_balanced)\n",
    "split = int(0.7 * n_balanced)\n",
    "train_indices, test_indices = shuffled_local_indices[:split], shuffled_local_indices[split:]\n",
    "\n",
    "print(f\"\\nCampioni bilanciati totali: {n_balanced}\")\n",
    "print(f\"Train: {len(train_indices)}, Test: {len(test_indices)}\")\n",
    "print(f\"Label train - Hall: {np.sum(balanced_labels[train_indices]==1)}, Non-Hall: {np.sum(balanced_labels[train_indices]==0)}\")\n",
    "print(f\"Label test  - Hall: {np.sum(balanced_labels[test_indices]==1)}, Non-Hall: {np.sum(balanced_labels[test_indices]==0)}\")\n",
    "\n",
    "device = DEVICE\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "results_log = []\n",
    "\n",
    "# Define scenarios\n",
    "scenarios = [\n",
    "    {\"teacher\": MODEL_A, \"student\": MODEL_B},\n",
    "    {\"teacher\": MODEL_B, \"student\": MODEL_A}\n",
    "]\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  LATENT_DIM: {ENCODER_CONFIG['latent_dim']}\")\n",
    "print(f\"  HIDDEN_DIM: {ENCODER_CONFIG['hidden_dim']}\")\n",
    "print(f\"  MAX_EPOCHS: {ENCODER_CONFIG['max_epochs']}\")\n",
    "print(f\"  BATCH_SIZE: {ENCODER_CONFIG['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ede023",
   "metadata": {},
   "source": [
    "## Cella 10: Main Execution - Loop per Layer Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d7bd1e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING LAYER TYPE: ATTN\n",
      "============================================================\n",
      "Loading data for gemma-2-9b-it...\n",
      " Loading gemma-2-9b-it [attn]: layers [21, 24, 27]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 21 (new structure)... done ((934, 3584))\n",
      "  Loading layer 24 (new structure)... done ((934, 3584))\n",
      "  Loading layer 24 (new structure)... done ((934, 3584))\n",
      "  Loading layer 27 (new structure)... done ((934, 3584))\n",
      "  Loading layer 27 (new structure)... done ((934, 3584))\n",
      " Completed! Train: (653, 10752), Test: (281, 10752)\n",
      "Loading data for Llama-3.1-8B-Instruct...\n",
      " Loading Llama-3.1-8B-Instruct [attn]: layers [8, 13, 14]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 8 (new structure)... done ((934, 3584))\n",
      " Completed! Train: (653, 10752), Test: (281, 10752)\n",
      "Loading data for Llama-3.1-8B-Instruct...\n",
      " Loading Llama-3.1-8B-Instruct [attn]: layers [8, 13, 14]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 8 (new structure)... done ((934, 4096))\n",
      "  Loading layer 13 (new structure)... done ((934, 4096))\n",
      "  Loading layer 13 (new structure)... done ((934, 4096))\n",
      "  Loading layer 14 (new structure)... done ((934, 4096))\n",
      "  Loading layer 14 (new structure)... done ((934, 4096))\n",
      " Completed! Train: (653, 12288), Test: (281, 12288)\n",
      "\n",
      "--- SCENARIO: Teacher=gemma-2-9b-it -> Student=Llama-3.1-8B-Instruct ---\n",
      "   [Teacher] Training full pipeline for gemma-2-9b-it...\n",
      "done ((934, 4096))\n",
      " Completed! Train: (653, 12288), Test: (281, 12288)\n",
      "\n",
      "--- SCENARIO: Teacher=gemma-2-9b-it -> Student=Llama-3.1-8B-Instruct ---\n",
      "   [Teacher] Training full pipeline for gemma-2-9b-it...\n",
      "     Early stopping at epoch 28. Best Acc: 0.9794\n",
      "   [Result] Teacher (gemma-2-9b-it) Test F1: 0.9439 | Acc: 0.9395 | AUROC: 0.9806\n",
      "   [Student] Training Adapter Encoder for Llama-3.1-8B-Instruct (Head Frozen)...\n",
      "     Early stopping at epoch 28. Best Acc: 0.9794\n",
      "   [Result] Teacher (gemma-2-9b-it) Test F1: 0.9439 | Acc: 0.9395 | AUROC: 0.9806\n",
      "   [Student] Training Adapter Encoder for Llama-3.1-8B-Instruct (Head Frozen)...\n",
      "     Early stopping at epoch 20. Best Acc: 0.9897\n",
      "   [Result] Student (Llama-3.1-8B-Instruct) Adapter Test F1: 0.9673 | Acc: 0.9644 | AUROC: 0.9938\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/attn/frozen_head_encoder_gemma-2-9b-it.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/attn/frozen_head_shared_head_gemma-2-9b-it.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/attn/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt\n",
      "     Early stopping at epoch 20. Best Acc: 0.9897\n",
      "   [Result] Student (Llama-3.1-8B-Instruct) Adapter Test F1: 0.9673 | Acc: 0.9644 | AUROC: 0.9938\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/attn/frozen_head_encoder_gemma-2-9b-it.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/attn/frozen_head_shared_head_gemma-2-9b-it.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/attn/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt\n",
      "\n",
      "--- SCENARIO: Teacher=Llama-3.1-8B-Instruct -> Student=gemma-2-9b-it ---\n",
      "   [Teacher] Training full pipeline for Llama-3.1-8B-Instruct...\n",
      "\n",
      "--- SCENARIO: Teacher=Llama-3.1-8B-Instruct -> Student=gemma-2-9b-it ---\n",
      "   [Teacher] Training full pipeline for Llama-3.1-8B-Instruct...\n",
      "     Early stopping at epoch 30. Best Acc: 0.9897\n",
      "   [Result] Teacher (Llama-3.1-8B-Instruct) Test F1: 0.9803 | Acc: 0.9786 | AUROC: 0.9965\n",
      "   [Student] Training Adapter Encoder for gemma-2-9b-it (Head Frozen)...\n",
      "     Early stopping at epoch 30. Best Acc: 0.9897\n",
      "   [Result] Teacher (Llama-3.1-8B-Instruct) Test F1: 0.9803 | Acc: 0.9786 | AUROC: 0.9965\n",
      "   [Student] Training Adapter Encoder for gemma-2-9b-it (Head Frozen)...\n",
      "     Early stopping at epoch 38. Best Acc: 0.9794\n",
      "   [Result] Student (gemma-2-9b-it) Adapter Test F1: 0.9533 | Acc: 0.9502 | AUROC: 0.9871\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/attn/frozen_head_encoder_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/attn/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/attn/frozen_head_encoder_gemma-2-9b-it_adapter.pt\n",
      "     Early stopping at epoch 38. Best Acc: 0.9794\n",
      "   [Result] Student (gemma-2-9b-it) Adapter Test F1: 0.9533 | Acc: 0.9502 | AUROC: 0.9871\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/attn/frozen_head_encoder_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/attn/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/attn/frozen_head_encoder_gemma-2-9b-it_adapter.pt\n",
      "\n",
      "============================================================\n",
      "PROCESSING LAYER TYPE: MLP\n",
      "============================================================\n",
      "Loading data for gemma-2-9b-it...\n",
      " Loading gemma-2-9b-it [mlp]: layers [22, 25, 27]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 22 (new structure)... \n",
      "============================================================\n",
      "PROCESSING LAYER TYPE: MLP\n",
      "============================================================\n",
      "Loading data for gemma-2-9b-it...\n",
      " Loading gemma-2-9b-it [mlp]: layers [22, 25, 27]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 22 (new structure)... done ((934, 3584))\n",
      "  Loading layer 25 (new structure)... done ((934, 3584))\n",
      "  Loading layer 25 (new structure)... done ((934, 3584))\n",
      "  Loading layer 27 (new structure)... done ((934, 3584))\n",
      "  Loading layer 27 (new structure)... done ((934, 3584))\n",
      " Completed! Train: (653, 10752), Test: (281, 10752)\n",
      "Loading data for Llama-3.1-8B-Instruct...\n",
      " Loading Llama-3.1-8B-Instruct [mlp]: layers [14, 15, 21]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 14 (new structure)... done ((934, 3584))\n",
      " Completed! Train: (653, 10752), Test: (281, 10752)\n",
      "Loading data for Llama-3.1-8B-Instruct...\n",
      " Loading Llama-3.1-8B-Instruct [mlp]: layers [14, 15, 21]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 14 (new structure)... done ((934, 4096))\n",
      "  Loading layer 15 (new structure)... done ((934, 4096))\n",
      "  Loading layer 15 (new structure)... done ((934, 4096))\n",
      "  Loading layer 21 (new structure)... done ((934, 4096))\n",
      "  Loading layer 21 (new structure)... done ((934, 4096))\n",
      " Completed! Train: (653, 12288), Test: (281, 12288)\n",
      "done ((934, 4096))\n",
      " Completed! Train: (653, 12288), Test: (281, 12288)\n",
      "\n",
      "--- SCENARIO: Teacher=gemma-2-9b-it -> Student=Llama-3.1-8B-Instruct ---\n",
      "   [Teacher] Training full pipeline for gemma-2-9b-it...\n",
      "\n",
      "--- SCENARIO: Teacher=gemma-2-9b-it -> Student=Llama-3.1-8B-Instruct ---\n",
      "   [Teacher] Training full pipeline for gemma-2-9b-it...\n",
      "     Early stopping at epoch 46. Best Acc: 0.9897\n",
      "   [Result] Teacher (gemma-2-9b-it) Test F1: 0.9338 | Acc: 0.9288 | AUROC: 0.9838\n",
      "   [Student] Training Adapter Encoder for Llama-3.1-8B-Instruct (Head Frozen)...\n",
      "     Early stopping at epoch 46. Best Acc: 0.9897\n",
      "   [Result] Teacher (gemma-2-9b-it) Test F1: 0.9338 | Acc: 0.9288 | AUROC: 0.9838\n",
      "   [Student] Training Adapter Encoder for Llama-3.1-8B-Instruct (Head Frozen)...\n",
      "     Early stopping at epoch 18. Best Acc: 0.9691\n",
      "   [Result] Student (Llama-3.1-8B-Instruct) Adapter Test F1: 0.9600 | Acc: 0.9573 | AUROC: 0.9866\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/mlp/frozen_head_encoder_gemma-2-9b-it.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/mlp/frozen_head_shared_head_gemma-2-9b-it.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/mlp/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt\n",
      "     Early stopping at epoch 18. Best Acc: 0.9691\n",
      "   [Result] Student (Llama-3.1-8B-Instruct) Adapter Test F1: 0.9600 | Acc: 0.9573 | AUROC: 0.9866\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/mlp/frozen_head_encoder_gemma-2-9b-it.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/mlp/frozen_head_shared_head_gemma-2-9b-it.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/mlp/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt\n",
      "\n",
      "--- SCENARIO: Teacher=Llama-3.1-8B-Instruct -> Student=gemma-2-9b-it ---\n",
      "   [Teacher] Training full pipeline for Llama-3.1-8B-Instruct...\n",
      "\n",
      "--- SCENARIO: Teacher=Llama-3.1-8B-Instruct -> Student=gemma-2-9b-it ---\n",
      "   [Teacher] Training full pipeline for Llama-3.1-8B-Instruct...\n",
      "     Early stopping at epoch 37. Best Acc: 0.9794\n",
      "   [Result] Teacher (Llama-3.1-8B-Instruct) Test F1: 0.9739 | Acc: 0.9715 | AUROC: 0.9961\n",
      "   [Student] Training Adapter Encoder for gemma-2-9b-it (Head Frozen)...\n",
      "     Early stopping at epoch 37. Best Acc: 0.9794\n",
      "   [Result] Teacher (Llama-3.1-8B-Instruct) Test F1: 0.9739 | Acc: 0.9715 | AUROC: 0.9961\n",
      "   [Student] Training Adapter Encoder for gemma-2-9b-it (Head Frozen)...\n",
      "     Early stopping at epoch 17. Best Acc: 0.9691\n",
      "   [Result] Student (gemma-2-9b-it) Adapter Test F1: 0.9545 | Acc: 0.9502 | AUROC: 0.9841\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/mlp/frozen_head_encoder_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/mlp/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/mlp/frozen_head_encoder_gemma-2-9b-it_adapter.pt\n",
      "     Early stopping at epoch 17. Best Acc: 0.9691\n",
      "   [Result] Student (gemma-2-9b-it) Adapter Test F1: 0.9545 | Acc: 0.9502 | AUROC: 0.9841\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/mlp/frozen_head_encoder_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/mlp/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/mlp/frozen_head_encoder_gemma-2-9b-it_adapter.pt\n",
      "\n",
      "============================================================\n",
      "PROCESSING LAYER TYPE: HIDDEN\n",
      "============================================================\n",
      "Loading data for gemma-2-9b-it...\n",
      " Loading gemma-2-9b-it [hidden]: layers [23, 26, 34]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 23 (new structure)... \n",
      "============================================================\n",
      "PROCESSING LAYER TYPE: HIDDEN\n",
      "============================================================\n",
      "Loading data for gemma-2-9b-it...\n",
      " Loading gemma-2-9b-it [hidden]: layers [23, 26, 34]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 23 (new structure)... done ((934, 3584))\n",
      "  Loading layer 26 (new structure)... done ((934, 3584))\n",
      "  Loading layer 26 (new structure)... done ((934, 3584))\n",
      "  Loading layer 34 (new structure)... done ((934, 3584))\n",
      "  Loading layer 34 (new structure)... done ((934, 3584))\n",
      " Completed! Train: (653, 10752), Test: (281, 10752)\n",
      "Loading data for Llama-3.1-8B-Instruct...\n",
      " Loading Llama-3.1-8B-Instruct [hidden]: layers [14, 15, 16]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 14 (new structure)... done ((934, 3584))\n",
      " Completed! Train: (653, 10752), Test: (281, 10752)\n",
      "Loading data for Llama-3.1-8B-Instruct...\n",
      " Loading Llama-3.1-8B-Instruct [hidden]: layers [14, 15, 16]...\n",
      "  Struttura rilevata: new\n",
      "  Loading layer 14 (new structure)... done ((934, 4096))\n",
      "  Loading layer 15 (new structure)... done ((934, 4096))\n",
      "  Loading layer 15 (new structure)... done ((934, 4096))\n",
      "  Loading layer 16 (new structure)... done ((934, 4096))\n",
      "  Loading layer 16 (new structure)... done ((934, 4096))\n",
      " Completed! Train: (653, 12288), Test: (281, 12288)\n",
      "\n",
      "--- SCENARIO: Teacher=gemma-2-9b-it -> Student=Llama-3.1-8B-Instruct ---\n",
      "   [Teacher] Training full pipeline for gemma-2-9b-it...\n",
      "done ((934, 4096))\n",
      " Completed! Train: (653, 12288), Test: (281, 12288)\n",
      "\n",
      "--- SCENARIO: Teacher=gemma-2-9b-it -> Student=Llama-3.1-8B-Instruct ---\n",
      "   [Teacher] Training full pipeline for gemma-2-9b-it...\n",
      "     Early stopping at epoch 29. Best Acc: 0.9794\n",
      "   [Result] Teacher (gemma-2-9b-it) Test F1: 0.9446 | Acc: 0.9395 | AUROC: 0.9798\n",
      "   [Student] Training Adapter Encoder for Llama-3.1-8B-Instruct (Head Frozen)...\n",
      "     Early stopping at epoch 29. Best Acc: 0.9794\n",
      "   [Result] Teacher (gemma-2-9b-it) Test F1: 0.9446 | Acc: 0.9395 | AUROC: 0.9798\n",
      "   [Student] Training Adapter Encoder for Llama-3.1-8B-Instruct (Head Frozen)...\n",
      "     Early stopping at epoch 23. Best Acc: 0.9794\n",
      "   [Result] Student (Llama-3.1-8B-Instruct) Adapter Test F1: 0.9770 | Acc: 0.9751 | AUROC: 0.9927\n",
      "   Saving models...\n",
      "     Early stopping at epoch 23. Best Acc: 0.9794\n",
      "   [Result] Student (Llama-3.1-8B-Instruct) Adapter Test F1: 0.9770 | Acc: 0.9751 | AUROC: 0.9927\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/hidden/frozen_head_encoder_gemma-2-9b-it.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/hidden/frozen_head_shared_head_gemma-2-9b-it.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/hidden/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/hidden/frozen_head_encoder_gemma-2-9b-it.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/hidden/frozen_head_shared_head_gemma-2-9b-it.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/hidden/frozen_head_encoder_Llama-3.1-8B-Instruct_adapter.pt\n",
      "\n",
      "--- SCENARIO: Teacher=Llama-3.1-8B-Instruct -> Student=gemma-2-9b-it ---\n",
      "   [Teacher] Training full pipeline for Llama-3.1-8B-Instruct...\n",
      "\n",
      "--- SCENARIO: Teacher=Llama-3.1-8B-Instruct -> Student=gemma-2-9b-it ---\n",
      "   [Teacher] Training full pipeline for Llama-3.1-8B-Instruct...\n",
      "     Early stopping at epoch 27. Best Acc: 0.9794\n",
      "   [Result] Teacher (Llama-3.1-8B-Instruct) Test F1: 0.9632 | Acc: 0.9609 | AUROC: 0.9907\n",
      "   [Student] Training Adapter Encoder for gemma-2-9b-it (Head Frozen)...\n",
      "     Early stopping at epoch 27. Best Acc: 0.9794\n",
      "   [Result] Teacher (Llama-3.1-8B-Instruct) Test F1: 0.9632 | Acc: 0.9609 | AUROC: 0.9907\n",
      "   [Student] Training Adapter Encoder for gemma-2-9b-it (Head Frozen)...\n",
      "     Early stopping at epoch 23. Best Acc: 0.9794\n",
      "   [Result] Student (gemma-2-9b-it) Adapter Test F1: 0.9536 | Acc: 0.9502 | AUROC: 0.9856\n",
      "     Early stopping at epoch 23. Best Acc: 0.9794\n",
      "   [Result] Student (gemma-2-9b-it) Adapter Test F1: 0.9536 | Acc: 0.9502 | AUROC: 0.9856\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/hidden/frozen_head_encoder_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/hidden/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/hidden/frozen_head_encoder_gemma-2-9b-it_adapter.pt\n",
      "   Saving models...\n",
      "     ✓ Teacher Encoder saved: models_frozen_head/hidden/frozen_head_encoder_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Shared Head saved: models_frozen_head/hidden/frozen_head_shared_head_Llama-3.1-8B-Instruct.pt\n",
      "     ✓ Student Encoder saved: models_frozen_head/hidden/frozen_head_encoder_gemma-2-9b-it_adapter.pt\n"
     ]
    }
   ],
   "source": [
    "for layer_type in ['attn', 'mlp', 'hidden']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING LAYER TYPE: {layer_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load and Preprocess Data for BOTH models\n",
    "    print(f\"Loading data for {MODEL_A}...\")\n",
    "    X_model_a_tr, X_model_a_te, y_model_a_tr, y_model_a_te = load_and_split_layers(\n",
    "        MODEL_A, DATASET_NAME, LAYER_CONFIG[MODEL_A][layer_type], \n",
    "        layer_type, balanced_indices, balanced_labels, train_indices, test_indices)\n",
    "\n",
    "    print(f\"Loading data for {MODEL_B}...\")\n",
    "    X_model_b_tr, X_model_b_te, y_model_b_tr, y_model_b_te = load_and_split_layers(\n",
    "        MODEL_B, DATASET_NAME, LAYER_CONFIG[MODEL_B][layer_type], \n",
    "        layer_type, balanced_indices, balanced_labels, train_indices, test_indices)\n",
    "\n",
    "    # Scaling (Independent for each model)\n",
    "    s_model_a = StandardScaler()\n",
    "    X_model_a_tr = s_model_a.fit_transform(X_model_a_tr).astype(np.float32)\n",
    "    X_model_a_te = s_model_a.transform(X_model_a_te).astype(np.float32)\n",
    "\n",
    "    s_model_b = StandardScaler()\n",
    "    X_model_b_tr = s_model_b.fit_transform(X_model_b_tr).astype(np.float32)\n",
    "    X_model_b_te = s_model_b.transform(X_model_b_te).astype(np.float32)\n",
    "\n",
    "    # Pack data - labels are the SAME for both models now!\n",
    "    data_map = {\n",
    "        MODEL_A: {\"X_tr\": X_model_a_tr, \"y_tr\": y_model_a_tr, \"X_te\": X_model_a_te, \"y_te\": y_model_a_te},\n",
    "        MODEL_B: {\"X_tr\": X_model_b_tr, \"y_tr\": y_model_b_tr, \"X_te\": X_model_b_te, \"y_te\": y_model_b_te}\n",
    "    }\n",
    "\n",
    "    # Run Both Scenarios\n",
    "    for sc in scenarios:\n",
    "        t_name = sc['teacher']\n",
    "        s_name = sc['student']\n",
    "        print(f\"\\n--- SCENARIO: Teacher={t_name} -> Student={s_name} ---\")\n",
    "        \n",
    "        teacher_data = data_map[t_name]\n",
    "        student_data = data_map[s_name]\n",
    "        \n",
    "        # Split Train into Train/Val for early stopping\n",
    "        n_tr = len(teacher_data[\"X_tr\"])\n",
    "        idx = np.arange(n_tr)\n",
    "        np.random.seed(SEED)  \n",
    "        np.random.shuffle(idx)\n",
    "        v_size = int(0.15 * n_tr)\n",
    "        tr_idx, val_idx = idx[v_size:], idx[:v_size]\n",
    "        \n",
    "        # --- PHASE 1: Train Teacher ---\n",
    "        enc_teacher, head_shared, best_acc_t, teacher_epochs = train_teacher_pipeline(\n",
    "            teacher_data[\"X_tr\"][tr_idx], teacher_data[\"y_tr\"][tr_idx],\n",
    "            teacher_data[\"X_tr\"][val_idx], teacher_data[\"y_tr\"][val_idx],\n",
    "            input_dim=teacher_data[\"X_tr\"].shape[1],\n",
    "            device=device, model_name=t_name,\n",
    "            encoder_config=ENCODER_CONFIG, head_config=HEAD_CONFIG\n",
    "        )\n",
    "        \n",
    "        # Evaluate Teacher on Test\n",
    "        enc_teacher.eval()\n",
    "        head_shared.eval()\n",
    "        with torch.no_grad():\n",
    "            z_t = enc_teacher(torch.from_numpy(teacher_data[\"X_te\"]).float().to(device))\n",
    "            preds_t = head_shared.predict(z_t).cpu().numpy()\n",
    "            # Get probabilities for AUROC\n",
    "            logits_t = head_shared(z_t)\n",
    "            probs_t = torch.sigmoid(logits_t).cpu().numpy()\n",
    "        \n",
    "        t_f1 = f1_score(teacher_data[\"y_te\"], preds_t)\n",
    "        t_acc = accuracy_score(teacher_data[\"y_te\"], preds_t)\n",
    "        t_prec = precision_score(teacher_data[\"y_te\"], preds_t)\n",
    "        t_rec = recall_score(teacher_data[\"y_te\"], preds_t)\n",
    "        t_cm = confusion_matrix(teacher_data[\"y_te\"], preds_t)\n",
    "        t_auroc = roc_auc_score(teacher_data[\"y_te\"], probs_t)\n",
    "        print(f\"   [Result] Teacher ({t_name}) Test F1: {t_f1:.4f} | Acc: {t_acc:.4f} | AUROC: {t_auroc:.4f}\")\n",
    "        plot_confusion_matrix(teacher_data[\"y_te\"], preds_t, \n",
    "                             f\"Teacher {t_name} ({layer_type})\", \n",
    "                             f\"cm_{layer_type}_teacher_{t_name}.png\")\n",
    "\n",
    "        # --- PHASE 2: Train Student with Frozen Head ---\n",
    "        enc_student, best_acc_s, student_epochs = train_student_adapter(\n",
    "            student_data[\"X_tr\"][tr_idx], student_data[\"y_tr\"][tr_idx],\n",
    "            student_data[\"X_tr\"][val_idx], student_data[\"y_tr\"][val_idx],\n",
    "            input_dim=student_data[\"X_tr\"].shape[1],\n",
    "            frozen_head=head_shared,\n",
    "            device=device, student_name=s_name,\n",
    "            encoder_config=ENCODER_CONFIG\n",
    "        )\n",
    "        \n",
    "        # Evaluate Student on Test\n",
    "        enc_student.eval()\n",
    "        with torch.no_grad():\n",
    "            z_s = enc_student(torch.from_numpy(student_data[\"X_te\"]).float().to(device))\n",
    "            preds_s = head_shared.predict(z_s).cpu().numpy()\n",
    "            # Get probabilities for AUROC\n",
    "            logits_s = head_shared(z_s)\n",
    "            probs_s = torch.sigmoid(logits_s).cpu().numpy()\n",
    "        \n",
    "        s_f1 = f1_score(student_data[\"y_te\"], preds_s)\n",
    "        s_acc = accuracy_score(student_data[\"y_te\"], preds_s)\n",
    "        s_prec = precision_score(student_data[\"y_te\"], preds_s)\n",
    "        s_rec = recall_score(student_data[\"y_te\"], preds_s)\n",
    "        s_cm = confusion_matrix(student_data[\"y_te\"], preds_s)\n",
    "        s_auroc = roc_auc_score(student_data[\"y_te\"], probs_s)\n",
    "        \n",
    "        print(f\"   [Result] Student ({s_name}) Adapter Test F1: {s_f1:.4f} | Acc: {s_acc:.4f} | AUROC: {s_auroc:.4f}\")\n",
    "        plot_confusion_matrix(student_data[\"y_te\"], preds_s, \n",
    "                             f\"Student {s_name} Adapter ({layer_type})\", \n",
    "                             f\"cm_{layer_type}_{s_name}_adapter.png\")\n",
    "        \n",
    "        # --- PHASE 3: Save Models ---\n",
    "        print(\"   Saving models...\")\n",
    "        model_save_dir = os.path.join(\"models_frozen_head\", layer_type)\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save Teacher Encoder\n",
    "        teacher_encoder_filename = os.path.join(model_save_dir, f\"frozen_head_encoder_{t_name}.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': enc_teacher.state_dict(),\n",
    "            'encoder_config': ENCODER_CONFIG,\n",
    "            'input_dim': int(teacher_data[\"X_tr\"].shape[1]),\n",
    "            'latent_dim': ENCODER_CONFIG['latent_dim'],\n",
    "            'best_val_acc': best_acc_t,\n",
    "            'epochs_trained': teacher_epochs,\n",
    "            'model_name': t_name,\n",
    "            'layer_type': layer_type,\n",
    "            'scenario': f\"{t_name}_teacher\"\n",
    "        }, teacher_encoder_filename)\n",
    "        print(f\"     ✓ Teacher Encoder saved: {teacher_encoder_filename}\")\n",
    "        \n",
    "        # Save Shared Head\n",
    "        head_filename = os.path.join(model_save_dir, f\"frozen_head_shared_head_{t_name}.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': head_shared.state_dict(),\n",
    "            'head_config': HEAD_CONFIG,\n",
    "            'latent_dim': ENCODER_CONFIG['latent_dim'],\n",
    "            'best_val_acc': best_acc_t,\n",
    "            'epochs_trained': teacher_epochs,\n",
    "            'teacher_model': t_name,\n",
    "            'layer_type': layer_type,\n",
    "            'scenario': f\"{t_name}_head\"\n",
    "        }, head_filename)\n",
    "        print(f\"     ✓ Shared Head saved: {head_filename}\")\n",
    "        \n",
    "        # Save Student Encoder\n",
    "        student_encoder_filename = os.path.join(model_save_dir, f\"frozen_head_encoder_{s_name}_adapter.pt\")\n",
    "        torch.save({\n",
    "            'model_state_dict': enc_student.state_dict(),\n",
    "            'encoder_config': ENCODER_CONFIG,\n",
    "            'input_dim': int(student_data[\"X_tr\"].shape[1]),\n",
    "            'latent_dim': ENCODER_CONFIG['latent_dim'],\n",
    "            'best_val_acc': best_acc_s,\n",
    "            'epochs_trained': student_epochs,\n",
    "            'model_name': s_name,\n",
    "            'layer_type': layer_type,\n",
    "            'scenario': f\"{s_name}_student_adapter\"\n",
    "        }, student_encoder_filename)\n",
    "        print(f\"     ✓ Student Encoder saved: {student_encoder_filename}\")\n",
    "        \n",
    "        # Log results with model paths\n",
    "        results_log.append({\n",
    "            \"layer\": layer_type,\n",
    "            \"teacher\": t_name,\n",
    "            \"student\": s_name,\n",
    "            \"teacher_auroc\": t_auroc,\n",
    "            \"teacher_cm\": t_cm.tolist(),\n",
    "            \"teacher_acc\":t_acc,\n",
    "            \"teacher_prec\":t_prec,\n",
    "            \"teacher_rec\":t_rec,\n",
    "            \"teacher_f1\":t_f1,\n",
    "            \"teacher_auroc\":t_auroc,\n",
    "            \"teacher_epochs\": teacher_epochs,\n",
    "            \"student_acc\": s_acc,\n",
    "            \"student_f1\": s_f1,\n",
    "            \"student_prec\": s_prec,\n",
    "            \"student_rec\": s_rec,\n",
    "            \"student_auroc\": s_auroc,\n",
    "            \"student_cm\": s_cm.tolist(),\n",
    "            \"student_epochs\": student_epochs,\n",
    "            \"gap_acc\": t_acc - s_acc,\n",
    "            \"teacher_encoder_path\": teacher_encoder_filename,\n",
    "            \"teacher_input_dim\": int(teacher_data[\"X_tr\"].shape[1]),\n",
    "            \"shared_head_path\": head_filename,\n",
    "            \"student_encoder_path\": student_encoder_filename,\n",
    "            \"student_input_dim\": int(student_data[\"X_tr\"].shape[1]),\n",
    "            \"encoder_config\": ENCODER_CONFIG,\n",
    "            \"head_config\": HEAD_CONFIG\n",
    "        })\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3774338",
   "metadata": {},
   "source": [
    "## Cella 11: Salvataggio Risultati e Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8ae95a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DONE! Summary:\n",
      "[attn] gemma-2-9b-it->Llama-3.1-8B-Instruct | T_Acc: 0.940 | S_Acc: 0.964 | Gap: -0.025\n",
      "         Teacher epochs: 13, Student epochs: 5\n",
      "[attn] Llama-3.1-8B-Instruct->gemma-2-9b-it | T_Acc: 0.979 | S_Acc: 0.950 | Gap: 0.028\n",
      "         Teacher epochs: 15, Student epochs: 23\n",
      "[mlp] gemma-2-9b-it->Llama-3.1-8B-Instruct | T_Acc: 0.929 | S_Acc: 0.957 | Gap: -0.028\n",
      "         Teacher epochs: 31, Student epochs: 3\n",
      "[mlp] Llama-3.1-8B-Instruct->gemma-2-9b-it | T_Acc: 0.972 | S_Acc: 0.950 | Gap: 0.021\n",
      "         Teacher epochs: 22, Student epochs: 2\n",
      "[hidden] gemma-2-9b-it->Llama-3.1-8B-Instruct | T_Acc: 0.940 | S_Acc: 0.975 | Gap: -0.036\n",
      "         Teacher epochs: 14, Student epochs: 8\n",
      "[hidden] Llama-3.1-8B-Instruct->gemma-2-9b-it | T_Acc: 0.961 | S_Acc: 0.950 | Gap: 0.011\n",
      "         Teacher epochs: 12, Student epochs: 8\n",
      "\n",
      "============================================================\n",
      "✓ Detailed results saved to: results_metrics/approach3_frozen_head_results.json\n",
      "✓ Models saved in: models_frozen_head/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save final metrics with detailed information\n",
    "os.makedirs(\"results_metrics\", exist_ok=True)\n",
    "metrics_file = \"results_metrics/approach3_frozen_head_results.json\"\n",
    "\n",
    "detailed_results = []\n",
    "\n",
    "for r in results_log:\n",
    "    result_entry = {\n",
    "        \"layer_type\": r['layer'],\n",
    "        \"teacher_model\": r['teacher'],\n",
    "        \"student_model\": r['student'],\n",
    "        \"data_info\": {\n",
    "            \"total_balanced_samples\": int(n_balanced),\n",
    "            \"train_samples\": int(len(train_indices)),\n",
    "            \"test_samples\": int(len(test_indices)),\n",
    "            \"concordant_undersampling\": True\n",
    "        },\n",
    "        \n",
    "        # ==================== ENCODER CONFIG ====================\n",
    "        \"encoder_config\": {\n",
    "            \"architecture\": {\n",
    "                \"latent_dim\": r['encoder_config']['latent_dim'],\n",
    "                \"hidden_dim\": r['encoder_config']['hidden_dim'],\n",
    "                \"dropout\": r['encoder_config']['dropout']\n",
    "            },\n",
    "            \"training_hyperparameters\": {\n",
    "                \"learning_rate\": r['encoder_config']['learning_rate'],\n",
    "                \"weight_decay\": r['encoder_config']['weight_decay'],\n",
    "                \"batch_size\": r['encoder_config']['batch_size'],\n",
    "                \"max_epochs\": r['encoder_config']['max_epochs'],\n",
    "                \"early_stopping_patience\": r['encoder_config']['early_stopping_patience'],\n",
    "                \"early_stopping_min_delta\": r['encoder_config']['early_stopping_min_delta'],\n",
    "                \"gradient_clip_max_norm\": r['encoder_config']['gradient_clip_max_norm'],\n",
    "                \"optimizer\": r['encoder_config']['optimizer'],\n",
    "                \"scheduler\": r['encoder_config']['scheduler'],\n",
    "                \"loss_function\": r['encoder_config']['loss_function'],\n",
    "                \"use_class_weights\": r['encoder_config']['use_class_weights']\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # ==================== HEAD CONFIG ====================\n",
    "        \"head_config\": {\n",
    "            \"architecture\": {\n",
    "                \"latent_dim\": r['head_config']['latent_dim'],\n",
    "                \"hidden_dim\": r['head_config']['hidden_dim'],\n",
    "                \"dropout\": r['head_config']['dropout']\n",
    "            },\n",
    "            \"training_hyperparameters\": {\n",
    "                \"learning_rate\": r['head_config']['learning_rate'],\n",
    "                \"weight_decay\": r['head_config']['weight_decay'],\n",
    "                \"batch_size\": r['head_config']['batch_size'],\n",
    "                \"max_epochs\": r['head_config']['max_epochs'],\n",
    "                \"early_stopping_patience\": r['head_config']['early_stopping_patience'],\n",
    "                \"early_stopping_min_delta\": r['head_config']['early_stopping_min_delta'],\n",
    "                \"gradient_clip_max_norm\": r['head_config']['gradient_clip_max_norm'],\n",
    "                \"optimizer\": r['head_config']['optimizer'],\n",
    "                \"scheduler\": r['head_config']['scheduler'],\n",
    "                \"loss_function\": r['head_config']['loss_function'],\n",
    "                \"use_class_weights\": r['head_config']['use_class_weights']\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # ==================== TRAINING RESULTS ====================\n",
    "        \"training_results\": {\n",
    "            \"teacher_encoder\": {\n",
    "                \"input_dim\": r['teacher_input_dim'],\n",
    "                \"epochs_trained\": r['teacher_epochs'],\n",
    "                \"model_saved_path\": r['teacher_encoder_path']\n",
    "            },\n",
    "            \"shared_head\": {\n",
    "                \"epochs_trained\": r['teacher_epochs'],\n",
    "                \"model_saved_path\": r['shared_head_path']\n",
    "            },\n",
    "            \"student_encoder\": {\n",
    "                \"input_dim\": r['student_input_dim'],\n",
    "                \"epochs_trained\": r['student_epochs'],\n",
    "                \"model_saved_path\": r['student_encoder_path']\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # ==================== PERFORMANCE METRICS ====================\n",
    "        \"metrics\": {\n",
    "            \"teacher\": {\n",
    "                \"accuracy\": round(r['teacher_acc'], 4),\n",
    "                \"precision\": round(r['teacher_prec'], 4),\n",
    "                \"recall\": round(r['teacher_rec'], 4),\n",
    "                \"f1_score\": round(r['teacher_f1'], 4),\n",
    "                \"auroc\": round(r['teacher_auroc'], 4),\n",
    "                \"confusion_matrix\": {\n",
    "                    \"TN\": int(r['teacher_cm'][0][0]),\n",
    "                    \"FP\": int(r['teacher_cm'][0][1]),\n",
    "                    \"FN\": int(r['teacher_cm'][1][0]),\n",
    "                    \"TP\": int(r['teacher_cm'][1][1])\n",
    "                }\n",
    "            },\n",
    "            \"student_adapter\": {\n",
    "                \"accuracy\": round(r['student_acc'], 4),\n",
    "                \"precision\": round(r['student_prec'], 4),\n",
    "                \"recall\": round(r['student_rec'], 4),\n",
    "                \"f1_score\": round(r['student_f1'], 4),\n",
    "                \"auroc\": round(r['student_auroc'], 4),\n",
    "                \"confusion_matrix\": {\n",
    "                    \"TN\": int(r['student_cm'][0][0]),\n",
    "                    \"FP\": int(r['student_cm'][0][1]),\n",
    "                    \"FN\": int(r['student_cm'][1][0]),\n",
    "                    \"TP\": int(r['student_cm'][1][1])\n",
    "                }\n",
    "            },\n",
    "            \"transfer_gap\": {\n",
    "                \"accuracy_gap\": round(r['gap_acc'], 4)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    detailed_results.append(result_entry)\n",
    "\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    json.dump(detailed_results, f, indent=2)\n",
    "\n",
    "print(\"\\nDONE! Summary:\")\n",
    "for r in results_log:\n",
    "    print(f\"[{r['layer']}] {r['teacher']}->{r['student']} | T_Acc: {r['teacher_acc']:.3f} | S_Acc: {r['student_acc']:.3f} | Gap: {r['gap_acc']:.3f}\")\n",
    "    print(f\"         Teacher epochs: {r['teacher_epochs']}, Student epochs: {r['student_epochs']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Detailed results saved to: {metrics_file}\")\n",
    "\n",
    "print(f\"✓ Models saved in: models_frozen_head/\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
