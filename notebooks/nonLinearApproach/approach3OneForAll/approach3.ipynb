{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6012de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import traceback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# ==================================================================\n",
    "# REPRODUCIBILITY SETTINGS\n",
    "# ==================================================================\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set all seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set seeds at import time\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8bd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "CACHE_DIR_NAME = \"activation_cache\"\n",
    "\n",
    "LAYER_CONFIG = {\n",
    "    \"Qwen2.5-7B\": \n",
    "    {\n",
    "        \"attn\": [15,16,18],\n",
    "        \"mlp\": [16,18,20],\n",
    "        \"hidden\": [18,19,20]\n",
    "    },    \n",
    "    \"Falcon3-7B-Base\": \n",
    "    {\n",
    "        \"attn\": [2,7,12],\n",
    "        \"mlp\": [10,11,12],\n",
    "        \"hidden\": [2,3,19]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================================================================\n",
    "# HYPERPARAMETERS\n",
    "# ==================================================================\n",
    "# The architecture is now: Input -> Encoder -> Latent(256) -> Head -> Output\n",
    "LATENT_DIM = 256\n",
    "HIDDEN_DIM = 1024  # Internal dimension for the Encoder\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LR = 1e-3\n",
    "PATIENCE = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91839351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_per_json(model_name, dataset_name):\n",
    "    file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name, \"generations\", \"hallucination_labels.json\")\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    total = len(data)\n",
    "    hallucinations = sum(1 for item in data if item['is_hallucination'])\n",
    "    allucinated_items = [item['instance_id'] for item in data if item['is_hallucination']]\n",
    "    return {\n",
    "        'total': total,\n",
    "        'hallucinations': hallucinations,\n",
    "        'hallucinated_items': allucinated_items,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "qwen_stats = stats_per_json(\"Qwen2.5-7B\", \"belief_bank\")\n",
    "falcon_stats = stats_per_json(\"Falcon3-7B-Base\", \"belief_bank\")\n",
    "\n",
    "def load_and_split_layers(model_name, dataset_name, layer_indices, type_layer, stats, train_indices, test_indices):\n",
    "    print(f\" Loading {model_name} [{type_layer}]: layers {layer_indices}...\")\n",
    "    total_samples = stats['total']\n",
    "    hallucinated_set = set(stats['hallucinated_items'])\n",
    "    \n",
    "    y_full = np.zeros(total_samples, dtype=np.int8)\n",
    "    y_full[list(hallucinated_set)] = 1\n",
    "    y_train = y_full[train_indices]\n",
    "    y_test  = y_full[test_indices]\n",
    "\n",
    "    all_features = []\n",
    "    for layer_idx in layer_indices:\n",
    "        file_path = os.path.join(PROJECT_ROOT, CACHE_DIR_NAME, model_name, dataset_name,\n",
    "                                 \"activation_\"+type_layer, f\"layer{layer_idx}_activations.pt\")\n",
    "        if not os.path.exists(file_path): continue\n",
    "        acts = torch.load(file_path, map_location='cpu')\n",
    "        if acts.shape[0] > total_samples: acts = acts[:total_samples]\n",
    "        X_layer = acts.float().numpy() if isinstance(acts, torch.Tensor) else acts.astype(np.float32)\n",
    "        if X_layer.ndim > 2: X_layer = X_layer.reshape(X_layer.shape[0], -1)\n",
    "        all_features.append(X_layer)\n",
    "        del acts; gc.collect()\n",
    "\n",
    "    if not all_features: raise ValueError(f\"No layers found for {model_name}\")\n",
    "    X_full = np.concatenate(all_features, axis=1)\n",
    "    X_train = X_full[train_indices]\n",
    "    X_test  = X_full[test_indices]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float() # BCE expects float\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "def get_generator(seed=SEED):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4637a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. ENCODER: Maps Input Dimension -> Latent Dimension\n",
    "# ------------------------------------------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 1024, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, latent_dim),\n",
    "            nn.LayerNorm(latent_dim) # Normalize latent space for stability\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. CLASSIFICATION HEAD: Maps Latent Dimension -> Probability\n",
    "# ------------------------------------------------------------------\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, latent_dim: int, hidden_dim: int = 128, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1) # Binary output\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return (torch.sigmoid(logits) > 0.5).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e326fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# STRATEGY 1: TRAIN TEACHER (Encoder + Head Jointly)\n",
    "# ==================================================================\n",
    "def train_teacher_pipeline(X_train, y_train, X_val, y_val, input_dim, device, model_name):\n",
    "    print(f\"   [Teacher] Training full pipeline for {model_name}...\")\n",
    "    \n",
    "    # Initialize separate modules\n",
    "    encoder = Encoder(input_dim, LATENT_DIM, HIDDEN_DIM).to(device)\n",
    "    head = ClassificationHead(LATENT_DIM).to(device)\n",
    "    \n",
    "    # Combine parameters for optimizer\n",
    "    params = list(encoder.parameters()) + list(head.parameters())\n",
    "    optimizer = optim.AdamW(params, lr=LR, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # Class weights for imbalance\n",
    "    n_pos = y_train.sum()\n",
    "    n_neg = len(y_train) - n_pos\n",
    "    pos_weight = torch.tensor([n_neg / n_pos]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    train_loader = DataLoader(SimpleDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, generator=get_generator())\n",
    "    val_loader = DataLoader(SimpleDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_states = None\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        encoder.train(); head.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            latents = encoder(X_batch)\n",
    "            logits = head(latents)\n",
    "            \n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        encoder.eval(); head.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                latents = encoder(X_batch)\n",
    "                preds = head.predict(latents)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            patience_counter = 0\n",
    "            best_states = {\n",
    "                'encoder': encoder.state_dict().copy(),\n",
    "                'head': head.state_dict().copy()\n",
    "            }\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"     Early stopping at epoch {epoch+1}. Best Acc: {best_acc:.4f}\")\n",
    "                break\n",
    "                \n",
    "    encoder.load_state_dict(best_states['encoder'])\n",
    "    head.load_state_dict(best_states['head'])\n",
    "    \n",
    "    return encoder, head, best_acc\n",
    "\n",
    "# ==================================================================\n",
    "# STRATEGY 2: TRAIN STUDENT (New Encoder + Frozen Head)\n",
    "# ==================================================================\n",
    "def train_student_adapter(X_train, y_train, X_val, y_val, input_dim, frozen_head, device, student_name):\n",
    "    print(f\"   [Student] Training Adapter Encoder for {student_name} (Head Frozen)...\")\n",
    "    \n",
    "    # 1. Freeze the Head\n",
    "    frozen_head.eval()\n",
    "    for param in frozen_head.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # 2. New Encoder for Student\n",
    "    encoder = Encoder(input_dim, LATENT_DIM, HIDDEN_DIM).to(device)\n",
    "    \n",
    "    # 3. Optimize ONLY the encoder\n",
    "    optimizer = optim.AdamW(encoder.parameters(), lr=LR, weight_decay=1e-2)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    n_pos = y_train.sum()\n",
    "    n_neg = len(y_train) - n_pos\n",
    "    pos_weight = torch.tensor([n_neg / n_pos]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    train_loader = DataLoader(SimpleDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, generator=get_generator())\n",
    "    val_loader = DataLoader(SimpleDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        encoder.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: Student Input -> Student Encoder -> Frozen Head -> Loss\n",
    "            latents = encoder(X_batch)\n",
    "            logits = frozen_head(latents) # Head is fixed\n",
    "            \n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        encoder.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                latents = encoder(X_batch)\n",
    "                preds = frozen_head.predict(latents)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(y_batch.numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            patience_counter = 0\n",
    "            best_state = encoder.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"     Early stopping at epoch {epoch+1}. Best Acc: {best_acc:.4f}\")\n",
    "                break\n",
    "                \n",
    "    encoder.load_state_dict(best_state)\n",
    "    return encoder, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f30f691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============================================================\n",
      "PROCESSING LAYER TYPE: ATTN\n",
      "============================================================\n",
      "Loading data for Qwen...\n",
      " Loading Qwen2.5-7B [attn]: layers [15, 16, 18]...\n",
      "Loading data for Falcon...\n",
      " Loading Falcon3-7B-Base [attn]: layers [2, 7, 12]...\n",
      "\n",
      "--- SCENARIO: Teacher=Qwen2.5-7B -> Student=Falcon3-7B-Base ---\n",
      "   [Teacher] Training full pipeline for Qwen2.5-7B...\n",
      "     Early stopping at epoch 72. Best Acc: 0.9965\n",
      "   [Result] Teacher (Qwen2.5-7B) Test F1: 0.9923 | Acc: 0.9910\n",
      "   [Student] Training Adapter Encoder for Falcon3-7B-Base (Head Frozen)...\n",
      "     Early stopping at epoch 55. Best Acc: 0.9312\n",
      "   [Result] Student (Falcon3-7B-Base) Adapter Test F1: 0.9378 | Acc: 0.9244\n",
      "\n",
      "--- SCENARIO: Teacher=Falcon3-7B-Base -> Student=Qwen2.5-7B ---\n",
      "   [Teacher] Training full pipeline for Falcon3-7B-Base...\n",
      "     Early stopping at epoch 56. Best Acc: 0.9354\n",
      "   [Result] Teacher (Falcon3-7B-Base) Test F1: 0.9429 | Acc: 0.9300\n",
      "   [Student] Training Adapter Encoder for Qwen2.5-7B (Head Frozen)...\n",
      "     Early stopping at epoch 63. Best Acc: 0.9920\n",
      "   [Result] Student (Qwen2.5-7B) Adapter Test F1: 0.9924 | Acc: 0.9911\n",
      "\n",
      "============================================================\n",
      "PROCESSING LAYER TYPE: MLP\n",
      "============================================================\n",
      "Loading data for Qwen...\n",
      " Loading Qwen2.5-7B [mlp]: layers [16, 18, 20]...\n",
      "Loading data for Falcon...\n",
      " Loading Falcon3-7B-Base [mlp]: layers [10, 11, 12]...\n",
      "\n",
      "--- SCENARIO: Teacher=Qwen2.5-7B -> Student=Falcon3-7B-Base ---\n",
      "   [Teacher] Training full pipeline for Qwen2.5-7B...\n",
      "     Early stopping at epoch 55. Best Acc: 0.9934\n",
      "   [Result] Teacher (Qwen2.5-7B) Test F1: 0.9920 | Acc: 0.9906\n",
      "   [Student] Training Adapter Encoder for Falcon3-7B-Base (Head Frozen)...\n",
      "     Early stopping at epoch 54. Best Acc: 0.9260\n",
      "   [Result] Student (Falcon3-7B-Base) Adapter Test F1: 0.9266 | Acc: 0.9105\n",
      "\n",
      "--- SCENARIO: Teacher=Falcon3-7B-Base -> Student=Qwen2.5-7B ---\n",
      "   [Teacher] Training full pipeline for Falcon3-7B-Base...\n",
      "     Early stopping at epoch 24. Best Acc: 0.9208\n",
      "   [Result] Teacher (Falcon3-7B-Base) Test F1: 0.9209 | Acc: 0.9044\n",
      "   [Student] Training Adapter Encoder for Qwen2.5-7B (Head Frozen)...\n",
      "     Early stopping at epoch 62. Best Acc: 0.9934\n",
      "   [Result] Student (Qwen2.5-7B) Adapter Test F1: 0.9910 | Acc: 0.9895\n",
      "\n",
      "============================================================\n",
      "PROCESSING LAYER TYPE: HIDDEN\n",
      "============================================================\n",
      "Loading data for Qwen...\n",
      " Loading Qwen2.5-7B [hidden]: layers [18, 19, 20]...\n",
      "Loading data for Falcon...\n",
      " Loading Falcon3-7B-Base [hidden]: layers [2, 3, 19]...\n",
      "\n",
      "--- SCENARIO: Teacher=Qwen2.5-7B -> Student=Falcon3-7B-Base ---\n",
      "   [Teacher] Training full pipeline for Qwen2.5-7B...\n",
      "     Early stopping at epoch 69. Best Acc: 0.9906\n",
      "   [Result] Teacher (Qwen2.5-7B) Test F1: 0.9891 | Acc: 0.9874\n",
      "   [Student] Training Adapter Encoder for Falcon3-7B-Base (Head Frozen)...\n",
      "     Early stopping at epoch 73. Best Acc: 0.9211\n",
      "   [Result] Student (Falcon3-7B-Base) Adapter Test F1: 0.9240 | Acc: 0.9069\n",
      "\n",
      "--- SCENARIO: Teacher=Falcon3-7B-Base -> Student=Qwen2.5-7B ---\n",
      "   [Teacher] Training full pipeline for Falcon3-7B-Base...\n",
      "     Early stopping at epoch 37. Best Acc: 0.9197\n",
      "   [Result] Teacher (Falcon3-7B-Base) Test F1: 0.9183 | Acc: 0.9012\n",
      "   [Student] Training Adapter Encoder for Qwen2.5-7B (Head Frozen)...\n",
      "     Early stopping at epoch 66. Best Acc: 0.9927\n",
      "   [Result] Student (Qwen2.5-7B) Adapter Test F1: 0.9922 | Acc: 0.9909\n",
      "\n",
      "DONE! Summary:\n",
      "[attn] Qwen2.5-7B->Falcon3-7B-Base | T_Acc: 0.991 | S_Acc: 0.924 | Gap: 0.067\n",
      "[attn] Falcon3-7B-Base->Qwen2.5-7B | T_Acc: 0.930 | S_Acc: 0.991 | Gap: -0.061\n",
      "[mlp] Qwen2.5-7B->Falcon3-7B-Base | T_Acc: 0.991 | S_Acc: 0.911 | Gap: 0.080\n",
      "[mlp] Falcon3-7B-Base->Qwen2.5-7B | T_Acc: 0.904 | S_Acc: 0.990 | Gap: -0.085\n",
      "[hidden] Qwen2.5-7B->Falcon3-7B-Base | T_Acc: 0.987 | S_Acc: 0.907 | Gap: 0.080\n",
      "[hidden] Falcon3-7B-Base->Qwen2.5-7B | T_Acc: 0.901 | S_Acc: 0.991 | Gap: -0.090\n"
     ]
    }
   ],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title, filename):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(\"confusion_matrices_frozen_head\", exist_ok=True)\n",
    "    plt.savefig(os.path.join(\"confusion_matrices_frozen_head\", filename))\n",
    "    plt.close()\n",
    "\n",
    "# ==================================================================\n",
    "# MAIN EXECUTION\n",
    "# ==================================================================\n",
    "n_samples = qwen_stats['total']\n",
    "rng = np.random.RandomState(SEED)\n",
    "shuffled_indices = rng.permutation(n_samples)\n",
    "split = int(0.7 * n_samples)\n",
    "train_indices, test_indices = shuffled_indices[:split], shuffled_indices[split:]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "results_log = []\n",
    "\n",
    "# Define the two configurations\n",
    "scenarios = [\n",
    "    {\"teacher\": \"Qwen2.5-7B\", \"student\": \"Falcon3-7B-Base\"},\n",
    "    {\"teacher\": \"Falcon3-7B-Base\", \"student\": \"Qwen2.5-7B\"}\n",
    "]\n",
    "\n",
    "for layer_type in ['attn', 'mlp', 'hidden']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING LAYER TYPE: {layer_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Load and Preprocess Data for BOTH models first\n",
    "    print(\"Loading data for Qwen...\")\n",
    "    X_qwen_tr, X_qwen_te, y_qwen_tr, y_qwen_te = load_and_split_layers(\n",
    "        \"Qwen2.5-7B\", \"belief_bank\", LAYER_CONFIG[\"Qwen2.5-7B\"][layer_type], \n",
    "        layer_type, qwen_stats, train_indices, test_indices)\n",
    "\n",
    "    print(\"Loading data for Falcon...\")\n",
    "    X_falcon_tr, X_falcon_te, y_falcon_tr, y_falcon_te = load_and_split_layers(\n",
    "        \"Falcon3-7B-Base\", \"belief_bank\", LAYER_CONFIG[\"Falcon3-7B-Base\"][layer_type], \n",
    "        layer_type, falcon_stats, train_indices, test_indices)\n",
    "\n",
    "    # Scaling (Independent for each model)\n",
    "    s_qwen = StandardScaler()\n",
    "    X_qwen_tr = s_qwen.fit_transform(X_qwen_tr)\n",
    "    X_qwen_te = s_qwen.transform(X_qwen_te)\n",
    "\n",
    "    s_falcon = StandardScaler()\n",
    "    X_falcon_tr = s_falcon.fit_transform(X_falcon_tr)\n",
    "    X_falcon_te = s_falcon.transform(X_falcon_te)\n",
    "\n",
    "    # Pack data into a dictionary for easier access\n",
    "    data_map = {\n",
    "        \"Qwen2.5-7B\": {\"X_tr\": X_qwen_tr, \"y_tr\": y_qwen_tr, \"X_te\": X_qwen_te, \"y_te\": y_qwen_te},\n",
    "        \"Falcon3-7B-Base\": {\"X_tr\": X_falcon_tr, \"y_tr\": y_falcon_tr, \"X_te\": X_falcon_te, \"y_te\": y_falcon_te}\n",
    "    }\n",
    "\n",
    "    # 2. Run Both Scenarios\n",
    "    for sc in scenarios:\n",
    "        t_name = sc['teacher']\n",
    "        s_name = sc['student']\n",
    "        print(f\"\\n--- SCENARIO: Teacher={t_name} -> Student={s_name} ---\")\n",
    "        \n",
    "        # Get Data\n",
    "        teacher_data = data_map[t_name]\n",
    "        student_data = data_map[s_name]\n",
    "        \n",
    "        # Split Train into Train/Val for early stopping\n",
    "        n_tr = len(teacher_data[\"X_tr\"])\n",
    "        idx = np.arange(n_tr)\n",
    "        np.random.shuffle(idx)\n",
    "        v_size = int(0.15 * n_tr)\n",
    "        tr_idx, val_idx = idx[v_size:], idx[:v_size]\n",
    "        \n",
    "        # --- PHASE 1: Train Teacher ---\n",
    "        enc_teacher, head_shared, best_acc_t = train_teacher_pipeline(\n",
    "            teacher_data[\"X_tr\"][tr_idx], teacher_data[\"y_tr\"][tr_idx],\n",
    "            teacher_data[\"X_tr\"][val_idx], teacher_data[\"y_tr\"][val_idx],\n",
    "            input_dim=teacher_data[\"X_tr\"].shape[1],\n",
    "            device=device, model_name=t_name\n",
    "        )\n",
    "        \n",
    "        # Evaluate Teacher on Test\n",
    "        enc_teacher.eval(); head_shared.eval()\n",
    "        with torch.no_grad():\n",
    "            z_t = enc_teacher(torch.from_numpy(teacher_data[\"X_te\"]).float().to(device))\n",
    "            preds_t = head_shared.predict(z_t).cpu().numpy()\n",
    "        \n",
    "        t_f1 = f1_score(teacher_data[\"y_te\"], preds_t)\n",
    "        t_acc = accuracy_score(teacher_data[\"y_te\"], preds_t)\n",
    "        print(f\"   [Result] Teacher ({t_name}) Test F1: {t_f1:.4f} | Acc: {t_acc:.4f}\")\n",
    "        plot_confusion_matrix(teacher_data[\"y_te\"], preds_t, \n",
    "                              f\"Teacher {t_name} ({layer_type})\", f\"cm_{layer_type}_teacher_{t_name}.png\")\n",
    "\n",
    "        # --- PHASE 2: Train Student with Frozen Head ---\n",
    "        enc_student, best_acc_s = train_student_adapter(\n",
    "            student_data[\"X_tr\"][tr_idx], student_data[\"y_tr\"][tr_idx],\n",
    "            student_data[\"X_tr\"][val_idx], student_data[\"y_tr\"][val_idx],\n",
    "            input_dim=student_data[\"X_tr\"].shape[1],\n",
    "            frozen_head=head_shared, # PASS FROZEN HEAD\n",
    "            device=device, student_name=s_name\n",
    "        )\n",
    "        \n",
    "        # Evaluate Student on Test\n",
    "        enc_student.eval() # Head is already eval/frozen\n",
    "        with torch.no_grad():\n",
    "            z_s = enc_student(torch.from_numpy(student_data[\"X_te\"]).float().to(device))\n",
    "            preds_s = head_shared.predict(z_s).cpu().numpy()\n",
    "            \n",
    "        s_f1 = f1_score(student_data[\"y_te\"], preds_s)\n",
    "        s_acc = accuracy_score(student_data[\"y_te\"], preds_s)\n",
    "        \n",
    "        print(f\"   [Result] Student ({s_name}) Adapter Test F1: {s_f1:.4f} | Acc: {s_acc:.4f}\")\n",
    "        plot_confusion_matrix(student_data[\"y_te\"], preds_s, \n",
    "                              f\"Student {s_name} Adapter ({layer_type})\", f\"cm_{layer_type}_{s_name}_adapter.png\")\n",
    "        \n",
    "        # Log results\n",
    "        results_log.append({\n",
    "            \"layer\": layer_type,\n",
    "            \"teacher\": t_name,\n",
    "            \"student\": s_name,\n",
    "            \"teacher_acc\": t_acc,\n",
    "            \"teacher_f1\": t_f1,\n",
    "            \"student_acc\": s_acc,\n",
    "            \"student_f1\": s_f1,\n",
    "            \"gap_acc\": t_acc - s_acc\n",
    "        })\n",
    "\n",
    "    # Cleanup memory for next layer\n",
    "    del X_qwen_tr, X_falcon_tr, data_map\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save final metrics\n",
    "os.makedirs(\"results_metrics\", exist_ok=True)\n",
    "with open(\"results_metrics/frozen_head_results.json\", \"w\") as f:\n",
    "    json.dump(results_log, f, indent=4)\n",
    "\n",
    "print(\"\\nDONE! Summary:\")\n",
    "for r in results_log:\n",
    "    print(f\"[{r['layer']}] {r['teacher']}->{r['student']} | T_Acc: {r['teacher_acc']:.3f} | S_Acc: {r['student_acc']:.3f} | Gap: {r['gap_acc']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
