{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f8d9727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PROJECT_ROOT: /home/efontana/HallucinationDetection\n",
      "CACHE_DIR: /home/efontana/HallucinationDetection/activation_cache\n",
      "ONEFORALL_DIR: /home/efontana/HallucinationDetection/notebooks/nonLinearApproach/approach3OneForAll\n"
     ]
    }
   ],
   "source": [
    "# Cross-dataset evaluation (OneForAll / frozen-head)\n",
    "# - NO training: loads saved checkpoints from an existing run directory\n",
    "# - Evaluates activations from a different dataset on the saved prober (shared head + encoder)\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    " )\n",
    "\n",
    "# ---------------------------\n",
    "# Repro / device\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ---------------------------\n",
    "# Project paths\n",
    "# ---------------------------\n",
    "# NOTE: this notebook lives in notebooks/nonLinearApproach/approach3OneForAll/\n",
    "PROJECT_ROOT = Path(os.getcwd()).resolve()\n",
    "while PROJECT_ROOT.name != \"HallucinationDetection\" and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "if PROJECT_ROOT.name != \"HallucinationDetection\":\n",
    "    raise RuntimeError(\"Could not locate project root 'HallucinationDetection' from cwd\")\n",
    "\n",
    "CACHE_DIR = PROJECT_ROOT / \"activation_cache\"\n",
    "ONEFORALL_DIR = PROJECT_ROOT / \"notebooks\" / \"nonLinearApproach\" / \"approach3OneForAll\"\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"CACHE_DIR:\", CACHE_DIR)\n",
    "print(\"ONEFORALL_DIR:\", ONEFORALL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fa6fd",
   "metadata": {},
   "source": [
    "# Cross-dataset evaluation (OneForAll)\n",
    "Questo notebook fa **transfer cross-dataset** senza ri-addestrare nulla:\n",
    "- Scegli una run OneForAll già eseguita (dove ci sono `models_frozen_head/.../*.pt`).\n",
    "- Carica **Teacher encoder + shared head** e **Student adapter encoder** da quella run.\n",
    "- Usa lo **scaler stimato dal dataset di training** (ricostruito dalle attivazioni del dataset di training).\n",
    "- Valuta su un altro dataset: attivazioni $\\to$ (encoder) $\\to$ head (prober).\n",
    "\n",
    "Nota: OneForAll qui è la variante **Frozen Head** (encoder+head per teacher, encoder adapter per student, head condiviso congelato)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6162b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found OneForAll runs:\n",
      " - notebooks/nonLinearApproach/approach3OneForAll/LLama_Gemma_BBC\n",
      " - notebooks/nonLinearApproach/approach3OneForAll/LLama_Gemma_BBF\n",
      " - notebooks/nonLinearApproach/approach3OneForAll/LLama_Gemma_HE\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Discovery: where checkpoints are saved\n",
    "# ---------------------------\n",
    "def find_oneforall_runs(base_dir: Path) -> List[Path]:\n",
    "    \"\"\"Find run folders that contain models_frozen_head.\"\"\"\n",
    "    runs = []\n",
    "    if not base_dir.exists():\n",
    "        return runs\n",
    "    for p in base_dir.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        if (p / \"models_frozen_head\").exists():\n",
    "            runs.append(p)\n",
    "    return sorted(runs)\n",
    "\n",
    "runs = find_oneforall_runs(ONEFORALL_DIR)\n",
    "print(\"Found OneForAll runs:\")\n",
    "for r in runs:\n",
    "    print(\" -\", r.relative_to(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a935b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Activation loading (same as OneForAll run)\n",
    "# ---------------------------\n",
    "def detect_structure_type(model_name: str, dataset_name: str, layer_type: str = \"attn\") -> str:\n",
    "    base_path = CACHE_DIR / model_name / dataset_name / f\"activation_{layer_type}\"\n",
    "    if (base_path / \"hallucinated\").is_dir():\n",
    "        return \"new\"\n",
    "    return \"old\"\n",
    "\n",
    "def stats_from_new_structure(model_name: str, dataset_name: str, layer_type: str = \"attn\") -> Dict:\n",
    "    base_path = CACHE_DIR / model_name / dataset_name / f\"activation_{layer_type}\"\n",
    "    hallucinated_path = base_path / \"hallucinated\"\n",
    "    not_hallucinated_path = base_path / \"not_hallucinated\"\n",
    "    hall_ids_path = hallucinated_path / \"layer0_instance_ids.json\"\n",
    "    not_hall_ids_path = not_hallucinated_path / \"layer0_instance_ids.json\"\n",
    "    with open(hall_ids_path, \"r\") as f:\n",
    "        hallucinated_ids = json.load(f)\n",
    "    with open(not_hall_ids_path, \"r\") as f:\n",
    "        not_hallucinated_ids = json.load(f)\n",
    "    total = len(hallucinated_ids) + len(not_hallucinated_ids)\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"hallucinations\": len(hallucinated_ids),\n",
    "        \"not_hallucinations\": len(not_hallucinated_ids),\n",
    "        \"hallucinated_ids\": hallucinated_ids,\n",
    "        \"not_hallucinated_ids\": not_hallucinated_ids,\n",
    "        \"hallucinated_items\": hallucinated_ids,  # compat\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "\n",
    "def stats_old_structure(model_name: str, dataset_name: str) -> Dict:\n",
    "    file_path = CACHE_DIR / model_name / dataset_name / \"generations\" / \"hallucination_labels.json\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    total = len(data)\n",
    "    hallucinated_items = [item[\"instance_id\"] for item in data if item[\"is_hallucination\"]]\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"hallucinations\": len(hallucinated_items),\n",
    "        \"hallucinated_items\": hallucinated_items,\n",
    "        \"model_name\": model_name,\n",
    "    }\n",
    "\n",
    "def get_stats(model_name: str, dataset_name: str, layer_type: str = \"attn\") -> Dict:\n",
    "    st = detect_structure_type(model_name, dataset_name, layer_type=layer_type)\n",
    "    if st == \"new\":\n",
    "        return stats_from_new_structure(model_name, dataset_name, layer_type=layer_type)\n",
    "    return stats_old_structure(model_name, dataset_name)\n",
    "\n",
    "def get_balanced_indices(y: np.ndarray, seed: int = SEED) -> np.ndarray:\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    min_count = counts.min()\n",
    "    selected = []\n",
    "    for cls in unique:\n",
    "        cls_idx = np.where(y == cls)[0]\n",
    "        if len(cls_idx) > min_count:\n",
    "            sampled = rng.choice(cls_idx, size=min_count, replace=False)\n",
    "            selected.extend(sampled)\n",
    "        else:\n",
    "            selected.extend(cls_idx)\n",
    "    return np.sort(np.array(selected))\n",
    "\n",
    "def get_undersampled_indices_per_model(model_stats: Dict, seed: int = SEED) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    total = model_stats[\"total\"]\n",
    "    hall_set = set(model_stats[\"hallucinated_items\"])\n",
    "    y = np.array([1 if i in hall_set else 0 for i in range(total)], dtype=np.int64)\n",
    "    balanced_idx = get_balanced_indices(y, seed=seed)\n",
    "    balanced_labels = y[balanced_idx]\n",
    "    return balanced_idx, balanced_labels\n",
    "\n",
    "def load_features_for_indices(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    layer_indices: List[int],\n",
    "    layer_type: str,\n",
    "    select_indices: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Load concatenated activations for given global instance indices (already ordered).\"\"\"\n",
    "    structure_type = detect_structure_type(model_name, dataset_name, layer_type=layer_type)\n",
    "    all_features = []\n",
    "    for layer_idx in layer_indices:\n",
    "        base_path = CACHE_DIR / model_name / dataset_name / f\"activation_{layer_type}\"\n",
    "        if structure_type == \"new\":\n",
    "            hall_path = base_path / \"hallucinated\" / f\"layer{layer_idx}_activations.pt\"\n",
    "            not_hall_path = base_path / \"not_hallucinated\" / f\"layer{layer_idx}_activations.pt\"\n",
    "            hall_ids_path = base_path / \"hallucinated\" / f\"layer{layer_idx}_instance_ids.json\"\n",
    "            not_hall_ids_path = base_path / \"not_hallucinated\" / f\"layer{layer_idx}_instance_ids.json\"\n",
    "            if not hall_path.exists() or not not_hall_path.exists():\n",
    "                raise FileNotFoundError(f\"Missing layer {layer_idx} for {model_name}/{dataset_name}/{layer_type}\")\n",
    "            acts_hall = torch.load(hall_path, map_location=\"cpu\")\n",
    "            acts_not_hall = torch.load(not_hall_path, map_location=\"cpu\")\n",
    "            with open(hall_ids_path, \"r\") as f:\n",
    "                hall_ids = json.load(f)\n",
    "            with open(not_hall_ids_path, \"r\") as f:\n",
    "                not_hall_ids = json.load(f)\n",
    "            X_hall = acts_hall.float().numpy() if isinstance(acts_hall, torch.Tensor) else acts_hall.astype(np.float32)\n",
    "            X_not = acts_not_hall.float().numpy() if isinstance(acts_not_hall, torch.Tensor) else acts_not_hall.astype(np.float32)\n",
    "            if X_hall.ndim > 2:\n",
    "                X_hall = X_hall.reshape(X_hall.shape[0], -1)\n",
    "            if X_not.ndim > 2:\n",
    "                X_not = X_not.reshape(X_not.shape[0], -1)\n",
    "            total_samples = len(hall_ids) + len(not_hall_ids)\n",
    "            feature_dim = X_hall.shape[1]\n",
    "            X_layer = np.zeros((total_samples, feature_dim), dtype=np.float32)\n",
    "            for i, inst_id in enumerate(hall_ids):\n",
    "                X_layer[inst_id] = X_hall[i]\n",
    "            for i, inst_id in enumerate(not_hall_ids):\n",
    "                X_layer[inst_id] = X_not[i]\n",
    "            del acts_hall, acts_not_hall, X_hall, X_not\n",
    "        else:\n",
    "            file_path = base_path / f\"layer{layer_idx}_activations.pt\"\n",
    "            if not file_path.exists():\n",
    "                raise FileNotFoundError(f\"Missing layer {layer_idx} for {model_name}/{dataset_name}/{layer_type}\")\n",
    "            acts = torch.load(file_path, map_location=\"cpu\")\n",
    "            X_layer = acts.float().numpy() if isinstance(acts, torch.Tensor) else acts.astype(np.float32)\n",
    "            if X_layer.ndim > 2:\n",
    "                X_layer = X_layer.reshape(X_layer.shape[0], -1)\n",
    "            del acts\n",
    "        X_layer = X_layer[select_indices]\n",
    "        all_features.append(X_layer)\n",
    "        gc.collect()\n",
    "    X = np.concatenate(all_features, axis=1).astype(np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c03377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# OneForAll models (frozen-head)\n",
    "# ---------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, hidden_dim: int = 1024, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, latent_dim),\n",
    "            nn.LayerNorm(latent_dim),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, latent_dim: int, hidden_dim: int = 128, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).squeeze(-1)\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.forward(x)\n",
    "        return (torch.sigmoid(logits) > 0.5).long()\n",
    "\n",
    "def load_teacher_encoder(run_dir: Path, layer_type: str, teacher_model: str) -> Encoder:\n",
    "    ckpt_path = run_dir / \"models_frozen_head\" / layer_type / f\"frozen_head_encoder_{teacher_model}.pt\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    cfg = ckpt[\"encoder_config\"]\n",
    "    enc = Encoder(\n",
    "        input_dim=int(ckpt[\"input_dim\"]),\n",
    "        latent_dim=int(cfg[\"latent_dim\"]),\n",
    "        hidden_dim=int(cfg[\"hidden_dim\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "    )\n",
    "    enc.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    enc.to(DEVICE).eval()\n",
    "    return enc\n",
    "\n",
    "def load_student_encoder(run_dir: Path, layer_type: str, student_model: str) -> Encoder:\n",
    "    ckpt_path = run_dir / \"models_frozen_head\" / layer_type / f\"frozen_head_encoder_{student_model}_adapter.pt\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    cfg = ckpt[\"encoder_config\"]\n",
    "    enc = Encoder(\n",
    "        input_dim=int(ckpt[\"input_dim\"]),\n",
    "        latent_dim=int(cfg[\"latent_dim\"]),\n",
    "        hidden_dim=int(cfg[\"hidden_dim\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "    )\n",
    "    enc.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    enc.to(DEVICE).eval()\n",
    "    return enc\n",
    "\n",
    "def load_shared_head(run_dir: Path, layer_type: str, teacher_model: str) -> ClassificationHead:\n",
    "    ckpt_path = run_dir / \"models_frozen_head\" / layer_type / f\"frozen_head_shared_head_{teacher_model}.pt\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    cfg = ckpt[\"head_config\"]\n",
    "    head = ClassificationHead(\n",
    "        latent_dim=int(ckpt[\"latent_dim\"]),\n",
    "        hidden_dim=int(cfg[\"hidden_dim\"]),\n",
    "        dropout=float(cfg[\"dropout\"]),\n",
    "    )\n",
    "    head.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    head.to(DEVICE).eval()\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26454b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Eval helpers\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def predict_with_encoder_head(encoder: Encoder, head: ClassificationHead, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X_t = torch.from_numpy(X).float().to(DEVICE)\n",
    "    z = encoder(X_t)\n",
    "    logits = head(z)\n",
    "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "    preds = (probs > 0.5).astype(np.int64)\n",
    "    return preds, probs\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_prob: np.ndarray) -> Dict:\n",
    "    # AUROC requires both classes present; if not, return NaN\n",
    "    try:\n",
    "        auroc = float(roc_auc_score(y_true, y_prob))\n",
    "    except Exception:\n",
    "        auroc = float(\"nan\")\n",
    "    cm = confusion_matrix(y_true, y_pred).tolist()\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        \"auroc\": auroc,\n",
    "        \"confusion_matrix\": cm,\n",
    "    }\n",
    "\n",
    "def fit_scaler_from_training_dataset(\n",
    "    model_name: str,\n",
    "    dataset_train: str,\n",
    "    layer_indices: List[int],\n",
    "    layer_type: str,\n",
    "    seed: int = SEED,\n",
    "    train_fraction: float = 0.7,\n",
    "    scaler_fit_on: str = \"train\",  # 'train' or 'all'\n",
    " ) -> StandardScaler:\n",
    "    \"\"\"Rebuild the StandardScaler used in the original run (approx.).\n",
    "    - We undersample deterministically per model (seed)\n",
    "    - We split deterministically (seed)\n",
    "    - Fit scaler either on train only (recommended) or all balanced samples (fallback)\n",
    "    \"\"\"\n",
    "    stats = get_stats(model_name, dataset_train, layer_type=layer_type)\n",
    "    balanced_idx, _ = get_undersampled_indices_per_model(stats, seed=seed)\n",
    "    X_bal = load_features_for_indices(model_name, dataset_train, layer_indices, layer_type, balanced_idx)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    perm = rng.permutation(len(balanced_idx))\n",
    "    split = int(train_fraction * len(perm))\n",
    "    train_local = perm[:split]\n",
    "    if scaler_fit_on == \"all\":\n",
    "        X_fit = X_bal\n",
    "    else:\n",
    "        X_fit = X_bal[train_local]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_fit)\n",
    "    return scaler\n",
    "\n",
    "def load_balanced_eval_set(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    layer_indices: List[int],\n",
    "    layer_type: str,\n",
    "    seed: int = SEED,\n",
    " ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    stats = get_stats(model_name, dataset_name, layer_type=layer_type)\n",
    "    balanced_idx, balanced_y = get_undersampled_indices_per_model(stats, seed=seed)\n",
    "    X = load_features_for_indices(model_name, dataset_name, layer_indices, layer_type, balanced_idx)\n",
    "    return X, balanced_y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fcb8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Cross-dataset evaluation core\n",
    "# ---------------------------\n",
    "def eval_cross_dataset_oneforall(\n",
    "    encoder_run_dir: Path,\n",
    "    head_run_dir: Path,\n",
    "    dataset_train: str,\n",
    "    dataset_activation: str,\n",
    "    dataset_head: str,\n",
    "    teacher_model: str,\n",
    "    student_model: str,\n",
    "    layer_type: str,\n",
    "    layer_config: Dict[str, Dict[str, List[int]]],\n",
    "    seed_teacher_scaler: int = SEED,\n",
    "    seed_student_scaler: int = SEED + 1,\n",
    "    seed_teacher_eval: int = SEED,\n",
    "    seed_student_eval: int = SEED,\n",
    "    scaler_fit_on: str = \"train\",\n",
    " ) -> Dict:\n",
    "    \"\"\"\n",
    "    Loads encoder from encoder_run_dir (trained on dataset_train).\n",
    "    Loads head from head_run_dir (trained on dataset_head).\n",
    "    Evaluates on dataset_activation.\n",
    "    \"\"\"\n",
    "    # 1) Load checkpoints\n",
    "    teacher_enc = load_teacher_encoder(encoder_run_dir, layer_type, teacher_model)\n",
    "    student_enc = load_student_encoder(encoder_run_dir, layer_type, student_model)\n",
    "    \n",
    "    # Load shared head from the HEAD run\n",
    "    shared_head = load_shared_head(head_run_dir, layer_type, teacher_model)\n",
    "\n",
    "    # 2) Rebuild scalers from TRAIN dataset (to match encoder training preprocessing)\n",
    "    teacher_layers = layer_config[teacher_model][layer_type]\n",
    "    student_layers = layer_config[student_model][layer_type]\n",
    "    scaler_teacher = fit_scaler_from_training_dataset(\n",
    "        model_name=teacher_model,\n",
    "        dataset_train=dataset_train,\n",
    "        layer_indices=teacher_layers,\n",
    "        layer_type=layer_type,\n",
    "        seed=seed_teacher_scaler,\n",
    "        scaler_fit_on=scaler_fit_on,\n",
    "    )\n",
    "    scaler_student = fit_scaler_from_training_dataset(\n",
    "        model_name=student_model,\n",
    "        dataset_train=dataset_train,\n",
    "        layer_indices=student_layers,\n",
    "        layer_type=layer_type,\n",
    "        seed=seed_student_scaler,\n",
    "        scaler_fit_on=scaler_fit_on,\n",
    "    )\n",
    "\n",
    "    # 3) Load ACTIVATION dataset (balanced)\n",
    "    X_t, y_t = load_balanced_eval_set(teacher_model, dataset_activation, teacher_layers, layer_type, seed=seed_teacher_eval)\n",
    "    X_s, y_s = load_balanced_eval_set(student_model, dataset_activation, student_layers, layer_type, seed=seed_student_eval)\n",
    "\n",
    "    # 4) Apply TRAIN scalers to ACTIVATION features\n",
    "    X_t = scaler_teacher.transform(X_t).astype(np.float32)\n",
    "    X_s = scaler_student.transform(X_s).astype(np.float32)\n",
    "\n",
    "    # 5) Predict + metrics\n",
    "    pred_t, prob_t = predict_with_encoder_head(teacher_enc, shared_head, X_t)\n",
    "    pred_s, prob_s = predict_with_encoder_head(student_enc, shared_head, X_s)\n",
    "\n",
    "    out = {\n",
    "        \"encoder_run_dir\": str(encoder_run_dir.relative_to(PROJECT_ROOT)),\n",
    "        \"head_run_dir\": str(head_run_dir.relative_to(PROJECT_ROOT)),\n",
    "        \"dataset_train\": dataset_train,\n",
    "        \"dataset_activation\": dataset_activation,\n",
    "        \"dataset_head\": dataset_head,\n",
    "        \"layer_type\": layer_type,\n",
    "        \"teacher_model\": teacher_model,\n",
    "        \"student_model\": student_model,\n",
    "        \"scaler_fit_on\": scaler_fit_on,\n",
    "        \"eval\": {\n",
    "            \"teacher_on_eval\": compute_metrics(y_t, pred_t, prob_t),\n",
    "            \"student_adapter_on_eval\": compute_metrics(y_s, pred_s, prob_s),\n",
    "        },\n",
    "        \"n_samples\": {\n",
    "            \"teacher_eval\": int(len(y_t)),\n",
    "            \"student_eval\": int(len(y_s)),\n",
    "        }\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28f079cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder run: notebooks/nonLinearApproach/approach3OneForAll/LLama_Gemma_BBF\n",
      "Head run: notebooks/nonLinearApproach/approach3OneForAll/LLama_Gemma_BBC\n",
      "Train dataset: belief_bank_facts\n",
      "Activation dataset: belief_bank_facts\n",
      "Head dataset: belief_bank_constraints\n",
      "Teacher: gemma-2-9b-it Student: Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Configure YOUR experiments here\n",
    "# ---------------------------\n",
    "# 1) Run that produced the encoder, adapter and scaler for DATASET_TRAIN\n",
    "ENCODER_RUN_DIR = ONEFORALL_DIR / \"LLama_Gemma_BBF\"  # <- change if needed\n",
    "# 2) Separate run whose shared head you want to reuse (typically trained on DATASET_HEAD)\n",
    "HEAD_RUN_DIR = ONEFORALL_DIR / \"LLama_Gemma_BBC\"  # <- change if needed\n",
    "\n",
    "# 3) Dataset used to train the encoder/scaler run (used only to rebuild scalers)\n",
    "DATASET_TRAIN = \"belief_bank_facts\"  # <- change if needed\n",
    "# 4) Dataset whose activations you feed through that encoder\n",
    "DATASET_ACTIVATION = \"belief_bank_facts\"  # <- change if needed\n",
    "# 5) Dataset whose run produced the shared head you reuse for scoring\n",
    "DATASET_HEAD = \"belief_bank_constraints\"  # <- change if needed\n",
    "\n",
    "# 6) Scenario (teacher -> student) used in the encoder run; we load the corresponding checkpoints\n",
    "TEACHER_MODEL = \"gemma-2-9b-it\"\n",
    "STUDENT_MODEL = \"Llama-3.1-8B-Instruct\"\n",
    "#TEACHER_MODEL = \"Llama-3.1-8B-Instruct\"\n",
    "#STUDENT_MODEL = \"gemma-2-9b-it\"\n",
    "\n",
    "# 7) Layer configuration must match the run settings (same as in app3.py)\n",
    "LAYER_CONFIG = {\n",
    "    \"gemma-2-9b-it\": {\n",
    "        \"attn\": [21, 24, 27],\n",
    "        \"mlp\": [22, 25, 27],\n",
    "        \"hidden\": [23,26, 34],\n",
    "    },\n",
    "    \"Llama-3.1-8B-Instruct\": {\n",
    "        \"attn\": [8, 13, 14],\n",
    "        \"mlp\": [14, 15, 21],\n",
    "        \"hidden\": [14, 15, 16],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Sanity\n",
    "assert ENCODER_RUN_DIR.exists(), f\"Missing ENCODER_RUN_DIR: {ENCODER_RUN_DIR}\"\n",
    "assert (ENCODER_RUN_DIR / \"models_frozen_head\").exists(), \"ENCODER_RUN_DIR must contain models_frozen_head/\"\n",
    "assert HEAD_RUN_DIR.exists(), f\"Missing HEAD_RUN_DIR: {HEAD_RUN_DIR}\"\n",
    "assert (HEAD_RUN_DIR / \"models_frozen_head\").exists(), \"HEAD_RUN_DIR must contain models_frozen_head/\"\n",
    "print(\"Encoder run:\", ENCODER_RUN_DIR.relative_to(PROJECT_ROOT))\n",
    "print(\"Head run:\", HEAD_RUN_DIR.relative_to(PROJECT_ROOT))\n",
    "print(\"Train dataset:\", DATASET_TRAIN)\n",
    "print(\"Activation dataset:\", DATASET_ACTIVATION)\n",
    "print(\"Head dataset:\", DATASET_HEAD)\n",
    "print(\"Teacher:\", TEACHER_MODEL, \"Student:\", STUDENT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b550f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LAYER: attn\n",
      "================================================================================\n",
      "Teacher on eval: {'accuracy': 0.986284289276808, 'precision': 0.9911838790931989, 'recall': 0.9812967581047382, 'f1': 0.9862155388471178, 'auroc': 0.9967249892724548, 'confusion_matrix': [[795, 7], [15, 787]]}\n",
      "Student adapter on eval: {'accuracy': 0.9941634241245136, 'precision': 0.994988864142539, 'recall': 0.9933296275708727, 'f1': 0.9941585535465924, 'auroc': 0.9973266600770424, 'confusion_matrix': [[1790, 9], [12, 1787]]}\n",
      "\n",
      "================================================================================\n",
      "LAYER: mlp\n",
      "================================================================================\n",
      "Teacher on eval: {'accuracy': 0.9825436408977556, 'precision': 0.9825436408977556, 'recall': 0.9825436408977556, 'f1': 0.9825436408977556, 'auroc': 0.99172113357504, 'confusion_matrix': [[788, 14], [14, 788]]}\n",
      "Student adapter on eval: {'accuracy': 0.9936075597554197, 'precision': 0.9922394678492239, 'recall': 0.9949972206781545, 'f1': 0.993616430752151, 'auroc': 0.9979892788316403, 'confusion_matrix': [[1785, 14], [9, 1790]]}\n",
      "\n",
      "================================================================================\n",
      "LAYER: hidden\n",
      "================================================================================\n",
      "Teacher on eval: {'accuracy': 0.9719451371571073, 'precision': 0.9821656050955414, 'recall': 0.9613466334164589, 'f1': 0.9716446124763705, 'auroc': 0.9946875330377298, 'confusion_matrix': [[788, 14], [31, 771]]}\n",
      "Student adapter on eval: {'accuracy': 0.9933296275708727, 'precision': 0.9927817878956136, 'recall': 0.9938854919399667, 'f1': 0.9933333333333333, 'auroc': 0.9980309918332122, 'confusion_matrix': [[1786, 13], [11, 1788]]}\n",
      "\n",
      "Saved: notebooks/nonLinearApproach/approach3OneForAll/LLama_Gemma_BBF/results_metrics/cross_dataset_eval__activation-belief_bank_facts__head-belief_bank_constraints__train-belief_bank_facts__teacher-gemma-2-9b-it__student-Llama-3.1-8B-Instruct.json\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Run cross-dataset evaluation (NO training)\n",
    "# ---------------------------\n",
    "results = []\n",
    "for layer_type in [\"attn\", \"mlp\", \"hidden\"]:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LAYER:\", layer_type)\n",
    "    print(\"=\" * 80)\n",
    "    r = eval_cross_dataset_oneforall(\n",
    "        encoder_run_dir=ENCODER_RUN_DIR,\n",
    "        head_run_dir=HEAD_RUN_DIR,\n",
    "        dataset_train=DATASET_TRAIN,\n",
    "        dataset_activation=DATASET_ACTIVATION,\n",
    "        dataset_head=DATASET_HEAD,\n",
    "        teacher_model=TEACHER_MODEL,\n",
    "        student_model=STUDENT_MODEL,\n",
    "        layer_type=layer_type,\n",
    "        layer_config=LAYER_CONFIG,\n",
    "        scaler_fit_on=\"train\",\n",
    "    )\n",
    "    results.append(r)\n",
    "    print(\"Teacher on eval:\", r[\"eval\"][\"teacher_on_eval\"])\n",
    "    print(\"Student adapter on eval:\", r[\"eval\"][\"student_adapter_on_eval\"])\n",
    "\n",
    "# Save JSON next to the run folder (separate from original training results)\n",
    "out_dir = ENCODER_RUN_DIR / \"results_metrics\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / f\"cross_dataset_eval__activation-{DATASET_ACTIVATION}__head-{DATASET_HEAD}__train-{DATASET_TRAIN}__teacher-{TEACHER_MODEL}__student-{STUDENT_MODEL}.json\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"\\nSaved:\", out_path.relative_to(PROJECT_ROOT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinationdetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
